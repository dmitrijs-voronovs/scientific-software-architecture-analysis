id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/398:264,security,rotat,rotation,264,"Hmm, I closed this but then noticed that @aderzelle said you're not closing on purpose. @aderzelle Is there anything else we can help here? You mentioned that you'll get a colleague to give more suggestions. Just to make it easy to triage for the next teammate on rotation, I'm going to close this for now. But please feel free to open again as needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
https://github.com/google/deepvariant/issues/398:7,usability,close,closed,7,"Hmm, I closed this but then noticed that @aderzelle said you're not closing on purpose. @aderzelle Is there anything else we can help here? You mentioned that you'll get a colleague to give more suggestions. Just to make it easy to triage for the next teammate on rotation, I'm going to close this for now. But please feel free to open again as needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
https://github.com/google/deepvariant/issues/398:129,usability,help,help,129,"Hmm, I closed this but then noticed that @aderzelle said you're not closing on purpose. @aderzelle Is there anything else we can help here? You mentioned that you'll get a colleague to give more suggestions. Just to make it easy to triage for the next teammate on rotation, I'm going to close this for now. But please feel free to open again as needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
https://github.com/google/deepvariant/issues/398:287,usability,close,close,287,"Hmm, I closed this but then noticed that @aderzelle said you're not closing on purpose. @aderzelle Is there anything else we can help here? You mentioned that you'll get a colleague to give more suggestions. Just to make it easy to triage for the next teammate on rotation, I'm going to close this for now. But please feel free to open again as needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
https://github.com/google/deepvariant/issues/399:25,integrability,sub,submitting,25,"Hi Ariel,. Thank you for submitting the issue. This is the right place so please submit the output and other info you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:81,integrability,sub,submit,81,"Hi Ariel,. Thank you for submitting the issue. This is the right place so please submit the output and other info you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:185,availability,down,download,185,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:562,availability,error,errors,562,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1127,availability,reliab,reliably,1127,"udo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <un",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1443,availability,checkpoint,checkpoint,1443,".io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1913,availability,servic,service,1913," ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1921,availability,servic,service,1921,"task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol':",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1941,availability,servic,service,1941," much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_chec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2064,availability,servic,service,2064,"ps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2072,availability,servic,service,2072,"caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2842,availability,Cluster,ClusterSpec,2842,"000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_prov",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5028,availability,operat,operator,5028,iders.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5637,availability,Restor,Restoring,5637,"ill be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=Tru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5991,availability,Restor,Restoring,5991,"dating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:7002,availability,checkpoint,checkpoint,7002,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:104,deployability,instal,install,104,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:121,deployability,fail,failed,121,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:150,deployability,updat,update,150,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:225,deployability,Releas,Release,225,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:250,deployability,Releas,Release,250,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:284,deployability,instal,install,284,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:333,deployability,instal,installation,333,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:394,deployability,instal,installation,394,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:434,deployability,instal,install,434,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1913,deployability,servic,service,1913," ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1921,deployability,servic,service,1921,"task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol':",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1941,deployability,servic,service,1941," much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_chec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2064,deployability,servic,service,2064,"ps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2072,deployability,servic,service,2072,"caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2842,deployability,Cluster,ClusterSpec,2842,"000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_prov",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:3719,deployability,version,version,3719,"pe': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast inste",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:3745,deployability,updat,updating,3745,"istribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:3776,deployability,automat,automatically,3776,"e, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4171,deployability,version,version,4171,"cas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4197,deployability,updat,updating,4197,"log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] G",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4669,deployability,version,version,4669,is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/model,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4695,deployability,updat,updating,4695,removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4968,deployability,version,version,4968,ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4994,deployability,updat,updating,4994,_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restori,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5487,deployability,version,version,5487,"/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5513,deployability,updat,updating,5513,"files/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5542,deployability,API,APIs,5542,"t/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6306,deployability,modul,module,6306,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1460,energy efficiency,model,models,1460,"now if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.44509",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1471,energy efficiency,model,model,1471," is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1609,energy efficiency,core,core,1609,"q 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1654,energy efficiency,CPU,CPU,1654,"riant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1784,energy efficiency,core,core,1784,"xamples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1829,energy efficiency,CPU,CPU,1829,"mples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1833,energy efficiency,Frequenc,Frequency,1833,"frecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2182,energy efficiency,core,core,2182,"e docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2383,energy efficiency,model,modeling,2383,"deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2413,energy efficiency,model,model,2413,"xamples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2481,energy efficiency,estimat,estimator,2481,"217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.45034",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2526,energy efficiency,model,model,2526,"riants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2597,energy efficiency,estimat,estimator,2597,"orflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4387,energy efficiency,estimat,estimator,4387,ng calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from ten,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4558,energy efficiency,model,modeling,4558,kages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for fi,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4861,energy efficiency,model,modeling,4861,f.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5094,energy efficiency,estimat,estimator,5094,ython.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5668,energy efficiency,model,models,5668,"rsion. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5679,energy efficiency,model,model,5679,"ructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5904,energy efficiency,model,modeling,5904,"s.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6022,energy efficiency,model,models,6022,"of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6033,energy efficiency,model,model,6033," or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:7019,energy efficiency,model,models,7019,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:7030,energy efficiency,model,model,7030,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:165,integrability,repositor,repository,165,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:638,integrability,buffer,buffer,638,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1913,integrability,servic,service,1913," ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1921,integrability,servic,service,1921,"task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol':",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1941,integrability,servic,service,1941," much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_chec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2064,integrability,servic,service,2064,"ps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2072,integrability,servic,service,2072,"caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:3719,integrability,version,version,3719,"pe': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast inste",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4171,integrability,version,version,4171,"cas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4669,integrability,version,version,4669,is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/model,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4968,integrability,version,version,4968,ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5487,integrability,version,version,5487,"/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5542,integrability,API,APIs,5542,"t/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6131,integrability,batch,batches,6131," I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6602,integrability,sub,subprocess,6602,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6695,integrability,sub,subprocess,6695,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6776,integrability,sub,subprocess,6776,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:165,interoperability,repositor,repository,165,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1614,interoperability,platform,platform,1614," parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Usi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1789,interoperability,platform,platform,1789," ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_clust",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1985,interoperability,platform,platform,1985,"pposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_ever",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5528,interoperability,standard,standard,5528,"e_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5542,interoperability,API,APIs,5542,"t/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:306,modifiability,Pac,Package,306,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1913,modifiability,servic,service,1913," ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1921,modifiability,servic,service,1921,"task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol':",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1941,modifiability,servic,service,1941," much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_chec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2064,modifiability,servic,service,2064,"ps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2072,modifiability,servic,service,2072,"caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2431,modifiability,paramet,parameters,2431,"z"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:3559,modifiability,pac,packages,3559,"7 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/model",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:3719,modifiability,version,version,3719,"pe': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast inste",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4171,modifiability,version,version,4171,"cas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4669,modifiability,version,version,4669,is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/model,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4968,modifiability,version,version,4968,ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5314,modifiability,pac,packages,5314,"73] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5487,modifiability,version,version,5487,"/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5647,modifiability,paramet,parameters,5647,"oved in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6001,modifiability,paramet,parameters,6001,"eprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --chec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6306,modifiability,modul,module,6306,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6366,modifiability,pac,packages,6366,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6466,modifiability,pac,packages,6466,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:562,performance,error,errors,562,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:604,performance,time,time,604,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:619,performance,parallel,parallel,619,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1110,performance,time,times,1110,"ommands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1150,performance,time,time,1150,"date. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1257,performance,time,time,1257,"ile. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1654,performance,CPU,CPU,1654,"riant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1829,performance,CPU,CPU,1829,"mples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2281,performance,Tune,Tune,2281,"call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2332,performance,perform,performance,2332,"ll_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6131,performance,batch,batches,6131," I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6816,performance,time,time,6816,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:121,reliability,fail,failed,121,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:234,reliability,doe,does,234,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:964,reliability,doe,does,964,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1127,reliability,reliab,reliably,1127,"udo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <un",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1443,reliability,checkpoint,checkpoint,1443,".io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5637,reliability,Restor,Restoring,5637,"ill be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=Tru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5991,reliability,Restor,Restoring,5991,"dating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:7002,reliability,checkpoint,checkpoint,7002,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:150,safety,updat,update,150,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:545,safety,compl,complete,545,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:562,safety,error,errors,562,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:703,safety,input,input,703,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:733,safety,input,input,733,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1100,safety,test,test,1100,"r install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:3745,safety,updat,updating,3745,"istribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4197,safety,updat,updating,4197,"log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] G",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4695,safety,updat,updating,4695,removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4994,safety,updat,updating,4994,_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restori,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5513,safety,updat,updating,5513,"files/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6306,safety,modul,module,6306,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:135,security,apt,apt-get,135,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:150,security,updat,update,150,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:269,security,apt,apt-get,269,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:430,security,apt,apt,430,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:545,security,compl,complete,545,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1460,security,model,models,1460,"now if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.44509",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1471,security,model,model,1471," is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2383,security,model,modeling,2383,"deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2413,security,model,model,2413,"xamples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2526,security,model,model,2526,"riants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:3745,security,updat,updating,3745,"istribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4197,security,updat,updating,4197,"log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] G",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4558,security,model,modeling,4558,kages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for fi,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4695,security,updat,updating,4695,removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4861,security,model,modeling,4861,f.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:4994,security,updat,updating,4994,_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restori,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5513,security,updat,updating,5513,"files/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5668,security,model,models,5668,"rsion. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5679,security,model,model,5679,"ructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:882: div (from tensorflow.python.ops.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:5904,security,model,modeling,5904,"s.m. ath_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6022,security,model,models,6022,"of operator or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6033,security,model,model,6033," or tf.math.divide. I1217 09:08:46.069416 139680301201152 estimator.py:1113] Done calling model_fn. I1217 09:08:47.837505 139680301201152 monitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:7019,security,model,models,7019,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:7030,security,model,model,7030,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1100,testability,test,test,1100,"r install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:3776,testability,automat,automatically,3776,"e, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow. python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. I1217 09:08:42.537085 139680301201152 data_providers.py:367] self.input_read_threads=8. W1217 09:08:42.537606 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/data_providers.py:372: parallel_interleave (from t. ensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I1217 09:08:42.568870 139680301201152 data_providers.py:373] self.input_map_threads=48. I1217 09:08:42.607381 139680301201152 estimator.py:1111] Calling model_fn. W1217 09:08:42.607652 139680301201152 deprecation.py:323] From /tmp/Bazel.runfiles_ik_chB/runfiles/com_google_deepvariant/deepvariant/modeling.py:880: to_float (from tensorflow.python. ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W1217 09:08:42.611661 139680301201152 deprecation.py:323",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6208,testability,Trace,Traceback,6208,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:112,usability,command,commands,112,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:510,usability,command,command,510,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:562,usability,error,errors,562,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:589,usability,command,command,589,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:703,usability,input,input,703,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:733,usability,input,input,733,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1167,usability,command,command,1167,"tory 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1242,usability,command,command,1242,"e a Release file. sudo apt-get -qq -y install docker-ce. E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****. time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1658,usability,support,supports,1658,"bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:2332,usability,perform,performance,2332,"ll_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --. checkpoint ""/opt/models/wgs/model.ckpt"". I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0. 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz. 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:. 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for . best performance. I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters. W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T. I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra. in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non. e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':. None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus. ter': 0, '_master': ''}. I1217 09:08",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6178,usability,user,user,6178,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6624,usability,command,command,6624,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:6807,usability,Command,Command,6807,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:7066,usability,statu,status,7066,"onitored_session.py:222] Graph was finalized. W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho. n.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating:. Use standard file APIs to check for files with this prefix. I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op. I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op. I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA... I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt. I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]. real 2m5.783s. user 0m49.664s. sys 0m7.008s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm. p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,. Ariel .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:391,availability,error,error,391,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:10,deployability,instal,installation,10,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:137,deployability,instal,install,137,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:153,deployability,instal,install-using-the-repository,153,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:554,deployability,contain,contains,554,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:777,deployability,automat,automatically,777,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:805,deployability,configurat,configuration,805,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:907,deployability,version,version,907,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:934,deployability,releas,released,934,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:978,deployability,version,version,978,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1070,deployability,version,version,1070,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:247,energy efficiency,core,cores,247,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:318,energy efficiency,core,cores,318,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:171,integrability,repositor,repository,171,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:805,integrability,configur,configuration,805,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:907,integrability,version,version,907,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:978,integrability,version,version,978,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1070,integrability,version,version,1070,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:42,interoperability,specif,specific,42,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:171,interoperability,repositor,repository,171,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:276,interoperability,standard,standard-,276,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:805,modifiability,configur,configuration,805,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:907,modifiability,version,version,907,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:978,modifiability,version,version,978,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1070,modifiability,version,version,1070,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:391,performance,error,error,391,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:513,performance,memor,memory,513,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:391,safety,error,error,391,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:797,safety,compl,complex,797,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:797,security,compl,complex,797,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:805,security,configur,configuration,805,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:777,testability,automat,automatically,777,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:358,usability,command,command,358,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:376,usability,clear,clear,376,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:391,usability,error,error,391,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:420,usability,command,command,420,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:494,usability,command,command,494,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:513,usability,memor,memory,513,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:1081,usability,prefer,preferred,1081,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository. 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. . 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory. 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances. 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:48,modifiability,scenario,scenarios,48,There are multiple tutorials covering different scenarios of running DeepVariant on [DeepVariant GitHib page](https://github.com/google/deepvariant/blob/r1.1/docs/README.md).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:319,deployability,instal,install,319,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:375,deployability,releas,release,375,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:592,deployability,deploy,deploy,592,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:152,energy efficiency,core,cores,152,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:240,energy efficiency,core,cores,240,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:246,energy efficiency,alloc,allocated,246,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:430,energy efficiency,Cloud,Cloud,430,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:648,energy efficiency,cloud,cloud,648,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:461,interoperability,specif,specifically,461,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:654,interoperability,platform,platform,654,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:203,usability,support,support,203,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:327,usability,command,command,327,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:474,usability,indicat,indicate,474,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:626,usability,experien,experienced,626,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,. Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/400:32,availability,error,error,32,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:38,integrability,messag,message,38,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:38,interoperability,messag,message,38,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:32,performance,error,error,32,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:69,performance,disk,disk,69,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:118,performance,memor,memory,118,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:154,performance,disk,disk,154,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:32,safety,error,error,32,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:32,usability,error,error,32,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:49,usability,indicat,indicating,49,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:118,usability,memor,memory,118,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:64,energy efficiency,current,currently,64,Hi @husamia . Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:56,reliability,doe,doesn,56,Hi @husamia . Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:74,usability,support,support,74,Hi @husamia . Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:39,availability,error,error,39,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:785,deployability,contain,containers,785,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:664,energy efficiency,current,currently,664,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:45,integrability,messag,message,45,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:45,interoperability,messag,message,45,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:39,performance,error,error,39,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:76,performance,disk,disk,76,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:125,performance,memor,memory,125,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:161,performance,disk,disk,161,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:471,performance,disk,disk,471,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:495,performance,time,times,495,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:525,performance,disk,disk,525,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:656,reliability,doe,doesn,656,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:39,safety,error,error,39,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:39,usability,error,error,39,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:56,usability,indicat,indicating,56,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:125,usability,memor,memory,125,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:674,usability,support,support,674,"> Hi @husamia. > . > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. > . > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia. > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:124,availability,error,error,124,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:154,deployability,contain,container,154,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:278,deployability,contain,container,278,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:345,deployability,fail,fails,345,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:426,deployability,fail,fails,426,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:462,deployability,fail,fails,462,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:527,deployability,contain,container,527,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:76,performance,disk,disk,76,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:124,performance,error,error,124,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:376,performance,disk,disk,376,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:506,performance,disk,disk,506,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:345,reliability,fail,fails,345,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:426,reliability,fail,fails,426,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:462,reliability,fail,fails,462,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:124,safety,error,error,124,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:390,safety,input,input,390,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:436,safety,compl,completes,436,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:436,security,compl,completes,436,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:124,usability,error,error,124,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:390,usability,input,input,390,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:102,modifiability,interm,intermediate,102,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:201,modifiability,interm,intermediate,201,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:267,performance,disk,disk,267,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:637,safety,test,testdata,637,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:757,safety,test,testdata,757,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:877,safety,test,testdata,877,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:997,safety,test,testdata,997,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:637,testability,test,testdata,637,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:757,testability,test,testdata,757,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:877,testability,test,testdata,877,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:997,testability,test,testdata,997,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:381,usability,help,help,381,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:1270,usability,close,close,1270,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:1332,usability,help,help,1332,"Hi @husamia ,. I see that you're running DeepTrio. For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam. TOTAL: 3 objects, 138872581896 bytes (129.34 GiB). ```. 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:393,energy efficiency,reduc,reduce,393,". 1. my files are larger than the examples . 60G Dec 6 2019 19CT021737-19CT021737-20190619_64683_S7_346963.bam. 60G Dec 6 2019 19CT021740-19CT021740-20190619_64686_S8_346962.bam. 61G Dec 6 2019 DS187706-DS187706_39743_S1_261735.bam. 2. I am going to try it with --regions 20. **3. I am only interested in the PASS variants, so is it possible to filter output to only those variants? this will reduce the size of the output considerably.**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:345,integrability,filter,filter,345,". 1. my files are larger than the examples . 60G Dec 6 2019 19CT021737-19CT021737-20190619_64683_S7_346963.bam. 60G Dec 6 2019 19CT021740-19CT021740-20190619_64686_S8_346962.bam. 61G Dec 6 2019 DS187706-DS187706_39743_S1_261735.bam. 2. I am going to try it with --regions 20. **3. I am only interested in the PASS variants, so is it possible to filter output to only those variants? this will reduce the size of the output considerably.**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:35,deployability,stage,stages,35,"@husamia . 1. All the intermediate stages (make_examples, call_variants) output some files. For example, when we run our [DeepTrio WGS case study](https://github.com/google/deepvariant/blob/r1.1/docs/deeptrio-wgs-case-study.md), the `intermediate_results_dir` directory contains about 260G of data:. ```. TOTAL: 387 objects, 277723436467 bytes (258.65 GiB). ```. 2. Let me know if just chromosome 20 works or not. With that, the intermediate files should be smaller. 3. You can use the `--only_keep_pass` flag in postprocess_variants. But this won't significantly reduce the amount of output, because this filtering is only done right before writing to VCF files. . https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/postprocess_variants.py#L124. If you're using run_deepvariant, you can add `--postprocess_variants_extra_args=""only_keep_pass=true""`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:270,deployability,contain,contains,270,"@husamia . 1. All the intermediate stages (make_examples, call_variants) output some files. For example, when we run our [DeepTrio WGS case study](https://github.com/google/deepvariant/blob/r1.1/docs/deeptrio-wgs-case-study.md), the `intermediate_results_dir` directory contains about 260G of data:. ```. TOTAL: 387 objects, 277723436467 bytes (258.65 GiB). ```. 2. Let me know if just chromosome 20 works or not. With that, the intermediate files should be smaller. 3. You can use the `--only_keep_pass` flag in postprocess_variants. But this won't significantly reduce the amount of output, because this filtering is only done right before writing to VCF files. . https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/postprocess_variants.py#L124. If you're using run_deepvariant, you can add `--postprocess_variants_extra_args=""only_keep_pass=true""`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:564,energy efficiency,reduc,reduce,564,"@husamia . 1. All the intermediate stages (make_examples, call_variants) output some files. For example, when we run our [DeepTrio WGS case study](https://github.com/google/deepvariant/blob/r1.1/docs/deeptrio-wgs-case-study.md), the `intermediate_results_dir` directory contains about 260G of data:. ```. TOTAL: 387 objects, 277723436467 bytes (258.65 GiB). ```. 2. Let me know if just chromosome 20 works or not. With that, the intermediate files should be smaller. 3. You can use the `--only_keep_pass` flag in postprocess_variants. But this won't significantly reduce the amount of output, because this filtering is only done right before writing to VCF files. . https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/postprocess_variants.py#L124. If you're using run_deepvariant, you can add `--postprocess_variants_extra_args=""only_keep_pass=true""`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:606,integrability,filter,filtering,606,"@husamia . 1. All the intermediate stages (make_examples, call_variants) output some files. For example, when we run our [DeepTrio WGS case study](https://github.com/google/deepvariant/blob/r1.1/docs/deeptrio-wgs-case-study.md), the `intermediate_results_dir` directory contains about 260G of data:. ```. TOTAL: 387 objects, 277723436467 bytes (258.65 GiB). ```. 2. Let me know if just chromosome 20 works or not. With that, the intermediate files should be smaller. 3. You can use the `--only_keep_pass` flag in postprocess_variants. But this won't significantly reduce the amount of output, because this filtering is only done right before writing to VCF files. . https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/postprocess_variants.py#L124. If you're using run_deepvariant, you can add `--postprocess_variants_extra_args=""only_keep_pass=true""`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:22,modifiability,interm,intermediate,22,"@husamia . 1. All the intermediate stages (make_examples, call_variants) output some files. For example, when we run our [DeepTrio WGS case study](https://github.com/google/deepvariant/blob/r1.1/docs/deeptrio-wgs-case-study.md), the `intermediate_results_dir` directory contains about 260G of data:. ```. TOTAL: 387 objects, 277723436467 bytes (258.65 GiB). ```. 2. Let me know if just chromosome 20 works or not. With that, the intermediate files should be smaller. 3. You can use the `--only_keep_pass` flag in postprocess_variants. But this won't significantly reduce the amount of output, because this filtering is only done right before writing to VCF files. . https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/postprocess_variants.py#L124. If you're using run_deepvariant, you can add `--postprocess_variants_extra_args=""only_keep_pass=true""`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:429,modifiability,interm,intermediate,429,"@husamia . 1. All the intermediate stages (make_examples, call_variants) output some files. For example, when we run our [DeepTrio WGS case study](https://github.com/google/deepvariant/blob/r1.1/docs/deeptrio-wgs-case-study.md), the `intermediate_results_dir` directory contains about 260G of data:. ```. TOTAL: 387 objects, 277723436467 bytes (258.65 GiB). ```. 2. Let me know if just chromosome 20 works or not. With that, the intermediate files should be smaller. 3. You can use the `--only_keep_pass` flag in postprocess_variants. But this won't significantly reduce the amount of output, because this filtering is only done right before writing to VCF files. . https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/postprocess_variants.py#L124. If you're using run_deepvariant, you can add `--postprocess_variants_extra_args=""only_keep_pass=true""`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:550,security,sign,significantly,550,"@husamia . 1. All the intermediate stages (make_examples, call_variants) output some files. For example, when we run our [DeepTrio WGS case study](https://github.com/google/deepvariant/blob/r1.1/docs/deeptrio-wgs-case-study.md), the `intermediate_results_dir` directory contains about 260G of data:. ```. TOTAL: 387 objects, 277723436467 bytes (258.65 GiB). ```. 2. Let me know if just chromosome 20 works or not. With that, the intermediate files should be smaller. 3. You can use the `--only_keep_pass` flag in postprocess_variants. But this won't significantly reduce the amount of output, because this filtering is only done right before writing to VCF files. . https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/postprocess_variants.py#L124. If you're using run_deepvariant, you can add `--postprocess_variants_extra_args=""only_keep_pass=true""`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:173,deployability,contain,container,173,I was able to solve the problem by using `--intermediate_results_dir tmp/` option. the files are being written to the tmp folder on local drive instead of inside the Docker container thus not running out of disk space.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:207,performance,disk,disk,207,I was able to solve the problem by using `--intermediate_results_dir tmp/` option. the files are being written to the tmp folder on local drive instead of inside the Docker container thus not running out of disk space.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/401:140,energy efficiency,model,model,140,"Hi @xuxif . When you said ""Incept V3 only accept no more than 3 channel"", what do you mean? Do you mean for the sake of warmstarting from a model with 3 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/401
https://github.com/google/deepvariant/issues/401:140,security,model,model,140,"Hi @xuxif . When you said ""Incept V3 only accept no more than 3 channel"", what do you mean? Do you mean for the sake of warmstarting from a model with 3 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/401
https://github.com/google/deepvariant/issues/401:160,safety,input,input,160,"Pileup images have 6 channels (base, quality,strand...) . But I found inception V3 only accept 3 channels inpput. I mean if deepvariant modify the inception v3 input channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/401
https://github.com/google/deepvariant/issues/401:136,security,modif,modify,136,"Pileup images have 6 channels (base, quality,strand...) . But I found inception V3 only accept 3 channels inpput. I mean if deepvariant modify the inception v3 input channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/401
https://github.com/google/deepvariant/issues/401:160,usability,input,input,160,"Pileup images have 6 channels (base, quality,strand...) . But I found inception V3 only accept 3 channels inpput. I mean if deepvariant modify the inception v3 input channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/401
https://github.com/google/deepvariant/issues/401:175,availability,sli,slim,175,"Hi @xuxif ,. I'm not sure which inception v3 implementation you're looking at. If it helps:. The InceptioV3 implementation that the DeepVariant codebase uses is the one in tf-slim, which supports different number of channels:. https://github.com/google-research/tf-slim/blob/master/tf_slim/nets/inception_v3.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/401
https://github.com/google/deepvariant/issues/401:265,availability,sli,slim,265,"Hi @xuxif ,. I'm not sure which inception v3 implementation you're looking at. If it helps:. The InceptioV3 implementation that the DeepVariant codebase uses is the one in tf-slim, which supports different number of channels:. https://github.com/google-research/tf-slim/blob/master/tf_slim/nets/inception_v3.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/401
https://github.com/google/deepvariant/issues/401:175,reliability,sli,slim,175,"Hi @xuxif ,. I'm not sure which inception v3 implementation you're looking at. If it helps:. The InceptioV3 implementation that the DeepVariant codebase uses is the one in tf-slim, which supports different number of channels:. https://github.com/google-research/tf-slim/blob/master/tf_slim/nets/inception_v3.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/401
https://github.com/google/deepvariant/issues/401:265,reliability,sli,slim,265,"Hi @xuxif ,. I'm not sure which inception v3 implementation you're looking at. If it helps:. The InceptioV3 implementation that the DeepVariant codebase uses is the one in tf-slim, which supports different number of channels:. https://github.com/google-research/tf-slim/blob/master/tf_slim/nets/inception_v3.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/401
https://github.com/google/deepvariant/issues/401:85,usability,help,helps,85,"Hi @xuxif ,. I'm not sure which inception v3 implementation you're looking at. If it helps:. The InceptioV3 implementation that the DeepVariant codebase uses is the one in tf-slim, which supports different number of channels:. https://github.com/google-research/tf-slim/blob/master/tf_slim/nets/inception_v3.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/401
https://github.com/google/deepvariant/issues/401:187,usability,support,supports,187,"Hi @xuxif ,. I'm not sure which inception v3 implementation you're looking at. If it helps:. The InceptioV3 implementation that the DeepVariant codebase uses is the one in tf-slim, which supports different number of channels:. https://github.com/google-research/tf-slim/blob/master/tf_slim/nets/inception_v3.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/401
https://github.com/google/deepvariant/issues/402:123,safety,test,testdata,123,"This seems to run fine when using the following script:. ```. #!/bin/sh. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:123,testability,test,testdata,123,"This seems to run fine when using the following script:. ```. #!/bin/sh. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:392,testability,unit,unittest,392,"This seems to run fine when using the following script:. ```. #!/bin/sh. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:75,availability,echo,echo,75,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:384,availability,echo,echo,384,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:903,availability,echo,echo,903,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:0,deployability,Updat,Updating,0,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:133,integrability,sub,subtle,133,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:47,interoperability,share,share,47,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:0,safety,Updat,Updating,0,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:334,safety,test,testdata,334,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:0,security,Updat,Updating,0,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:334,testability,test,testdata,334,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:602,testability,unit,unittest,602,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:1128,testability,unit,unittest,1128,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:80,usability,command,commands,80,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:198,usability,command,commands,198,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks! ```. BIN_VERSION=""1.0.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:11,safety,sanit,sanity,11,"As another sanity check, what's the output of `ls -l ${INPUT_DIR}`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:11,security,sanit,sanity,11,"As another sanity check, what's the output of `ls -l ${INPUT_DIR}`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:239,safety,test,testdata,239,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:321,safety,test,testdata,321,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:848,safety,test,testdata,848,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:930,safety,test,testdata,930,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:239,testability,test,testdata,239,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:264,testability,unit,unittest,264,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:321,testability,test,testdata,321,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:848,testability,test,testdata,848,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:873,testability,unit,unittest,873,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:930,testability,test,testdata,930,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:1593,testability,unit,unittest,1593,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:1666,testability,unit,unittest,1666,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:1747,testability,unit,unittest,1747,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:1823,testability,unit,unittest,1823,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:1906,testability,unit,unittest,1906,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:31,usability,command,commands,31,"Here's the output of those two commands:. ```. singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ```. The output of `ls -l ${INPUT_DIR}` is:. ```. -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam. -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai. -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed. -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai. -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz. -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai. -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi. ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:62,usability,command,command,62,@spencerking I'm not able to spot the issue with your initial command. But it seems like you are still able to run DeepVariant with the command in https://github.com/google/deepvariant/issues/402#issuecomment-756252754. Please let me know if you have any other questions or if this issue is still unresolved.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:136,usability,command,command,136,@spencerking I'm not able to spot the issue with your initial command. But it seems like you are still able to run DeepVariant with the command in https://github.com/google/deepvariant/issues/402#issuecomment-756252754. Please let me know if you have any other questions or if this issue is still unresolved.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:44,availability,error,error,44,"Might not be the same issue but: I got this error message when doing a custom hello world run. My mistake was that when I removed comments from the example command, a space character was accidentally left behind after a backslash, rendering it useless.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:50,integrability,messag,message,50,"Might not be the same issue but: I got this error message when doing a custom hello world run. My mistake was that when I removed comments from the example command, a space character was accidentally left behind after a backslash, rendering it useless.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:50,interoperability,messag,message,50,"Might not be the same issue but: I got this error message when doing a custom hello world run. My mistake was that when I removed comments from the example command, a space character was accidentally left behind after a backslash, rendering it useless.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:44,performance,error,error,44,"Might not be the same issue but: I got this error message when doing a custom hello world run. My mistake was that when I removed comments from the example command, a space character was accidentally left behind after a backslash, rendering it useless.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:44,safety,error,error,44,"Might not be the same issue but: I got this error message when doing a custom hello world run. My mistake was that when I removed comments from the example command, a space character was accidentally left behind after a backslash, rendering it useless.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:187,safety,accid,accidentally,187,"Might not be the same issue but: I got this error message when doing a custom hello world run. My mistake was that when I removed comments from the example command, a space character was accidentally left behind after a backslash, rendering it useless.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:44,usability,error,error,44,"Might not be the same issue but: I got this error message when doing a custom hello world run. My mistake was that when I removed comments from the example command, a space character was accidentally left behind after a backslash, rendering it useless.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:71,usability,custom,custom,71,"Might not be the same issue but: I got this error message when doing a custom hello world run. My mistake was that when I removed comments from the example command, a space character was accidentally left behind after a backslash, rendering it useless.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:156,usability,command,command,156,"Might not be the same issue but: I got this error message when doing a custom hello world run. My mistake was that when I removed comments from the example command, a space character was accidentally left behind after a backslash, rendering it useless.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/403:174,availability,consist,consists,174,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1073,deployability,log,logic,1073,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:750,integrability,event,event,750,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:681,modifiability,Pac,PacBio,681,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:333,performance,network,network,333,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:469,performance,network,network,469,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1198,performance,network,network,1198,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1307,performance,network,network,1307,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:32,safety,compl,complicated,32,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1073,safety,log,logic,1073,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1467,safety,compl,complicated,1467,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:32,security,compl,complicated,32,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:234,security,ident,identify,234,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:287,security,Ident,Identified,287,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:333,security,network,network,333,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:469,security,network,network,469,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1073,security,log,logic,1073,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1198,security,network,network,1198,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1307,security,network,network,1307,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1467,security,compl,complicated,1467,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1073,testability,log,logic,1073,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1089,testability,simpl,simple,1089,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:174,usability,consist,consists,174,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:620,usability,support,support,620,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:729,usability,support,support,729,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:763,usability,minim,minimum,763,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1089,usability,simpl,simple,1089,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1397,usability,help,helps,1397,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:599,integrability,filter,filtering,599,"Hi @AndrewCarroll,. Thank you very much for this detailed explanation, it is clear to me what is happening now. Unfortunately, this is rather problematic for my use case because I am processing a cohort (DeepVariant + GLnexus) and I impute the variants afterwards. The imputation is based on the PL values: If there are sites where some samples have an actual variant but most of the other samples have a no call (hence with a discrepancy between GQ and PLs), those sites will end up with a low imputation score. Since my reads are ONT corrected, I rely very much on the imputation scores as a post-filtering step to guarantee the quality of my set. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:77,usability,clear,clear,77,"Hi @AndrewCarroll,. Thank you very much for this detailed explanation, it is clear to me what is happening now. Unfortunately, this is rather problematic for my use case because I am processing a cohort (DeepVariant + GLnexus) and I impute the variants afterwards. The imputation is based on the PL values: If there are sites where some samples have an actual variant but most of the other samples have a no call (hence with a discrepancy between GQ and PLs), those sites will end up with a low imputation score. Since my reads are ONT corrected, I rely very much on the imputation scores as a post-filtering step to guarantee the quality of my set. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:947,availability,down,downstream,947,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself? 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)? 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation? Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1290,availability,down,downstream,1290,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself? 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)? 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation? Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1008,integrability,filter,filters,1008,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself? 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)? 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation? Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1214,integrability,transform,transformation,1214,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself? 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)? 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation? Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1214,interoperability,transform,transformation,1214,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself? 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)? 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation? Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1281,interoperability,specif,specific,1281,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself? 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)? 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation? Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:147,testability,understand,understand,147,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself? 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)? 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation? Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:786,testability,coverag,coverage,786,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself? 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)? 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation? Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1207,testability,simpl,simple,1207,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself? 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)? 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation? Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1207,usability,simpl,simple,1207,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself? 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)? 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation? Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:1326,usability,help,help,1326,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself? 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)? 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation? Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:381,availability,down,downstream,381,"Hi @tedyun,. 1. In this use case, we have phased and accurate data from the same cohort **X** that we use for the imputation. 2. I was actually thinking about simply deleting the GQ=0 sites from my GVCFs which seem to be the simpler solution. As you said, they don't provide any useful information. I just wanted to point out here that having those records in output might confuse downstream applications (i.e. imputation). 3. Unfortunately not. The problem is that our imputation system is exclusively based on the PL values and doesn't even read GT or GQ. Thank you for your questions and suggestions. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:530,reliability,doe,doesn,530,"Hi @tedyun,. 1. In this use case, we have phased and accurate data from the same cohort **X** that we use for the imputation. 2. I was actually thinking about simply deleting the GQ=0 sites from my GVCFs which seem to be the simpler solution. As you said, they don't provide any useful information. I just wanted to point out here that having those records in output might confuse downstream applications (i.e. imputation). 3. Unfortunately not. The problem is that our imputation system is exclusively based on the PL values and doesn't even read GT or GQ. Thank you for your questions and suggestions. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:159,testability,simpl,simply,159,"Hi @tedyun,. 1. In this use case, we have phased and accurate data from the same cohort **X** that we use for the imputation. 2. I was actually thinking about simply deleting the GQ=0 sites from my GVCFs which seem to be the simpler solution. As you said, they don't provide any useful information. I just wanted to point out here that having those records in output might confuse downstream applications (i.e. imputation). 3. Unfortunately not. The problem is that our imputation system is exclusively based on the PL values and doesn't even read GT or GQ. Thank you for your questions and suggestions. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:225,testability,simpl,simpler,225,"Hi @tedyun,. 1. In this use case, we have phased and accurate data from the same cohort **X** that we use for the imputation. 2. I was actually thinking about simply deleting the GQ=0 sites from my GVCFs which seem to be the simpler solution. As you said, they don't provide any useful information. I just wanted to point out here that having those records in output might confuse downstream applications (i.e. imputation). 3. Unfortunately not. The problem is that our imputation system is exclusively based on the PL values and doesn't even read GT or GQ. Thank you for your questions and suggestions. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:159,usability,simpl,simply,159,"Hi @tedyun,. 1. In this use case, we have phased and accurate data from the same cohort **X** that we use for the imputation. 2. I was actually thinking about simply deleting the GQ=0 sites from my GVCFs which seem to be the simpler solution. As you said, they don't provide any useful information. I just wanted to point out here that having those records in output might confuse downstream applications (i.e. imputation). 3. Unfortunately not. The problem is that our imputation system is exclusively based on the PL values and doesn't even read GT or GQ. Thank you for your questions and suggestions. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:225,usability,simpl,simpler,225,"Hi @tedyun,. 1. In this use case, we have phased and accurate data from the same cohort **X** that we use for the imputation. 2. I was actually thinking about simply deleting the GQ=0 sites from my GVCFs which seem to be the simpler solution. As you said, they don't provide any useful information. I just wanted to point out here that having those records in output might confuse downstream applications (i.e. imputation). 3. Unfortunately not. The problem is that our imputation system is exclusively based on the PL values and doesn't even read GT or GQ. Thank you for your questions and suggestions. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:309,availability,down,downstream,309,"Thank you for clarification Guillaume - I think deleting `GQ = 0` sites is a reasonable solution if that works for your use case. I agree with you that the current `GT`, `GQ`, `PL` values we emit in this particular case can be confusing. Please let us know if you have any suggestion on improving it for your downstream application (i.e. emitting different `PL` values) - I'll be sure to discuss it with my team. Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:156,energy efficiency,current,current,156,"Thank you for clarification Guillaume - I think deleting `GQ = 0` sites is a reasonable solution if that works for your use case. I agree with you that the current `GT`, `GQ`, `PL` values we emit in this particular case can be confusing. Please let us know if you have any suggestion on improving it for your downstream application (i.e. emitting different `PL` values) - I'll be sure to discuss it with my team. Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:407,security,team,team,407,"Thank you for clarification Guillaume - I think deleting `GQ = 0` sites is a reasonable solution if that works for your use case. I agree with you that the current `GT`, `GQ`, `PL` values we emit in this particular case can be confusing. Please let us know if you have any suggestion on improving it for your downstream application (i.e. emitting different `PL` values) - I'll be sure to discuss it with my team. Thank you,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/403:39,usability,feedback,feedback,39,"Sorry for the delay, I appreciate your feedback in the matter. Closing the issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/404:990,deployability,updat,update,990,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:265,energy efficiency,model,model-case-study,265,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:476,energy efficiency,model,model,476,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:559,energy efficiency,model,model,559,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:773,energy efficiency,model,model,773,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:805,interoperability,specif,specify,805,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:177,modifiability,Pac,PacBio,177,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:258,modifiability,pac,pacbio-model-case-study,258,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:737,modifiability,interm,intermediate,737,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:78,safety,test,tested,78,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:990,safety,updat,update,990,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:265,security,model,model-case-study,265,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:476,security,model,model,476,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:559,security,model,model,559,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:773,security,model,model,773,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:990,security,updat,update,990,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:78,testability,test,tested,78,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:290,usability,confirm,confirm,290,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:433,usability,user,users,433,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:510,usability,user,user,510,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:795,usability,user,users,795,"Hi @ASLeonard , thanks for the report. Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together. I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:. 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves. 2. At this line:. https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80. We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. . Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1788,availability,down,download,1788,"eepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2178,availability,unavail,unavailable,2178,"st-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: N",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2995,availability,ERROR,ERROR,2995,"tra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4297,availability,down,download,4297,"cified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:34,deployability,log,log,34,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:608,deployability,version,version,608,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1097,deployability,instal,install,1097,"didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/ques",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1255,deployability,instal,installing,1255,"e write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singula",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1275,deployability,log,logged,1275,"But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1293,deployability,log,logged,1293,"ee the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1306,deployability,instal,install,1306,"sue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://goog",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1488,deployability,version,version,1488,"? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1547,deployability,version,version,1547,"del files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1568,deployability,version,version,1568," to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(npr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2079,deployability,stack,stackoverflow,2079,".md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2157,deployability,resourc,resource-temporarily-unavailable,2157,"conda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Inpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2696,deployability,log,logs,2696,"cs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementati",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2753,deployability,updat,updating,2753,"all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2984,deployability,Log,Log,2984,"_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3280,deployability,Scale,Scale,3280,"ib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3311,deployability,Scale,Scale,3311,"eepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&sou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3710,deployability,Updat,Update,3710,"odel conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3721,deployability,configurat,configuration,3721,":. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.10708",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3782,deployability,configurat,configuration,3782,"til.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3850,deployability,API,API,3850,"parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3906,deployability,version,version,3906,"l.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3941,deployability,version,version,3941,"e/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4178,deployability,version,version,4178," Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4318,deployability,upgrad,upgrade,4318,"r: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run comp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4612,deployability,version,version,4612,"Board: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:6555,deployability,modul,module,6555,"ichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:508,energy efficiency,current,current,508,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:549,energy efficiency,model,model,549,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:828,energy efficiency,cpu,cpu-only-machine-on-google-cloud-platform,828,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1070,energy efficiency,model,model-case-study,1070,"penVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1526,energy efficiency,cpu,cpu,1526," where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1720,energy efficiency,model,model-case-study,1720,"and here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1906,energy efficiency,model,model-case-study,1906,"the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2157,energy efficiency,resourc,resource-temporarily-unavailable,2157,"conda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Inpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2712,energy efficiency,model,model,2712,"cbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Updat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2817,energy efficiency,Model,Model,2817,"o this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2823,energy efficiency,Optim,Optimizer,2823,"tep:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2885,energy efficiency,Model,Model,2885,"eepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2906,energy efficiency,model,model,2906,"el-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer ver",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2975,energy efficiency,model,model,2975," `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML fil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3045,energy efficiency,model,model,3045,"```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3102,energy efficiency,model,model,3102,"s/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3160,energy efficiency,model,model,3160,"ce-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3217,energy efficiency,model,model,3217,"=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3280,energy efficiency,Scale,Scale,3280,"ib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3311,energy efficiency,Scale,Scale,3311,"eepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&sou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3547,energy efficiency,model,model,3547," \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3595,energy efficiency,model,model,3595,"--call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be remove",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3822,energy efficiency,model,model,3822,"timizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.3674",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3890,energy efficiency,Model,Model,3890,"home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3896,energy efficiency,Optim,Optimizer,3896,"huan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3952,energy efficiency,model,model,3952,". - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3997,energy efficiency,model,model,3997,". - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4046,energy efficiency,model,model,4046,"l. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5565,energy efficiency,cpu,cpu,5565,"s deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5653,energy efficiency,model,model,5653,"nts steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5697,energy efficiency,cpu,cpu,5697,"```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=mai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5707,energy efficiency,model,model,5707,"116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=arg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5716,energy efficiency,model,model,5716,":04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5726,energy efficiency,model,model,5726," 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5740,energy efficiency,model,model,5740,"96 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tole",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5764,energy efficiency,model,model,5764,":118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5894,energy efficiency,cpu,cpu,5894,"all_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:7280,energy efficiency,model,model,7280,"eepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph. f.write(graph_def.SerializeToString()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write. self._prewrite_check(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check. compat.as_bytes(self.__name), compat.as_bytes(self.__mode)). tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:7286,energy efficiency,model,model,7286,"eepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph. f.write(graph_def.SerializeToString()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write. self._prewrite_check(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check. compat.as_bytes(self.__name), compat.as_bytes(self.__mode)). tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:7432,energy efficiency,model,model,7432,"eepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph. f.write(graph_def.SerializeToString()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write. self._prewrite_check(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check. compat.as_bytes(self.__name), compat.as_bytes(self.__mode)). tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:8003,energy efficiency,model,model,8003,"eepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph. f.write(graph_def.SerializeToString()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write. self._prewrite_check(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check. compat.as_bytes(self.__name), compat.as_bytes(self.__mode)). tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:608,integrability,version,version,608,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1488,integrability,version,version,1488,"? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1547,integrability,version,version,1547,"del files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1568,integrability,version,version,1568," to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(npr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3004,integrability,Batch,Batch,3004,"""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3721,integrability,configur,configuration,3721,":. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.10708",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3782,integrability,configur,configuration,3782,"til.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3850,integrability,API,API,3850,"parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3906,integrability,version,version,3906,"l.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3941,integrability,version,version,3941,"e/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4178,integrability,version,version,4178," Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4612,integrability,version,version,4612,"Board: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:861,interoperability,platform,platform,861,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2272,interoperability,bind,bind,2272," logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,12",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2718,interoperability,convers,conversion,2718,"el-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3015,interoperability,specif,specified,3015,"=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3072,interoperability,specif,specified,3072,"//stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3130,interoperability,specif,specified,3130,"d-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3187,interoperability,specif,specified,3187,"746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Int",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3298,interoperability,specif,specified,3298,"://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-downlo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3517,interoperability,specif,specific,3517,"cf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3570,interoperability,format,format,3570,"c) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is depr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3639,interoperability,share,shared,3639,"e"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3850,interoperability,API,API,3850,"parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3971,interoperability,XML,XML,3971,": model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4003,interoperability,xml,xml,4003,"atch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 150",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4198,interoperability,Distribut,Distribution,4198,"ted from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 14016",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5746,interoperability,xml,xml,5746,"envino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5962,interoperability,bind,bind,5962," openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:6651,interoperability,platform,platform,6651,"del.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph. f.write(graph_def.SerializeToString()). File ""/usr/loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:608,modifiability,version,version,608,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:981,modifiability,Pac,PacBio,981,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1063,modifiability,pac,pacbio-model-case-study,1063,"y+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # http",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1488,modifiability,version,version,1488,"? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1547,modifiability,version,version,1547,"del files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1568,modifiability,version,version,1568," to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(npr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1605,modifiability,Pac,PacBio,1605,"version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1713,modifiability,pac,pacbio-model-case-study,1713,"ommand here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model convers",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1899,modifiability,pac,pacbio-model-case-study,1899,"on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2272,modifiability,bind,bind,2272," logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,12",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2396,modifiability,PAC,PACBIO,2396,"scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2852,modifiability,paramet,parameters,2852,"le/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: No",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3026,modifiability,inherit,inherited,3026," my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3060,modifiability,layer,layers,3060,"10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCC",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3083,modifiability,inherit,inherited,3083,"flow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution tim",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3118,modifiability,layer,layers,3118,"las-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Mem",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3141,modifiability,inherit,inherited,3141,"ead-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3198,modifiability,inherit,inherited,3198,"6150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3280,modifiability,Scal,Scale,3280,"ib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3311,modifiability,Scal,Scale,3311,"eepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&sou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3526,modifiability,paramet,parameters,3526,"iant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.dat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3679,modifiability,layer,layers,3679,"ll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3721,modifiability,configur,configuration,3721,":. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.10708",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3782,modifiability,configur,configuration,3782,"til.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3906,modifiability,version,version,3906,"l.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3941,modifiability,version,version,3941,"e/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4178,modifiability,version,version,4178," Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4318,modifiability,upgrad,upgrade,4318,"r: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run comp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4612,modifiability,version,version,4612,"Board: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5962,modifiability,bind,bind,5962," openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:6086,modifiability,PAC,PACBIO,6086," openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in mai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:6555,modifiability,modul,module,6555,"ichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:6624,modifiability,pac,packages,6624,"n still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph. f.write(graph_def.SerializeT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:7676,modifiability,pac,packages,7676,"eepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph. f.write(graph_def.SerializeToString()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write. self._prewrite_check(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check. compat.as_bytes(self.__name), compat.as_bytes(self.__mode)). tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:7803,modifiability,pac,packages,7803,"eepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph. f.write(graph_def.SerializeToString()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write. self._prewrite_check(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check. compat.as_bytes(self.__name), compat.as_bytes(self.__mode)). tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:130,performance,time,time,130,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:828,performance,cpu,cpu-only-machine-on-google-cloud-platform,828,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1526,performance,cpu,cpu,1526," where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2157,performance,resourc,resource-temporarily-unavailable,2157,"conda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Inpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2823,performance,Optimiz,Optimizer,2823,"tep:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2995,performance,ERROR,ERROR,2995,"tra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3004,performance,Batch,Batch,3004,"""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3280,performance,Scale,Scale,3280,"ib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3311,performance,Scale,Scale,3311,"eepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&sou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3896,performance,Optimiz,Optimizer,3896,"huan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4075,performance,execution time,execution time,4075," inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 14016182",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4118,performance,Memor,Memory,4118,"ers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Proc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4326,performance,content,content,4326,". - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes wi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5565,performance,cpu,cpu,5565,"s deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5697,performance,cpu,cpu,5697,"```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=mai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5894,performance,cpu,cpu,5894,"all_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:34,safety,log,log,34,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:56,safety,test,test,56,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:455,safety,permiss,permission,455,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1275,safety,log,logged,1275,"But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1293,safety,log,logged,1293,"ee the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2157,safety,resourc,resource-temporarily-unavailable,2157,"conda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Inpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2466,safety,input,input,2466," is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: Fal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2696,safety,log,logs,2696,"cs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementati",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2753,safety,updat,updating,2753,"all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2879,safety,Input,Input,2879,"docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2984,safety,Log,Log,2984,"_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2995,safety,ERROR,ERROR,2995,"tra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3054,safety,Input,Input,3054,"it -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3169,safety,Input,Input,3169,"arily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3483,safety,input,input,3483,"hr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3541,safety,Input,Input,3541,"vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimenta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3710,safety,Updat,Update,3710,"odel conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3745,safety,input,input,3745," for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3840,safety,Detect,Detection,3840,"Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5318,safety,compl,completes,5318,"de&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5455,safety,permiss,permission,5455,"/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:6168,safety,input,input,6168,"INO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:6555,safety,modul,module,6555,"ichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:7980,safety,Permiss,PermissionDeniedError,7980,"eepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph. f.write(graph_def.SerializeToString()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write. self._prewrite_check(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check. compat.as_bytes(self.__name), compat.as_bytes(self.__mode)). tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:34,security,log,log,34,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:268,security,access,access,268,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:549,security,model,model,549,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:933,security,ssh,ssh,933,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1070,security,model,model-case-study,1070,"penVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1275,security,log,logged,1275,"But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1293,security,log,logged,1293,"ee the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1720,security,model,model-case-study,1720,"and here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1906,security,model,model-case-study,1906,"the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2696,security,log,logs,2696,"cs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementati",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2712,security,model,model,2712,"cbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Updat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2753,security,updat,updating,2753,"all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2817,security,Model,Model,2817,"o this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2885,security,Model,Model,2885,"eepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2906,security,model,model,2906,"el-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer ver",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2975,security,model,model,2975," `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML fil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2984,security,Log,Log,2984,"_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3045,security,model,model,3045,"```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3102,security,model,model,3102,"s/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3160,security,model,model,3160,"ce-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3217,security,model,model,3217,"=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3547,security,model,model,3547," \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3595,security,model,model,3595,"--call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be remove",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3710,security,Updat,Update,3710,"odel conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3721,security,configur,configuration,3721,":. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.10708",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3782,security,configur,configuration,3782,"til.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3822,security,model,model,3822,"timizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.3674",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3840,security,Detect,Detection,3840,"Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3890,security,Model,Model,3890,"home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3952,security,model,model,3952,". - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3997,security,model,model,3997,". - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4046,security,model,model,4046,"l. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5318,security,compl,completes,5318,"de&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5653,security,model,model,5653,"nts steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5707,security,model,model,5707,"116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=arg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5716,security,model,model,5716,":04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5726,security,model,model,5726," 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5740,security,model,model,5740,"96 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tole",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:5764,security,model,model,5764,":118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:7280,security,model,model,7280,"eepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph. f.write(graph_def.SerializeToString()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write. self._prewrite_check(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check. compat.as_bytes(self.__name), compat.as_bytes(self.__mode)). tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:7286,security,model,model,7286,"eepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph. f.write(graph_def.SerializeToString()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write. self._prewrite_check(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check. compat.as_bytes(self.__name), compat.as_bytes(self.__mode)). tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:7432,security,model,model,7432,"eepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph. f.write(graph_def.SerializeToString()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write. self._prewrite_check(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check. compat.as_bytes(self.__name), compat.as_bytes(self.__mode)). tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:8003,security,model,model,8003,"eepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__. freeze_graph(model, checkpoint_path, tensor_shape). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph. f.write(graph_def.SerializeToString()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write. self._prewrite_check(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check. compat.as_bytes(self.__name), compat.as_bytes(self.__mode)). tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:34,testability,log,log,34,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:56,testability,test,test,56,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1275,testability,log,logged,1275,"But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1293,testability,log,logged,1293,"ee the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2157,testability,resourc,resource-temporarily-unavailable,2157,"conda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Inpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2696,testability,log,logs,2696,"cs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementati",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2984,testability,Log,Log,2984,"_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:6407,testability,Trace,Traceback,6407,"s from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants. checkpoint_path, input_fn=tf_dataset, model=model). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:329,usability,command,command,329,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:428,usability,command,command,428,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:724,usability,command,command,724,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:814,usability,command,command-for-a-cpu-only-machine-on-google-cloud-platform,814,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:. (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.). (2) What is your Singularity version? I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine. I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1761,usability,command,commands,1761,"eepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine. After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2035,usability,command,command,2035,"b/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```. curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh. ```. After installing conda, I logged out and re-logged in. I install Singularity:. ```. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh. bash install_singularity.sh. ```. Here is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2466,usability,input,input,2466," is my Singularity version:. ```. (base) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.3.0-1. ```. ## Run through PacBio tutorial. I follow the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. and ran through all commands set up conda, and download all files. When I get to this step:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: Fal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2879,usability,Input,Input,2879,"docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2995,usability,ERROR,ERROR,2995,"tra_args ""use_openvino=true""`. So my command is:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3054,usability,Input,Input,3054,"it -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3169,usability,Input,Input,3169,"arily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3483,usability,input,input,3483,"hr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3541,usability,Input,Input,3541,"vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. This actually worked for me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimenta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3672,usability,custom,custom,3672," me. I'll paste some logs around the model conversion:. ```. Instructions for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:3745,usability,input,input,3745," for updating:. Use `tf.compat.v1.graph_util.remove_training_nodes`. Model Optimizer arguments:. Common parameters:. - Path to the Input Model: /home/pichuan/model.pb. - Path for generated IR: /home/pichuan/. - IR output name: model. - Log level: ERROR. - Batch: Not specified, inherited from the model. - Input layers: Not specified, inherited from the model. - Output layers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4118,usability,Memor,Memory,4118,"ers: Not specified, inherited from the model. - Input shapes: Not specified, inherited from the model. - Mean values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Proc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4227,usability,tool,toolkit,4227,"values: [128,128,128,128,128,128,128,128,128]. - Scale values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:4282,usability,tool,toolkit,4282,"values: Not specified. - Scale factor: 128.0. - Precision of IR: FP32. - Enable fusing: True. - Enable grouped convolutions fusing: True. - Move mean values to preprocess section: False. - Reverse input channels: False. TensorFlow specific parameters:. - Input model in text protobuf format: False. - Path to model dump for TensorBoard: None. - List of shared libraries with TensorFlow custom layers implementation: None. - Update the configuration file with input/output node names: None. - Use configuration file used to generate the model with Object Detection API: None. - Use the config file: None. Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /home/pichuan/./model.xml. [ SUCCESS ] BIN file: /home/pichuan/./model.bin. [ SUCCESS ] Total execution time: 24.29 seconds. [ SUCCESS ] Memory consumed: 761 MB. It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*. WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. ... ```. After this, the call_variants steps started running with no issues:. ```. ... I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants. I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO). I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO). I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:6168,usability,input,input,6168,"INO). I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO). ... ```. My run completes with no issue. (postprocess_variants finished running as well). ## What if I run this from a directory that I don't have write-permission? My previous run was done in a working directory that I can write to:. ```. (base) pichuan@pichuan-cpu:~$ pwd. /home/pichuan. ```. And in fact, after my run, I can still see some of the `model.*` files. ```. (base) pichuan@pichuan-cpu:~$ ls model.*. model.bin model.mapping model.xml. ```. I think model.pb got deleted programmatically. What if I repeat the run in a directory that I can't write to? ```. (base) pichuan@pichuan-cpu:~$ cd /. ```. And repeat the same run:. ```. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref $HOME/reference/GRCh38_no_alt_analysis_set.fasta \. --reads $HOME/input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf $HOME/deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20 \. --call_variants_extra_args ""use_openvino=true"". ```. With this, I'm now seeing the same issue:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_m__akao7/runfiles/com_google_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:224,interoperability,xml,xml,224,"@pichuan, Thanks for investigation! Sorry, I just not familiar with Singularity, but am I right that it writes all the files directly to host filesystem? Maybe, as you mentioned, it make sense to write intermediate .pb and .xml & .bin files to output directory? Basedir of `output_vcf` or `output_gvcf`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:202,modifiability,interm,intermediate,202,"@pichuan, Thanks for investigation! Sorry, I just not familiar with Singularity, but am I right that it writes all the files directly to host filesystem? Maybe, as you mentioned, it make sense to write intermediate .pb and .xml & .bin files to output directory? Basedir of `output_vcf` or `output_gvcf`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:118,deployability,contain,container,118,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:199,deployability,contain,container,199,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:98,energy efficiency,model,model,98,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:135,energy efficiency,model,models,135,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:149,energy efficiency,model,model,149,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:301,energy efficiency,current,current,301,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:254,interoperability,bind,binds,254,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:142,modifiability,pac,pacbio,142,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:254,modifiability,bind,binds,254,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:98,security,model,model,98,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:135,security,model,models,135,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:149,security,model,model,149,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:222,security,access,access,222,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:347,security,access,access,347,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:59,deployability,version,version,59,"Thanks @ASLeonard . Can you tell me:. 1. Which Singularity version do you have. (and OS version too). 2. What command did you run? I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look. I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:88,deployability,version,version,88,"Thanks @ASLeonard . Can you tell me:. 1. Which Singularity version do you have. (and OS version too). 2. What command did you run? I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look. I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:459,deployability,updat,update,459,"Thanks @ASLeonard . Can you tell me:. 1. Which Singularity version do you have. (and OS version too). 2. What command did you run? I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look. I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:397,energy efficiency,current,current,397,"Thanks @ASLeonard . Can you tell me:. 1. Which Singularity version do you have. (and OS version too). 2. What command did you run? I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look. I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:59,integrability,version,version,59,"Thanks @ASLeonard . Can you tell me:. 1. Which Singularity version do you have. (and OS version too). 2. What command did you run? I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look. I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:88,integrability,version,version,88,"Thanks @ASLeonard . Can you tell me:. 1. Which Singularity version do you have. (and OS version too). 2. What command did you run? I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look. I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:59,modifiability,version,version,59,"Thanks @ASLeonard . Can you tell me:. 1. Which Singularity version do you have. (and OS version too). 2. What command did you run? I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look. I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:88,modifiability,version,version,88,"Thanks @ASLeonard . Can you tell me:. 1. Which Singularity version do you have. (and OS version too). 2. What command did you run? I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look. I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:357,modifiability,interm,intermediate,357,"Thanks @ASLeonard . Can you tell me:. 1. Which Singularity version do you have. (and OS version too). 2. What command did you run? I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look. I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:459,safety,updat,update,459,"Thanks @ASLeonard . Can you tell me:. 1. Which Singularity version do you have. (and OS version too). 2. What command did you run? I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look. I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:459,security,updat,update,459,"Thanks @ASLeonard . Can you tell me:. 1. Which Singularity version do you have. (and OS version too). 2. What command did you run? I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look. I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:110,usability,command,command,110,"Thanks @ASLeonard . Can you tell me:. 1. Which Singularity version do you have. (and OS version too). 2. What command did you run? I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look. I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:219,usability,close,close,219,"Thanks @ASLeonard . Can you tell me:. 1. Which Singularity version do you have. (and OS version too). 2. What command did you run? I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look. I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:343,availability,error,error,343,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1060,availability,error,error,1060,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1300,availability,error,error,1300,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1430,availability,error,error,1430,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:0,deployability,Version,Versions,0,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:24,deployability,version,version,24,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:60,deployability,releas,release,60,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1161,deployability,contain,container,1161,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1396,deployability,log,login,1396,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1123,energy efficiency,model,model,1123,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1190,energy efficiency,model,models,1190,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1269,energy efficiency,model,models,1269,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:0,integrability,Version,Versions,0,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:24,integrability,version,version,24,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:218,interoperability,bind,bind,218,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:428,interoperability,bind,binding,428,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1256,interoperability,bind,bind,1256,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:0,modifiability,Version,Versions,0,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:24,modifiability,version,version,24,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:218,modifiability,bind,bind,218,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:428,modifiability,bind,binding,428,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:669,modifiability,PAC,PACBIO,669,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1197,modifiability,pac,pacbio,1197,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1256,modifiability,bind,bind,1256,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1276,modifiability,pac,pacbio,1276,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:343,performance,error,error,343,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1060,performance,error,error,1060,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1300,performance,error,error,1300,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1430,performance,error,error,1430,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:343,safety,error,error,343,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:471,safety,INPUT,INPUT,471,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:479,safety,input,input,479,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:687,safety,input,input,687,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:715,safety,input,input,715,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1060,safety,error,error,1060,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1300,safety,error,error,1300,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1396,safety,log,login,1396,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1430,safety,error,error,1430,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1474,safety,permiss,permissions,1474,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1123,security,model,model,1123,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1190,security,model,models,1190,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1269,security,model,models,1269,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1396,security,log,login,1396,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1396,testability,log,login,1396,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:126,usability,command,command,126,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:288,usability,command,command,288,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:343,usability,error,error,343,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:471,usability,INPUT,INPUT,471,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:479,usability,input,input,479,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:687,usability,input,input,687,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:715,usability,input,input,715,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1060,usability,error,error,1060,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1300,usability,error,error,1300,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1379,usability,command,command,1379,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1430,usability,error,error,1430,"Versions. - singularity version 3.6.4-1.el7. - CentOS Linux release 7.9.2009. - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue. . ```. singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \. deepvariant_1.1.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /input/asm.fasta \. --reads /input/hifi.bam \. --output_vcf /output/asm.output.vcf.gz \. --output_gvcf /output/asm.output.g.vcf.gz \. --num_shards ""${THREADS}"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /output/intermediate_results_dir. ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:936,availability,error,error,936,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:61,deployability,version,versions,61,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:119,deployability,build,build,119,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:896,deployability,version,version,896,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1040,deployability,version,version,1040,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1061,deployability,version,version,1061,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1019,energy efficiency,cpu,cpu,1019,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1115,energy efficiency,cpu,cpu,1115,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1146,energy efficiency,cpu,cpu,1146,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:61,integrability,version,versions,61,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:896,integrability,version,version,896,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1040,integrability,version,version,1040,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1061,integrability,version,version,1061,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:61,modifiability,version,versions,61,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:472,modifiability,PAC,PACBIO,472,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:896,modifiability,version,version,896,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1040,modifiability,version,version,1040,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1061,modifiability,version,version,1061,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:75,performance,time,time,75,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:936,performance,error,error,936,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1019,performance,cpu,cpu,1019,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1115,performance,cpu,cpu,1115,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1146,performance,cpu,cpu,1146,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:260,safety,input,input,260,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:267,safety,input,input,267,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:545,safety,input,input,545,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:936,safety,error,error,936,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:260,usability,input,input,260,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:267,usability,input,input,267,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:545,usability,input,input,545,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:936,usability,error,error,936,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. mkdir -p intermediate_results_dir. ```. ```. singularity run \. -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type ""PACBIO"" \. --ref /reference/GRCh38_no_alt_analysis_set.fasta \. --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf /deepvariant1/output.vcf.gz \. --output_gvcf /deepvariant1/output.g.vcf.gz \. --num_shards $(nproc) \. --regions ""chr20:10,000,000-10,010,000"" \. --call_variants_extra_args ""use_openvino=true"" \. --intermediate_results_dir /intermediate_results_dir. ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:. ```. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.6.4. (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:639,availability,checkpoint,checkpoint,639,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1291,availability,cluster,cluster,1291,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:261,deployability,contain,containall,261,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:779,deployability,version,version,779,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:816,deployability,version,version,816,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1176,deployability,contain,container,1176,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1291,deployability,cluster,cluster,1291,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1325,deployability,contain,container,1325,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:656,energy efficiency,model,models,656,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:670,energy efficiency,model,model,670,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:763,energy efficiency,Model,Model,763,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:769,energy efficiency,Optim,Optimizer,769,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:827,energy efficiency,model,model,827,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:866,energy efficiency,model,model,866,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:909,energy efficiency,model,model,909,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1029,energy efficiency,model,model,1029,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:779,integrability,version,version,779,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:816,integrability,version,version,816,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:846,interoperability,XML,XML,846,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:872,interoperability,xml,xml,872,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1047,interoperability,xml,xml,1047,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1116,interoperability,bind,binds,1116,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:663,modifiability,pac,pacbio,663,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:779,modifiability,version,version,779,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:816,modifiability,version,version,816,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1116,modifiability,bind,binds,1116,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:769,performance,Optimiz,Optimizer,769,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:938,performance,execution time,execution time,938,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:983,performance,Memor,Memory,983,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:639,reliability,checkpoint,checkpoint,639,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:39,safety,test,tested,39,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:284,safety,input,input,284,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:291,safety,input,input,291,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:656,security,model,models,656,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:670,security,model,model,670,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:763,security,Model,Model,763,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:827,security,model,model,827,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:866,security,model,model,866,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:909,security,model,model,909,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:1029,security,model,model,1029,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:39,testability,test,tested,39,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:159,usability,command,commands,159,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:284,usability,input,input,284,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:291,usability,input,input,291,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:729,usability,progress,progress,729,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:983,usability,Memor,Memory,983,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell. `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants. ```. cd ouput. ../opt/deepvariant/bin/call_variants \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \. --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \. --checkpoint ""/opt/models/pacbio/model.ckpt"" \. --use_openvino. ```. At this point, we made progress with the following. ```. Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model. [ SUCCESS ] XML file: /output/./model.xml. [ SUCCESS ] BIN file: /output/./model.bin. [ SUCCESS ] Total execution time: 30.04 seconds. . [ SUCCESS ] Memory consumed: 699 MB. ```. With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:589,availability,checkpoint,checkpoint,589,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:291,deployability,contain,containall,291,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:605,energy efficiency,model,models,605,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:619,energy efficiency,model,model,619,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:44,integrability,batch,batch,44,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:50,integrability,sub,submit,50,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:612,modifiability,pac,pacbio,612,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:44,performance,batch,batch,44,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:589,reliability,checkpoint,checkpoint,589,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:314,safety,input,input,314,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:321,safety,input,input,321,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:605,security,model,models,605,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:619,security,model,model,619,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:110,usability,command,commands,110,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:204,usability,command,command,204,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:222,usability,command,command,222,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:314,usability,input,input,314,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:321,usability,input,input,321,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was. ```. singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \. /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \. --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \. --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \. --checkpoint /opt/models/pacbio/model.ckpt \. --use_openvino"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:71,deployability,continu,continue,71,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:173,deployability,updat,updated,173,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:301,deployability,updat,update,301,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:38,energy efficiency,current,current,38,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:334,integrability,sub,submit,334,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:161,safety,test,test,161,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:173,safety,updat,updated,173,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:301,safety,updat,update,301,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:173,security,updat,updated,173,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:301,security,updat,update,301,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:161,testability,test,test,161,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:156,usability,help,help,156,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:263,usability,close,close,263,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/405:324,integrability,sub,subsequent,324,"Hi @aardes . Yes, this is exactly what GLnexus was designed for, to allow for efficient merging of the N+1 sample in a manner analogous to genomicsDB (the [GLnexus paper](https://www.biorxiv.org/content/10.1101/343970v1) is informative for understanding this. The generation of a gVCF from DeepVariant is independent of its subsequent analysis by GLnexus, so you can incrementally add more sample, and later run GLnexus to merge these gVCFs. Hopefully this answered your question. Please let me know if there is something that remains unclear. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/405
https://github.com/google/deepvariant/issues/405:195,performance,content,content,195,"Hi @aardes . Yes, this is exactly what GLnexus was designed for, to allow for efficient merging of the N+1 sample in a manner analogous to genomicsDB (the [GLnexus paper](https://www.biorxiv.org/content/10.1101/343970v1) is informative for understanding this. The generation of a gVCF from DeepVariant is independent of its subsequent analysis by GLnexus, so you can incrementally add more sample, and later run GLnexus to merge these gVCFs. Hopefully this answered your question. Please let me know if there is something that remains unclear. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/405
https://github.com/google/deepvariant/issues/405:240,testability,understand,understanding,240,"Hi @aardes . Yes, this is exactly what GLnexus was designed for, to allow for efficient merging of the N+1 sample in a manner analogous to genomicsDB (the [GLnexus paper](https://www.biorxiv.org/content/10.1101/343970v1) is informative for understanding this. The generation of a gVCF from DeepVariant is independent of its subsequent analysis by GLnexus, so you can incrementally add more sample, and later run GLnexus to merge these gVCFs. Hopefully this answered your question. Please let me know if there is something that remains unclear. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/405
https://github.com/google/deepvariant/issues/405:78,usability,efficien,efficient,78,"Hi @aardes . Yes, this is exactly what GLnexus was designed for, to allow for efficient merging of the N+1 sample in a manner analogous to genomicsDB (the [GLnexus paper](https://www.biorxiv.org/content/10.1101/343970v1) is informative for understanding this. The generation of a gVCF from DeepVariant is independent of its subsequent analysis by GLnexus, so you can incrementally add more sample, and later run GLnexus to merge these gVCFs. Hopefully this answered your question. Please let me know if there is something that remains unclear. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/405
https://github.com/google/deepvariant/issues/405:129,security,team,team,129,"Dear @AndrewCarroll ,. Thank you very much for the description, and for sharing the paper. I think I need to contact the GLnexux team because I am not able to find the necessary GLnexus documents/scripts regarding the pVCF and the database. thank you very much. Amin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/405
https://github.com/google/deepvariant/issues/405:186,usability,document,documents,186,"Dear @AndrewCarroll ,. Thank you very much for the description, and for sharing the paper. I think I need to contact the GLnexux team because I am not able to find the necessary GLnexus documents/scripts regarding the pVCF and the database. thank you very much. Amin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/405
https://github.com/google/deepvariant/issues/406:229,deployability,contain,contains,229,"Hi @xuxif, InceptionV3 works on images with different input sizes, so we don't need to do any special resizing/processing of our pileup images. [This file](https://github.com/google/deepvariant/blob/r1.1/deepvariant/modeling.py) contains the modeling code if you're interested.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/406
https://github.com/google/deepvariant/issues/406:216,energy efficiency,model,modeling,216,"Hi @xuxif, InceptionV3 works on images with different input sizes, so we don't need to do any special resizing/processing of our pileup images. [This file](https://github.com/google/deepvariant/blob/r1.1/deepvariant/modeling.py) contains the modeling code if you're interested.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/406
https://github.com/google/deepvariant/issues/406:242,energy efficiency,model,modeling,242,"Hi @xuxif, InceptionV3 works on images with different input sizes, so we don't need to do any special resizing/processing of our pileup images. [This file](https://github.com/google/deepvariant/blob/r1.1/deepvariant/modeling.py) contains the modeling code if you're interested.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/406
https://github.com/google/deepvariant/issues/406:54,safety,input,input,54,"Hi @xuxif, InceptionV3 works on images with different input sizes, so we don't need to do any special resizing/processing of our pileup images. [This file](https://github.com/google/deepvariant/blob/r1.1/deepvariant/modeling.py) contains the modeling code if you're interested.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/406
https://github.com/google/deepvariant/issues/406:216,security,model,modeling,216,"Hi @xuxif, InceptionV3 works on images with different input sizes, so we don't need to do any special resizing/processing of our pileup images. [This file](https://github.com/google/deepvariant/blob/r1.1/deepvariant/modeling.py) contains the modeling code if you're interested.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/406
https://github.com/google/deepvariant/issues/406:242,security,model,modeling,242,"Hi @xuxif, InceptionV3 works on images with different input sizes, so we don't need to do any special resizing/processing of our pileup images. [This file](https://github.com/google/deepvariant/blob/r1.1/deepvariant/modeling.py) contains the modeling code if you're interested.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/406
https://github.com/google/deepvariant/issues/406:54,usability,input,input,54,"Hi @xuxif, InceptionV3 works on images with different input sizes, so we don't need to do any special resizing/processing of our pileup images. [This file](https://github.com/google/deepvariant/blob/r1.1/deepvariant/modeling.py) contains the modeling code if you're interested.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/406
https://github.com/google/deepvariant/issues/408:192,availability,error,error,192,"Hi @husamia . The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:68,deployability,stage,stage,68,"Hi @husamia . The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:148,deployability,stage,stage,148,"Hi @husamia . The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:252,deployability,stage,stage,252,"Hi @husamia . The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:183,interoperability,standard,standard,183,"Hi @husamia . The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:192,performance,error,error,192,"Hi @husamia . The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:225,performance,time,time,225,"Hi @husamia . The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:192,safety,error,error,192,"Hi @husamia . The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:261,security,assess,assess,261,"Hi @husamia . The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:192,usability,error,error,192,"Hi @husamia . The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:199,availability,error,error,199,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:75,deployability,stage,stage,75,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:155,deployability,stage,stage,155,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:259,deployability,stage,stage,259,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:345,deployability,stage,stage,345,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:354,deployability,depend,dependent,354,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:354,integrability,depend,dependent,354,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:190,interoperability,standard,standard,190,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:354,modifiability,depend,dependent,354,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:199,performance,error,error,199,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:232,performance,time,time,232,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:312,performance,perform,performance,312,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:371,performance,disk,disk,371,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:199,safety,error,error,199,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:354,safety,depend,dependent,354,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:268,security,assess,assess,268,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:354,testability,depend,dependent,354,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:199,usability,error,error,199,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:312,usability,perform,performance,312,"> Hi @husamia. > . > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/410:144,deployability,observ,observed,144,"Hi @ASLeonard . For the hybrid model, we do not use haplotagged BAMs in the training process. In our initial benchmarks of the hybrid model, we observed a similar accuracy when training with haplotag as compared to training without it. Since haplotagging is an additional step, we decided not to include haplotag in the hybrid model. If you provide a haplotagged BAM to the hybrid model, it will still be able to read the BAM and should result in the same accuracy as when providing an untagged BAM, since the parsing of the haplotags is an additional step which is enabled by a flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/410
https://github.com/google/deepvariant/issues/410:31,energy efficiency,model,model,31,"Hi @ASLeonard . For the hybrid model, we do not use haplotagged BAMs in the training process. In our initial benchmarks of the hybrid model, we observed a similar accuracy when training with haplotag as compared to training without it. Since haplotagging is an additional step, we decided not to include haplotag in the hybrid model. If you provide a haplotagged BAM to the hybrid model, it will still be able to read the BAM and should result in the same accuracy as when providing an untagged BAM, since the parsing of the haplotags is an additional step which is enabled by a flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/410
https://github.com/google/deepvariant/issues/410:134,energy efficiency,model,model,134,"Hi @ASLeonard . For the hybrid model, we do not use haplotagged BAMs in the training process. In our initial benchmarks of the hybrid model, we observed a similar accuracy when training with haplotag as compared to training without it. Since haplotagging is an additional step, we decided not to include haplotag in the hybrid model. If you provide a haplotagged BAM to the hybrid model, it will still be able to read the BAM and should result in the same accuracy as when providing an untagged BAM, since the parsing of the haplotags is an additional step which is enabled by a flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/410
https://github.com/google/deepvariant/issues/410:327,energy efficiency,model,model,327,"Hi @ASLeonard . For the hybrid model, we do not use haplotagged BAMs in the training process. In our initial benchmarks of the hybrid model, we observed a similar accuracy when training with haplotag as compared to training without it. Since haplotagging is an additional step, we decided not to include haplotag in the hybrid model. If you provide a haplotagged BAM to the hybrid model, it will still be able to read the BAM and should result in the same accuracy as when providing an untagged BAM, since the parsing of the haplotags is an additional step which is enabled by a flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/410
https://github.com/google/deepvariant/issues/410:381,energy efficiency,model,model,381,"Hi @ASLeonard . For the hybrid model, we do not use haplotagged BAMs in the training process. In our initial benchmarks of the hybrid model, we observed a similar accuracy when training with haplotag as compared to training without it. Since haplotagging is an additional step, we decided not to include haplotag in the hybrid model. If you provide a haplotagged BAM to the hybrid model, it will still be able to read the BAM and should result in the same accuracy as when providing an untagged BAM, since the parsing of the haplotags is an additional step which is enabled by a flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/410
https://github.com/google/deepvariant/issues/410:31,security,model,model,31,"Hi @ASLeonard . For the hybrid model, we do not use haplotagged BAMs in the training process. In our initial benchmarks of the hybrid model, we observed a similar accuracy when training with haplotag as compared to training without it. Since haplotagging is an additional step, we decided not to include haplotag in the hybrid model. If you provide a haplotagged BAM to the hybrid model, it will still be able to read the BAM and should result in the same accuracy as when providing an untagged BAM, since the parsing of the haplotags is an additional step which is enabled by a flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/410
https://github.com/google/deepvariant/issues/410:134,security,model,model,134,"Hi @ASLeonard . For the hybrid model, we do not use haplotagged BAMs in the training process. In our initial benchmarks of the hybrid model, we observed a similar accuracy when training with haplotag as compared to training without it. Since haplotagging is an additional step, we decided not to include haplotag in the hybrid model. If you provide a haplotagged BAM to the hybrid model, it will still be able to read the BAM and should result in the same accuracy as when providing an untagged BAM, since the parsing of the haplotags is an additional step which is enabled by a flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/410
https://github.com/google/deepvariant/issues/410:327,security,model,model,327,"Hi @ASLeonard . For the hybrid model, we do not use haplotagged BAMs in the training process. In our initial benchmarks of the hybrid model, we observed a similar accuracy when training with haplotag as compared to training without it. Since haplotagging is an additional step, we decided not to include haplotag in the hybrid model. If you provide a haplotagged BAM to the hybrid model, it will still be able to read the BAM and should result in the same accuracy as when providing an untagged BAM, since the parsing of the haplotags is an additional step which is enabled by a flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/410
https://github.com/google/deepvariant/issues/410:381,security,model,model,381,"Hi @ASLeonard . For the hybrid model, we do not use haplotagged BAMs in the training process. In our initial benchmarks of the hybrid model, we observed a similar accuracy when training with haplotag as compared to training without it. Since haplotagging is an additional step, we decided not to include haplotag in the hybrid model. If you provide a haplotagged BAM to the hybrid model, it will still be able to read the BAM and should result in the same accuracy as when providing an untagged BAM, since the parsing of the haplotags is an additional step which is enabled by a flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/410
https://github.com/google/deepvariant/issues/410:144,testability,observ,observed,144,"Hi @ASLeonard . For the hybrid model, we do not use haplotagged BAMs in the training process. In our initial benchmarks of the hybrid model, we observed a similar accuracy when training with haplotag as compared to training without it. Since haplotagging is an additional step, we decided not to include haplotag in the hybrid model. If you provide a haplotagged BAM to the hybrid model, it will still be able to read the BAM and should result in the same accuracy as when providing an untagged BAM, since the parsing of the haplotags is an additional step which is enabled by a flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/410
https://github.com/google/deepvariant/issues/411:86,deployability,patch,patch,86,"The solution in this case was to use the original GRCh38 reference genome, not GRCh38 patch 12.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/411
https://github.com/google/deepvariant/issues/411:86,safety,patch,patch,86,"The solution in this case was to use the original GRCh38 reference genome, not GRCh38 patch 12.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/411
https://github.com/google/deepvariant/issues/411:86,security,patch,patch,86,"The solution in this case was to use the original GRCh38 reference genome, not GRCh38 patch 12.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/411
https://github.com/google/deepvariant/issues/412:124,availability,error,error,124,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:36,deployability,log,logs,36,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:130,integrability,messag,messages,130,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:130,interoperability,messag,messages,130,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:124,performance,error,error,124,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:36,safety,log,logs,36,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:124,safety,error,error,124,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:36,security,log,logs,36,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:36,testability,log,logs,36,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:92,testability,Trace,Traceback,92,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:124,usability,error,error,124,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:69,performance,memor,memory,69,@husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:107,performance,memor,memory,107,@husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:69,usability,memor,memory,69,@husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:107,usability,memor,memory,107,@husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:126,availability,error,error,126,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:38,deployability,log,logs,38,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:155,deployability,contain,container,155,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:132,integrability,messag,messages,132,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:132,interoperability,messag,messages,132,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:328,interoperability,specif,specify,328,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:126,performance,error,error,126,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:239,performance,memor,memory,239,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:277,performance,memor,memory,277,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:38,safety,log,logs,38,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:126,safety,error,error,126,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:38,security,log,logs,38,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:38,testability,log,logs,38,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:94,testability,Trace,Traceback,94,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:126,usability,error,error,126,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:144,usability,close,closed,144,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:239,usability,memor,memory,239,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:277,usability,memor,memory,277,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages? I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:48,usability,help,help,48,"@husamia I don't think setting temp folder will help with OOM problem. Can you try running with a machine with more RAM, or maybe trying running on a smaller region first (which will not solve the OOM problem but can at least confirm that it's possible to finish a smaller run).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:226,usability,confirm,confirm,226,"@husamia I don't think setting temp folder will help with OOM problem. Can you try running with a machine with more RAM, or maybe trying running on a smaller region first (which will not solve the OOM problem but can at least confirm that it's possible to finish a smaller run).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:50,usability,help,help,50,"> @husamia I don't think setting temp folder will help with OOM problem. Can you try running with a machine with more RAM, or maybe trying running on a smaller region first (which will not solve the OOM problem but can at least confirm that it's possible to finish a smaller run). I am running it on machine with 64GB of ram of which 80% is used by the Docker. I dont usually have problems with this setup with WGS data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:228,usability,confirm,confirm,228,"> @husamia I don't think setting temp folder will help with OOM problem. Can you try running with a machine with more RAM, or maybe trying running on a smaller region first (which will not solve the OOM problem but can at least confirm that it's possible to finish a smaller run). I am running it on machine with 64GB of ram of which 80% is used by the Docker. I dont usually have problems with this setup with WGS data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:40,availability,reliab,reliable,40,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:581,deployability,resourc,resources,581,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:630,deployability,resourc,resource,630,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:748,deployability,resourc,resource,748,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:757,deployability,manag,management,757,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:581,energy efficiency,resourc,resources,581,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:630,energy efficiency,resourc,resource,630,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:748,energy efficiency,resourc,resource,748,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:757,energy efficiency,manag,management,757,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:122,integrability,abstract,abstraction,122,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:252,integrability,abstract,abstractions,252,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:335,integrability,abstract,abstraction,335,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:400,integrability,Sub,Subsystem,400,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:439,integrability,abstract,abstractions,439,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:639,integrability,queue,queue,639,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:112,modifiability,layer,layers,112,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:122,modifiability,abstract,abstraction,122,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:242,modifiability,layer,layers,242,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:252,modifiability,abstract,abstractions,252,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:335,modifiability,abstract,abstraction,335,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:439,modifiability,abstract,abstractions,439,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:738,modifiability,layer,layers,738,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:98,performance,perform,performs,98,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:581,performance,resourc,resources,581,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:630,performance,resourc,resource,630,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:639,performance,queue,queue,639,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:748,performance,resourc,resource,748,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:40,reliability,reliab,reliable,40,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:581,safety,resourc,resources,581,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:630,safety,resourc,resource,630,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:707,safety,prevent,prevented,707,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:748,safety,resourc,resource,748,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:757,safety,manag,management,757,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:707,security,preven,prevented,707,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:581,testability,resourc,resources,581,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:630,testability,resourc,resource,630,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:748,testability,resourc,resource,748,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:98,usability,perform,performs,98,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:648,usability,clear,clear,648,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:683,usability,statu,status,683,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:23,usability,help,help,23,Thanks @pgrosu for the help! @husamia Feel free to open another issue if you have further questions. I'll close this now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:106,usability,close,close,106,Thanks @pgrosu for the help! @husamia Feel free to open another issue if you have further questions. I'll close this now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:62,deployability,releas,released,62,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:326,deployability,updat,updated,326,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:486,deployability,contain,containing,486,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:91,energy efficiency,model,model,91,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:39,performance,time,time,39,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:371,performance,memor,memory,371,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:326,safety,updat,updated,326,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:91,security,model,model,91,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:326,security,updat,updated,326,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:27,usability,close,closed,27,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:371,usability,memor,memory,371,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:378,usability,efficien,efficient,378,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:524,usability,efficien,efficiently,524,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:223,deployability,releas,released,223,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:499,deployability,updat,updated,499,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:663,deployability,contain,containing,663,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:17,energy efficiency,model,model,17,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:252,energy efficiency,model,model,252,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:1026,integrability,Messag,Message,1026,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:1026,interoperability,Messag,Message,1026,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:197,performance,time,time,197,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:544,performance,memor,memory,544,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:0,reliability,Doe,Does,0,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:499,safety,updat,updated,499,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:17,security,model,model,17,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:252,security,model,model,252,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:499,security,updat,updated,499,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:918,security,auth,auth,918,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:185,usability,close,closed,185,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:544,usability,memor,memory,544,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:551,usability,efficien,efficient,551,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/412:701,usability,efficien,efficiently,701,"Does the RNA-Seq model work with BAMs created with HISAT2? On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>. wrote:. > @husamia <https://github.com/husamia> although this was closed some time. > ago, we have just released an Illumina RNA-seq model and case study. > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>. > If you are still interested in calling variants from RNA-seq data using. > DeepVariant, this should work for you. >. > We have also updated the DeepVariant code base to be more memory efficient. > with RNA-seq data. This involves passing a new flag (--split_skip_reads),. > that allows for reads containing large SKIP to be processed efficiently. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>. > . > You are receiving this because you were mentioned.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/413:33,availability,error,error,33,"Hi @leorippel . In your log, the error says:. ```. 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. ```. Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:24,deployability,log,log,24,"Hi @leorippel . In your log, the error says:. ```. 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. ```. Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:322,interoperability,format,format,322,"Hi @leorippel . In your log, the error says:. ```. 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. ```. Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:33,performance,error,error,33,"Hi @leorippel . In your log, the error says:. ```. 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. ```. Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:24,safety,log,log,24,"Hi @leorippel . In your log, the error says:. ```. 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. ```. Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:33,safety,error,error,33,"Hi @leorippel . In your log, the error says:. ```. 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. ```. Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:128,safety,input,input,128,"Hi @leorippel . In your log, the error says:. ```. 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. ```. Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:24,security,log,log,24,"Hi @leorippel . In your log, the error says:. ```. 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. ```. Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:24,testability,log,log,24,"Hi @leorippel . In your log, the error says:. ```. 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. ```. Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:33,usability,error,error,33,"Hi @leorippel . In your log, the error says:. ```. 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. ```. Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:128,usability,input,input,128,"Hi @leorippel . In your log, the error says:. ```. 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. ```. Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:207,usability,confirm,confirm,207,"Hi @leorippel . In your log, the error says:. ```. 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. ```. Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:48,usability,help,help,48,@leorippel Please let us know if you still need help with this issue. I'll keep this open for a few more days.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:40,availability,error,error,40,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:31,deployability,log,log,31,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:345,interoperability,format,format,345,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:40,performance,error,error,40,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:31,safety,log,log,31,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:40,safety,error,error,40,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:143,safety,input,input,143,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:451,safety,input,input,451,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:501,safety,input,input,501,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:31,security,log,log,31,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:31,testability,log,log,31,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:532,testability,understand,understand,532,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:40,usability,error,error,40,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:143,usability,input,input,143,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:230,usability,confirm,confirm,230,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:451,usability,input,input,451,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:457,usability,command,command,457,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:501,usability,input,input,501,"> Hi @leorippel. > . > In your log, the error says:. > . > ```. > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory. > ```. > . > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument? Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:151,deployability,contain,contain,151,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:428,deployability,contain,contain,428,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:1048,energy efficiency,CPU,CPUs,1048,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:1094,energy efficiency,core,cores,1094,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:1218,energy efficiency,CPU,CPU,1218,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:17,interoperability,specif,specification,17,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:1009,performance,parallel,parallelizing,1009,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:1048,performance,CPU,CPUs,1048,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:1074,performance,parallel,parallelizes,1074,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:1105,performance,perform,performing,1105,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:1218,performance,CPU,CPU,1218,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:69,safety,input,input,69,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:271,safety,input,input,271,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:285,safety,input,input,285,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:325,safety,input,input,325,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:365,safety,input,input,365,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:69,usability,input,input,69,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:271,usability,input,input,271,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:285,usability,input,input,285,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:325,usability,input,input,325,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:365,usability,input,input,365,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:1105,usability,perform,performing,1105,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```. /input/gvcf.tfrecord-00000-of-00030.gz. /input/gvcf.tfrecord-00001-of-00030.gz. /input/gvcf.tfrecord-00002-of-00030.gz. ... ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:506,availability,error,errors,506,"I was able to run `make_examples` multiple times and have `call_variants` use multiple sharded tfrecord files at once. But I guess from this thread that the same is not possible for `postprocess_variants`? If I run:. ```. postprocess_variants --ref Homo_sapiens.GRCh37.dna.primary_assembly.fa \. --infile call_variants_mysample.tfrecord.gz --outfile mysample.vcf.gz \. --nonvariant_site_tfrecord_path gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord@32.gz \. --gvcf_outfile mysample.gvcf.gz. ```. I get errors like:. ```. gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord-00031-of-00032.gz; No such file or directory. ```. So is the solution to loop through (or run in parallel) my gvcf tfrecord files, output multiple gvcfs, and then concat them with bcftools? FYI, the `postprocess_variants` help page from docker://google/deepvariant:deeptrio-1.5.0-gpu implies that multiple tfrecord files will be accepted:. ```. --nonvariant_site_tfrecord_path: Optional. Path(s) to the non-variant sites protos in TFRecord format to convert to gVCF file. This should be the complete set of outputs from the --gvcf flag of make_examples.py. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:857,energy efficiency,gpu,gpu,857,"I was able to run `make_examples` multiple times and have `call_variants` use multiple sharded tfrecord files at once. But I guess from this thread that the same is not possible for `postprocess_variants`? If I run:. ```. postprocess_variants --ref Homo_sapiens.GRCh37.dna.primary_assembly.fa \. --infile call_variants_mysample.tfrecord.gz --outfile mysample.vcf.gz \. --nonvariant_site_tfrecord_path gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord@32.gz \. --gvcf_outfile mysample.gvcf.gz. ```. I get errors like:. ```. gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord-00031-of-00032.gz; No such file or directory. ```. So is the solution to loop through (or run in parallel) my gvcf tfrecord files, output multiple gvcfs, and then concat them with bcftools? FYI, the `postprocess_variants` help page from docker://google/deepvariant:deeptrio-1.5.0-gpu implies that multiple tfrecord files will be accepted:. ```. --nonvariant_site_tfrecord_path: Optional. Path(s) to the non-variant sites protos in TFRecord format to convert to gVCF file. This should be the complete set of outputs from the --gvcf flag of make_examples.py. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:1017,interoperability,format,format,1017,"I was able to run `make_examples` multiple times and have `call_variants` use multiple sharded tfrecord files at once. But I guess from this thread that the same is not possible for `postprocess_variants`? If I run:. ```. postprocess_variants --ref Homo_sapiens.GRCh37.dna.primary_assembly.fa \. --infile call_variants_mysample.tfrecord.gz --outfile mysample.vcf.gz \. --nonvariant_site_tfrecord_path gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord@32.gz \. --gvcf_outfile mysample.gvcf.gz. ```. I get errors like:. ```. gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord-00031-of-00032.gz; No such file or directory. ```. So is the solution to loop through (or run in parallel) my gvcf tfrecord files, output multiple gvcfs, and then concat them with bcftools? FYI, the `postprocess_variants` help page from docker://google/deepvariant:deeptrio-1.5.0-gpu implies that multiple tfrecord files will be accepted:. ```. --nonvariant_site_tfrecord_path: Optional. Path(s) to the non-variant sites protos in TFRecord format to convert to gVCF file. This should be the complete set of outputs from the --gvcf flag of make_examples.py. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:43,performance,time,times,43,"I was able to run `make_examples` multiple times and have `call_variants` use multiple sharded tfrecord files at once. But I guess from this thread that the same is not possible for `postprocess_variants`? If I run:. ```. postprocess_variants --ref Homo_sapiens.GRCh37.dna.primary_assembly.fa \. --infile call_variants_mysample.tfrecord.gz --outfile mysample.vcf.gz \. --nonvariant_site_tfrecord_path gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord@32.gz \. --gvcf_outfile mysample.gvcf.gz. ```. I get errors like:. ```. gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord-00031-of-00032.gz; No such file or directory. ```. So is the solution to loop through (or run in parallel) my gvcf tfrecord files, output multiple gvcfs, and then concat them with bcftools? FYI, the `postprocess_variants` help page from docker://google/deepvariant:deeptrio-1.5.0-gpu implies that multiple tfrecord files will be accepted:. ```. --nonvariant_site_tfrecord_path: Optional. Path(s) to the non-variant sites protos in TFRecord format to convert to gVCF file. This should be the complete set of outputs from the --gvcf flag of make_examples.py. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:506,performance,error,errors,506,"I was able to run `make_examples` multiple times and have `call_variants` use multiple sharded tfrecord files at once. But I guess from this thread that the same is not possible for `postprocess_variants`? If I run:. ```. postprocess_variants --ref Homo_sapiens.GRCh37.dna.primary_assembly.fa \. --infile call_variants_mysample.tfrecord.gz --outfile mysample.vcf.gz \. --nonvariant_site_tfrecord_path gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord@32.gz \. --gvcf_outfile mysample.gvcf.gz. ```. I get errors like:. ```. gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord-00031-of-00032.gz; No such file or directory. ```. So is the solution to loop through (or run in parallel) my gvcf tfrecord files, output multiple gvcfs, and then concat them with bcftools? FYI, the `postprocess_variants` help page from docker://google/deepvariant:deeptrio-1.5.0-gpu implies that multiple tfrecord files will be accepted:. ```. --nonvariant_site_tfrecord_path: Optional. Path(s) to the non-variant sites protos in TFRecord format to convert to gVCF file. This should be the complete set of outputs from the --gvcf flag of make_examples.py. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:674,performance,parallel,parallel,674,"I was able to run `make_examples` multiple times and have `call_variants` use multiple sharded tfrecord files at once. But I guess from this thread that the same is not possible for `postprocess_variants`? If I run:. ```. postprocess_variants --ref Homo_sapiens.GRCh37.dna.primary_assembly.fa \. --infile call_variants_mysample.tfrecord.gz --outfile mysample.vcf.gz \. --nonvariant_site_tfrecord_path gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord@32.gz \. --gvcf_outfile mysample.gvcf.gz. ```. I get errors like:. ```. gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord-00031-of-00032.gz; No such file or directory. ```. So is the solution to loop through (or run in parallel) my gvcf tfrecord files, output multiple gvcfs, and then concat them with bcftools? FYI, the `postprocess_variants` help page from docker://google/deepvariant:deeptrio-1.5.0-gpu implies that multiple tfrecord files will be accepted:. ```. --nonvariant_site_tfrecord_path: Optional. Path(s) to the non-variant sites protos in TFRecord format to convert to gVCF file. This should be the complete set of outputs from the --gvcf flag of make_examples.py. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:857,performance,gpu,gpu,857,"I was able to run `make_examples` multiple times and have `call_variants` use multiple sharded tfrecord files at once. But I guess from this thread that the same is not possible for `postprocess_variants`? If I run:. ```. postprocess_variants --ref Homo_sapiens.GRCh37.dna.primary_assembly.fa \. --infile call_variants_mysample.tfrecord.gz --outfile mysample.vcf.gz \. --nonvariant_site_tfrecord_path gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord@32.gz \. --gvcf_outfile mysample.gvcf.gz. ```. I get errors like:. ```. gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord-00031-of-00032.gz; No such file or directory. ```. So is the solution to loop through (or run in parallel) my gvcf tfrecord files, output multiple gvcfs, and then concat them with bcftools? FYI, the `postprocess_variants` help page from docker://google/deepvariant:deeptrio-1.5.0-gpu implies that multiple tfrecord files will be accepted:. ```. --nonvariant_site_tfrecord_path: Optional. Path(s) to the non-variant sites protos in TFRecord format to convert to gVCF file. This should be the complete set of outputs from the --gvcf flag of make_examples.py. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:506,safety,error,errors,506,"I was able to run `make_examples` multiple times and have `call_variants` use multiple sharded tfrecord files at once. But I guess from this thread that the same is not possible for `postprocess_variants`? If I run:. ```. postprocess_variants --ref Homo_sapiens.GRCh37.dna.primary_assembly.fa \. --infile call_variants_mysample.tfrecord.gz --outfile mysample.vcf.gz \. --nonvariant_site_tfrecord_path gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord@32.gz \. --gvcf_outfile mysample.gvcf.gz. ```. I get errors like:. ```. gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord-00031-of-00032.gz; No such file or directory. ```. So is the solution to loop through (or run in parallel) my gvcf tfrecord files, output multiple gvcfs, and then concat them with bcftools? FYI, the `postprocess_variants` help page from docker://google/deepvariant:deeptrio-1.5.0-gpu implies that multiple tfrecord files will be accepted:. ```. --nonvariant_site_tfrecord_path: Optional. Path(s) to the non-variant sites protos in TFRecord format to convert to gVCF file. This should be the complete set of outputs from the --gvcf flag of make_examples.py. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:1068,safety,compl,complete,1068,"I was able to run `make_examples` multiple times and have `call_variants` use multiple sharded tfrecord files at once. But I guess from this thread that the same is not possible for `postprocess_variants`? If I run:. ```. postprocess_variants --ref Homo_sapiens.GRCh37.dna.primary_assembly.fa \. --infile call_variants_mysample.tfrecord.gz --outfile mysample.vcf.gz \. --nonvariant_site_tfrecord_path gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord@32.gz \. --gvcf_outfile mysample.gvcf.gz. ```. I get errors like:. ```. gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord-00031-of-00032.gz; No such file or directory. ```. So is the solution to loop through (or run in parallel) my gvcf tfrecord files, output multiple gvcfs, and then concat them with bcftools? FYI, the `postprocess_variants` help page from docker://google/deepvariant:deeptrio-1.5.0-gpu implies that multiple tfrecord files will be accepted:. ```. --nonvariant_site_tfrecord_path: Optional. Path(s) to the non-variant sites protos in TFRecord format to convert to gVCF file. This should be the complete set of outputs from the --gvcf flag of make_examples.py. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:1068,security,compl,complete,1068,"I was able to run `make_examples` multiple times and have `call_variants` use multiple sharded tfrecord files at once. But I guess from this thread that the same is not possible for `postprocess_variants`? If I run:. ```. postprocess_variants --ref Homo_sapiens.GRCh37.dna.primary_assembly.fa \. --infile call_variants_mysample.tfrecord.gz --outfile mysample.vcf.gz \. --nonvariant_site_tfrecord_path gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord@32.gz \. --gvcf_outfile mysample.gvcf.gz. ```. I get errors like:. ```. gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord-00031-of-00032.gz; No such file or directory. ```. So is the solution to loop through (or run in parallel) my gvcf tfrecord files, output multiple gvcfs, and then concat them with bcftools? FYI, the `postprocess_variants` help page from docker://google/deepvariant:deeptrio-1.5.0-gpu implies that multiple tfrecord files will be accepted:. ```. --nonvariant_site_tfrecord_path: Optional. Path(s) to the non-variant sites protos in TFRecord format to convert to gVCF file. This should be the complete set of outputs from the --gvcf flag of make_examples.py. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:506,usability,error,errors,506,"I was able to run `make_examples` multiple times and have `call_variants` use multiple sharded tfrecord files at once. But I guess from this thread that the same is not possible for `postprocess_variants`? If I run:. ```. postprocess_variants --ref Homo_sapiens.GRCh37.dna.primary_assembly.fa \. --infile call_variants_mysample.tfrecord.gz --outfile mysample.vcf.gz \. --nonvariant_site_tfrecord_path gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord@32.gz \. --gvcf_outfile mysample.gvcf.gz. ```. I get errors like:. ```. gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord-00031-of-00032.gz; No such file or directory. ```. So is the solution to loop through (or run in parallel) my gvcf tfrecord files, output multiple gvcfs, and then concat them with bcftools? FYI, the `postprocess_variants` help page from docker://google/deepvariant:deeptrio-1.5.0-gpu implies that multiple tfrecord files will be accepted:. ```. --nonvariant_site_tfrecord_path: Optional. Path(s) to the non-variant sites protos in TFRecord format to convert to gVCF file. This should be the complete set of outputs from the --gvcf flag of make_examples.py. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:799,usability,help,help,799,"I was able to run `make_examples` multiple times and have `call_variants` use multiple sharded tfrecord files at once. But I guess from this thread that the same is not possible for `postprocess_variants`? If I run:. ```. postprocess_variants --ref Homo_sapiens.GRCh37.dna.primary_assembly.fa \. --infile call_variants_mysample.tfrecord.gz --outfile mysample.vcf.gz \. --nonvariant_site_tfrecord_path gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord@32.gz \. --gvcf_outfile mysample.gvcf.gz. ```. I get errors like:. ```. gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord-00031-of-00032.gz; No such file or directory. ```. So is the solution to loop through (or run in parallel) my gvcf tfrecord files, output multiple gvcfs, and then concat them with bcftools? FYI, the `postprocess_variants` help page from docker://google/deepvariant:deeptrio-1.5.0-gpu implies that multiple tfrecord files will be accepted:. ```. --nonvariant_site_tfrecord_path: Optional. Path(s) to the non-variant sites protos in TFRecord format to convert to gVCF file. This should be the complete set of outputs from the --gvcf flag of make_examples.py. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:285,energy efficiency,current,currently,285,"Hi @lvclark . For `--nonvariant_site_tfrecord_path`, multiple tfrecord files will be accepted, but not in a comma-delimited list. You can use the sharded form (like gvcf1_parent1.tfrecord@32.gz). And I think you can use wildcard (like gvcf1_parent1.tfrecord@.gz). But the code doesn't currently split a comma-delimited list for that argument. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:277,reliability,doe,doesn,277,"Hi @lvclark . For `--nonvariant_site_tfrecord_path`, multiple tfrecord files will be accepted, but not in a comma-delimited list. You can use the sharded form (like gvcf1_parent1.tfrecord@32.gz). And I think you can use wildcard (like gvcf1_parent1.tfrecord@.gz). But the code doesn't currently split a comma-delimited list for that argument. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:353,usability,help,helps,353,"Hi @lvclark . For `--nonvariant_site_tfrecord_path`, multiple tfrecord files will be accepted, but not in a comma-delimited list. You can use the sharded form (like gvcf1_parent1.tfrecord@32.gz). And I think you can use wildcard (like gvcf1_parent1.tfrecord@.gz). But the code doesn't currently split a comma-delimited list for that argument. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:130,deployability,pipelin,pipeline,130,"Thanks @lvclark for the context of why you're doing this. That's very helpful to know. In that case, you'll want to run the whole pipeline separately as well (meaning, make_examples -> call_variants -> postprocess_variants separately), one using `gvcf1_parent1.tfrecord@32.gz` , the other one with `gvcf3_parent1.tfrecord@32.gz`. And not to combine them mid-way. Our team is working on makeing chrX/Y calling a bit better for DeepVariant and DeepTrio. I'll make sure to pass this feedback as well so we'll think about the use case here a bit more later on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:130,integrability,pipelin,pipeline,130,"Thanks @lvclark for the context of why you're doing this. That's very helpful to know. In that case, you'll want to run the whole pipeline separately as well (meaning, make_examples -> call_variants -> postprocess_variants separately), one using `gvcf1_parent1.tfrecord@32.gz` , the other one with `gvcf3_parent1.tfrecord@32.gz`. And not to combine them mid-way. Our team is working on makeing chrX/Y calling a bit better for DeepVariant and DeepTrio. I'll make sure to pass this feedback as well so we'll think about the use case here a bit more later on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:367,security,team,team,367,"Thanks @lvclark for the context of why you're doing this. That's very helpful to know. In that case, you'll want to run the whole pipeline separately as well (meaning, make_examples -> call_variants -> postprocess_variants separately), one using `gvcf1_parent1.tfrecord@32.gz` , the other one with `gvcf3_parent1.tfrecord@32.gz`. And not to combine them mid-way. Our team is working on makeing chrX/Y calling a bit better for DeepVariant and DeepTrio. I'll make sure to pass this feedback as well so we'll think about the use case here a bit more later on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:24,testability,context,context,24,"Thanks @lvclark for the context of why you're doing this. That's very helpful to know. In that case, you'll want to run the whole pipeline separately as well (meaning, make_examples -> call_variants -> postprocess_variants separately), one using `gvcf1_parent1.tfrecord@32.gz` , the other one with `gvcf3_parent1.tfrecord@32.gz`. And not to combine them mid-way. Our team is working on makeing chrX/Y calling a bit better for DeepVariant and DeepTrio. I'll make sure to pass this feedback as well so we'll think about the use case here a bit more later on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:70,usability,help,helpful,70,"Thanks @lvclark for the context of why you're doing this. That's very helpful to know. In that case, you'll want to run the whole pipeline separately as well (meaning, make_examples -> call_variants -> postprocess_variants separately), one using `gvcf1_parent1.tfrecord@32.gz` , the other one with `gvcf3_parent1.tfrecord@32.gz`. And not to combine them mid-way. Our team is working on makeing chrX/Y calling a bit better for DeepVariant and DeepTrio. I'll make sure to pass this feedback as well so we'll think about the use case here a bit more later on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:480,usability,feedback,feedback,480,"Thanks @lvclark for the context of why you're doing this. That's very helpful to know. In that case, you'll want to run the whole pipeline separately as well (meaning, make_examples -> call_variants -> postprocess_variants separately), one using `gvcf1_parent1.tfrecord@32.gz` , the other one with `gvcf3_parent1.tfrecord@32.gz`. And not to combine them mid-way. Our team is working on makeing chrX/Y calling a bit better for DeepVariant and DeepTrio. I'll make sure to pass this feedback as well so we'll think about the use case here a bit more later on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/414:152,deployability,contain,container,152,"Hi @brentp . This is a reasonable request, and CRAM->BAM conversion is faster via a samtools command. I don't think it will add much to the size of the container to include this, so we'll look at doing this, and will probably plan to do so, likely in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:260,deployability,releas,release,260,"Hi @brentp . This is a reasonable request, and CRAM->BAM conversion is faster via a samtools command. I don't think it will add much to the size of the container to include this, so we'll look at doing this, and will probably plan to do so, likely in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:57,interoperability,convers,conversion,57,"Hi @brentp . This is a reasonable request, and CRAM->BAM conversion is faster via a samtools command. I don't think it will add much to the size of the container to include this, so we'll look at doing this, and will probably plan to do so, likely in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:226,testability,plan,plan,226,"Hi @brentp . This is a reasonable request, and CRAM->BAM conversion is faster via a samtools command. I don't think it will add much to the size of the container to include this, so we'll look at doing this, and will probably plan to do so, likely in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:93,usability,command,command,93,"Hi @brentp . This is a reasonable request, and CRAM->BAM conversion is faster via a samtools command. I don't think it will add much to the size of the container to include this, so we'll look at doing this, and will probably plan to do so, likely in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:394,deployability,integr,integrated,394,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it. Adding it to our Docker image should be quite straightforward. I have one more question:. Can you provide a specific example usage you have in mind? . You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script. In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:394,integrability,integr,integrated,394,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it. Adding it to our Docker image should be quite straightforward. I have one more question:. Can you provide a specific example usage you have in mind? . You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script. In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:199,interoperability,specif,specific,199,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it. Adding it to our Docker image should be quite straightforward. I have one more question:. Can you provide a specific example usage you have in mind? . You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script. In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:394,interoperability,integr,integrated,394,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it. Adding it to our Docker image should be quite straightforward. I have one more question:. Can you provide a specific example usage you have in mind? . You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script. In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:394,modifiability,integr,integrated,394,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it. Adding it to our Docker image should be quite straightforward. I have one more question:. Can you provide a specific example usage you have in mind? . You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script. In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:394,reliability,integr,integrated,394,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it. Adding it to our Docker image should be quite straightforward. I have one more question:. Can you provide a specific example usage you have in mind? . You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script. In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:394,security,integr,integrated,394,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it. Adding it to our Docker image should be quite straightforward. I have one more question:. Can you provide a specific example usage you have in mind? . You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script. In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:394,testability,integr,integrated,394,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it. Adding it to our Docker image should be quite straightforward. I have one more question:. Can you provide a specific example usage you have in mind? . You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script. In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:538,usability,document,documentation,538,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it. Adding it to our Docker image should be quite straightforward. I have one more question:. Can you provide a specific example usage you have in mind? . You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script. In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:600,usability,user,users,600,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it. Adding it to our Docker image should be quite straightforward. I have one more question:. Can you provide a specific example usage you have in mind? . You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script. In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:254,energy efficiency,model,model,254,"Hi, I use it like this:. ```. samtools view --threads 32 -bT $fasta -o ${sample}.bam $cram. samtools index -@ 32 ${sample}.bam. /opt/deepvariant/bin/run_deepvariant \. --reads=${sample}.bam \. --intermediate_results_dir=$TMPDIR/$sample/ \. --model_type=$model \. --output_gvcf=${sample}.gvcf.gz \. --output_vcf=${sample}.vcf.gz \. --num_shards=48 \. --ref=$fasta. ```. so just having it in PATH for manual calling would be sufficient for my use-case. Though if a flag like --conv-cram-to-bam existed, I'd be happy to use that instead. thanks for the responses.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:254,security,model,model,254,"Hi, I use it like this:. ```. samtools view --threads 32 -bT $fasta -o ${sample}.bam $cram. samtools index -@ 32 ${sample}.bam. /opt/deepvariant/bin/run_deepvariant \. --reads=${sample}.bam \. --intermediate_results_dir=$TMPDIR/$sample/ \. --model_type=$model \. --output_gvcf=${sample}.gvcf.gz \. --output_vcf=${sample}.vcf.gz \. --num_shards=48 \. --ref=$fasta. ```. so just having it in PATH for manual calling would be sufficient for my use-case. Though if a flag like --conv-cram-to-bam existed, I'd be happy to use that instead. thanks for the responses.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:43,deployability,updat,update,43,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out. Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:78,deployability,releas,release,78,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out. Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:137,deployability,updat,update,137,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out. Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:43,safety,updat,update,43,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out. Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:137,safety,updat,update,137,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out. Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:43,security,updat,update,43,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out. Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:137,security,updat,update,137,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out. Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:92,usability,close,close,92,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out. Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/414:174,usability,feedback,feedback,174,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out. Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414
https://github.com/google/deepvariant/issues/415:1444,availability,error,error,1444,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:826,deployability,artifact,artifactual,826,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:121,energy efficiency,cloud,cloud,121,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:317,energy efficiency,model,model,317,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:474,energy efficiency,model,models,474,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:502,energy efficiency,model,model,502,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:613,energy efficiency,model,model,613,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1087,energy efficiency,reduc,reduce,1087,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1364,energy efficiency,model,model,1364,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1431,energy efficiency,estimat,estimate,1431,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:453,interoperability,specif,specific-deepvariant-models,453,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1202,interoperability,specif,specific,1202,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1060,modifiability,Pac,PacBio,1060,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1185,performance,perform,performance,1185,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1373,performance,perform,perform,1373,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1444,performance,error,error,1444,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1444,safety,error,error,1444,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:317,security,model,model,317,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:474,security,model,models,474,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:502,security,model,model,502,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:613,security,model,model,613,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1364,security,model,model,1364,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1320,testability,coverag,coverage,1320,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1185,usability,perform,performance,1185,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1373,usability,perform,perform,1373,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/415:1444,usability,error,error,1444,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well. * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415
https://github.com/google/deepvariant/issues/416:166,deployability,BUILD,BUILD,166,"I was just double checking the openvino issue in case I missed something obvious, but there are explicitly `openvino_estimator` references present in the deepvariant BUILD file, but not in the deeptrio BUILD file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:202,deployability,BUILD,BUILD,202,"I was just double checking the openvino issue in case I missed something obvious, but there are explicitly `openvino_estimator` references present in the deepvariant BUILD file, but not in the deeptrio BUILD file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:17,deployability,observ,observed,17,"@ASLeonard . The observed warning is not correct and is an issue we are working on fixing. Currently, DeepTrio will report that warning for all models except for WES. However, it does not impact DeepTrio inference. We are also planning on adding OpenVINO support to the next release of DeepTrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:275,deployability,releas,release,275,"@ASLeonard . The observed warning is not correct and is an issue we are working on fixing. Currently, DeepTrio will report that warning for all models except for WES. However, it does not impact DeepTrio inference. We are also planning on adding OpenVINO support to the next release of DeepTrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:91,energy efficiency,Current,Currently,91,"@ASLeonard . The observed warning is not correct and is an issue we are working on fixing. Currently, DeepTrio will report that warning for all models except for WES. However, it does not impact DeepTrio inference. We are also planning on adding OpenVINO support to the next release of DeepTrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:144,energy efficiency,model,models,144,"@ASLeonard . The observed warning is not correct and is an issue we are working on fixing. Currently, DeepTrio will report that warning for all models except for WES. However, it does not impact DeepTrio inference. We are also planning on adding OpenVINO support to the next release of DeepTrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:179,reliability,doe,does,179,"@ASLeonard . The observed warning is not correct and is an issue we are working on fixing. Currently, DeepTrio will report that warning for all models except for WES. However, it does not impact DeepTrio inference. We are also planning on adding OpenVINO support to the next release of DeepTrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:151,safety,except,except,151,"@ASLeonard . The observed warning is not correct and is an issue we are working on fixing. Currently, DeepTrio will report that warning for all models except for WES. However, it does not impact DeepTrio inference. We are also planning on adding OpenVINO support to the next release of DeepTrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:144,security,model,models,144,"@ASLeonard . The observed warning is not correct and is an issue we are working on fixing. Currently, DeepTrio will report that warning for all models except for WES. However, it does not impact DeepTrio inference. We are also planning on adding OpenVINO support to the next release of DeepTrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:17,testability,observ,observed,17,"@ASLeonard . The observed warning is not correct and is an issue we are working on fixing. Currently, DeepTrio will report that warning for all models except for WES. However, it does not impact DeepTrio inference. We are also planning on adding OpenVINO support to the next release of DeepTrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:227,testability,plan,planning,227,"@ASLeonard . The observed warning is not correct and is an issue we are working on fixing. Currently, DeepTrio will report that warning for all models except for WES. However, it does not impact DeepTrio inference. We are also planning on adding OpenVINO support to the next release of DeepTrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:255,usability,support,support,255,"@ASLeonard . The observed warning is not correct and is an issue we are working on fixing. Currently, DeepTrio will report that warning for all models except for WES. However, it does not impact DeepTrio inference. We are also planning on adding OpenVINO support to the next release of DeepTrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:48,interoperability,specif,specify,48,"Great, so it seems there is no need to manually specify `--pileup_image_height_*=100`. Thanks for the details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:68,reliability,doe,doesn,68,"Did openvino make it into deeptrio-1.2.0? From some quick tests, it doesn't look like it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:58,safety,test,tests,58,"Did openvino make it into deeptrio-1.2.0? From some quick tests, it doesn't look like it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:58,testability,test,tests,58,"Did openvino make it into deeptrio-1.2.0? From some quick tests, it doesn't look like it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:119,deployability,updat,updating,119,"Thanks @ASLeonard for checking in. It was originally on our roadmap to add OpenVINO to DeepTrio. However, when we were updating DeepVariant's metrics.md, we found that with OpenVINO runs roughly the same as without it. ( (We observed a pretty significant speedup in r1.2 using just intel-tensorflow without OpenVINO.). As a result, I didn't add OpenVINO to DeepTrio this time. @ASLeonard You can still find OpenVINO in our DeepVariant Docker image. If you still observe a noticeable speedup using OpenVINO using our DeepVariant setting, let me know. I'd really love to hear what type of machine you're using. FYI for @dkurt because this is OpenVINO related.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:225,deployability,observ,observed,225,"Thanks @ASLeonard for checking in. It was originally on our roadmap to add OpenVINO to DeepTrio. However, when we were updating DeepVariant's metrics.md, we found that with OpenVINO runs roughly the same as without it. ( (We observed a pretty significant speedup in r1.2 using just intel-tensorflow without OpenVINO.). As a result, I didn't add OpenVINO to DeepTrio this time. @ASLeonard You can still find OpenVINO in our DeepVariant Docker image. If you still observe a noticeable speedup using OpenVINO using our DeepVariant setting, let me know. I'd really love to hear what type of machine you're using. FYI for @dkurt because this is OpenVINO related.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:462,deployability,observ,observe,462,"Thanks @ASLeonard for checking in. It was originally on our roadmap to add OpenVINO to DeepTrio. However, when we were updating DeepVariant's metrics.md, we found that with OpenVINO runs roughly the same as without it. ( (We observed a pretty significant speedup in r1.2 using just intel-tensorflow without OpenVINO.). As a result, I didn't add OpenVINO to DeepTrio this time. @ASLeonard You can still find OpenVINO in our DeepVariant Docker image. If you still observe a noticeable speedup using OpenVINO using our DeepVariant setting, let me know. I'd really love to hear what type of machine you're using. FYI for @dkurt because this is OpenVINO related.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:371,performance,time,time,371,"Thanks @ASLeonard for checking in. It was originally on our roadmap to add OpenVINO to DeepTrio. However, when we were updating DeepVariant's metrics.md, we found that with OpenVINO runs roughly the same as without it. ( (We observed a pretty significant speedup in r1.2 using just intel-tensorflow without OpenVINO.). As a result, I didn't add OpenVINO to DeepTrio this time. @ASLeonard You can still find OpenVINO in our DeepVariant Docker image. If you still observe a noticeable speedup using OpenVINO using our DeepVariant setting, let me know. I'd really love to hear what type of machine you're using. FYI for @dkurt because this is OpenVINO related.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:119,safety,updat,updating,119,"Thanks @ASLeonard for checking in. It was originally on our roadmap to add OpenVINO to DeepTrio. However, when we were updating DeepVariant's metrics.md, we found that with OpenVINO runs roughly the same as without it. ( (We observed a pretty significant speedup in r1.2 using just intel-tensorflow without OpenVINO.). As a result, I didn't add OpenVINO to DeepTrio this time. @ASLeonard You can still find OpenVINO in our DeepVariant Docker image. If you still observe a noticeable speedup using OpenVINO using our DeepVariant setting, let me know. I'd really love to hear what type of machine you're using. FYI for @dkurt because this is OpenVINO related.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:119,security,updat,updating,119,"Thanks @ASLeonard for checking in. It was originally on our roadmap to add OpenVINO to DeepTrio. However, when we were updating DeepVariant's metrics.md, we found that with OpenVINO runs roughly the same as without it. ( (We observed a pretty significant speedup in r1.2 using just intel-tensorflow without OpenVINO.). As a result, I didn't add OpenVINO to DeepTrio this time. @ASLeonard You can still find OpenVINO in our DeepVariant Docker image. If you still observe a noticeable speedup using OpenVINO using our DeepVariant setting, let me know. I'd really love to hear what type of machine you're using. FYI for @dkurt because this is OpenVINO related.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:243,security,sign,significant,243,"Thanks @ASLeonard for checking in. It was originally on our roadmap to add OpenVINO to DeepTrio. However, when we were updating DeepVariant's metrics.md, we found that with OpenVINO runs roughly the same as without it. ( (We observed a pretty significant speedup in r1.2 using just intel-tensorflow without OpenVINO.). As a result, I didn't add OpenVINO to DeepTrio this time. @ASLeonard You can still find OpenVINO in our DeepVariant Docker image. If you still observe a noticeable speedup using OpenVINO using our DeepVariant setting, let me know. I'd really love to hear what type of machine you're using. FYI for @dkurt because this is OpenVINO related.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:225,testability,observ,observed,225,"Thanks @ASLeonard for checking in. It was originally on our roadmap to add OpenVINO to DeepTrio. However, when we were updating DeepVariant's metrics.md, we found that with OpenVINO runs roughly the same as without it. ( (We observed a pretty significant speedup in r1.2 using just intel-tensorflow without OpenVINO.). As a result, I didn't add OpenVINO to DeepTrio this time. @ASLeonard You can still find OpenVINO in our DeepVariant Docker image. If you still observe a noticeable speedup using OpenVINO using our DeepVariant setting, let me know. I'd really love to hear what type of machine you're using. FYI for @dkurt because this is OpenVINO related.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:462,testability,observ,observe,462,"Thanks @ASLeonard for checking in. It was originally on our roadmap to add OpenVINO to DeepTrio. However, when we were updating DeepVariant's metrics.md, we found that with OpenVINO runs roughly the same as without it. ( (We observed a pretty significant speedup in r1.2 using just intel-tensorflow without OpenVINO.). As a result, I didn't add OpenVINO to DeepTrio this time. @ASLeonard You can still find OpenVINO in our DeepVariant Docker image. If you still observe a noticeable speedup using OpenVINO using our DeepVariant setting, let me know. I'd really love to hear what type of machine you're using. FYI for @dkurt because this is OpenVINO related.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:110,availability,slo,slower,110,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:238,deployability,version,version,238,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:389,energy efficiency,clock,clock,389,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:429,energy efficiency,CPU,CPU,429,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:238,integrability,version,version,238,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:238,modifiability,version,version,238,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:246,modifiability,extens,extensively,246,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:395,performance,time,time,395,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:429,performance,CPU,CPU,429,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:433,performance,time,time,433,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:485,performance,bottleneck,bottlenecking,485,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:110,reliability,slo,slower,110,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:223,safety,test,tested,223,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:300,security,ident,identical,300,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:223,testability,test,tested,223,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:527,usability,clear,clearer,527,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/418:542,integrability,sub,subscribed,542,"You could assemble your reads first. Then align them to the consensus to get the bam. Then call variants with the consensus as your reference. . Sent from my iPhone. > On Feb 4, 2021, at 8:37 PM, B10inform <notifications@github.com> wrote:. > . > . > DeepVariant without Reference:. > I am trying to phase a haploid genome. But i don't have a reference genome. How would deepvariant work without the reference genome. (How to get the bam file?). > . > Any help would be great. > . > Thanks. > . > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/418
https://github.com/google/deepvariant/issues/418:457,usability,help,help,457,"You could assemble your reads first. Then align them to the consensus to get the bam. Then call variants with the consensus as your reference. . Sent from my iPhone. > On Feb 4, 2021, at 8:37 PM, B10inform <notifications@github.com> wrote:. > . > . > DeepVariant without Reference:. > I am trying to phase a haploid genome. But i don't have a reference genome. How would deepvariant work without the reference genome. (How to get the bam file?). > . > Any help would be great. > . > Thanks. > . > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/418
https://github.com/google/deepvariant/issues/418:224,performance,perform,perform,224,"@B10inform I'm not sure phasing is necessary for a haploid genome - because there is only one set of chromosomes the expectation is that identified variants are all already in phase. Additionally, DeepVariant is designed to perform germline variant calling in diploid organisms, and requires a reference genome. If the species you are working with does not have a reference genome than as @husamia suggested, this sounds like more of an assembly issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/418
https://github.com/google/deepvariant/issues/418:348,reliability,doe,does,348,"@B10inform I'm not sure phasing is necessary for a haploid genome - because there is only one set of chromosomes the expectation is that identified variants are all already in phase. Additionally, DeepVariant is designed to perform germline variant calling in diploid organisms, and requires a reference genome. If the species you are working with does not have a reference genome than as @husamia suggested, this sounds like more of an assembly issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/418
https://github.com/google/deepvariant/issues/418:137,security,ident,identified,137,"@B10inform I'm not sure phasing is necessary for a haploid genome - because there is only one set of chromosomes the expectation is that identified variants are all already in phase. Additionally, DeepVariant is designed to perform germline variant calling in diploid organisms, and requires a reference genome. If the species you are working with does not have a reference genome than as @husamia suggested, this sounds like more of an assembly issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/418
https://github.com/google/deepvariant/issues/418:224,usability,perform,perform,224,"@B10inform I'm not sure phasing is necessary for a haploid genome - because there is only one set of chromosomes the expectation is that identified variants are all already in phase. Additionally, DeepVariant is designed to perform germline variant calling in diploid organisms, and requires a reference genome. If the species you are working with does not have a reference genome than as @husamia suggested, this sounds like more of an assembly issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/418
https://github.com/google/deepvariant/issues/418:83,testability,plan,plan,83,@husamia @danielecook I meant a diploid genome (I have corrected the question). My plan is to obtain heterozygous SNP sites using deepvariant and than to phase the genome. .,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/418
https://github.com/google/deepvariant/issues/419:69,deployability,fail,failed,69,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:521,deployability,modul,module,521,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:817,integrability,sub,subprocess,817,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:910,integrability,sub,subprocess,910,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:991,integrability,sub,subprocess,991,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1075,integrability,buffer,buffer,1075,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:521,modifiability,modul,module,521,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:581,modifiability,pac,packages,581,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:681,modifiability,pac,packages,681,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:50,performance,parallel,parallel,50,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1031,performance,time,time,1031,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1047,performance,parallel,parallel,1047,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:69,reliability,fail,failed,69,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:521,safety,modul,module,521,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:423,testability,Trace,Traceback,423,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:327,usability,user,user,327,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:839,usability,command,command,839,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1022,usability,Command,Command,1022,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1344,usability,statu,status,1344,"With DeepVariant 1.0.0, similar result:. ```bash. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. real 0m35.240s. user 0m1.356s. sys 0m1.216s. I0205 11:52:59.059437 47804134787776 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz"" --alt_aligned_pileup ""diff_channels"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 252.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:21,availability,error,error,21,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:53,deployability,log,logs,53,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:27,integrability,messag,messages,27,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:94,integrability,wrap,wrapper,94,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:27,interoperability,messag,messages,27,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:94,interoperability,wrapper,wrapper,94,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:21,performance,error,error,21,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:109,reliability,doe,doesn,109,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:21,safety,error,error,21,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:53,safety,log,logs,53,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:53,security,log,logs,53,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:53,testability,log,logs,53,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:21,usability,error,error,21,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:50,availability,error,error,50,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:841,availability,error,error,841,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:109,energy efficiency,core,core,109,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:847,integrability,messag,message,847,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:175,interoperability,bind,bind,175,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:517,interoperability,share,share,517,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:743,interoperability,standard,standard,743,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:847,interoperability,messag,message,847,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:175,modifiability,bind,bind,175,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:50,performance,error,error,50,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:841,performance,error,error,841,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:50,safety,error,error,50,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:841,safety,error,error,841,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:33,usability,indicat,indication,33,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:50,usability,error,error,50,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:841,usability,error,error,841,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:. ```bash. singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:46,reliability,doe,does,46,@williamrowell try removing the `--task` flag does that change anything?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:33,availability,error,error,33,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:15,energy efficiency,core,core,15,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:39,integrability,messag,message,39,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:39,interoperability,messag,message,39,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:33,performance,error,error,33,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:33,safety,error,error,33,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:57,safety,test,test,57,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:57,testability,test,test,57,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:33,usability,error,error,33,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:468,interoperability,coordinat,coordinate,468,"This works with docker:. ```bash. docker run -v $PWD:/data google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref /data/ref.fa --reads /data/reads.bam --examples /data/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Output:. ```bash. 2021-02-05 21:03:25.094808: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.095168 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.100202 140318393947904 make_examples.py:648] Task 0/24: Preparing inputs. 2021-02-05 21:03:25.143855: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.144100 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.145697 140318393947904 make_examples.py:648] Task 0/24: Common contigs are ['contig']. I0205 21:03:25.149076 140318393947904 make_examples.py:648] Task 0/24: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-02-05 21:03:25.149922: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2021-02-05 21:03:25.162121: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.162382 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. 2021-02-05 21:03:25.163749: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.163837 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.349464 140318393947904 make_examples.py:648] Task 0/24: Writing examples to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:814,interoperability,coordinat,coordinate,814,"This works with docker:. ```bash. docker run -v $PWD:/data google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref /data/ref.fa --reads /data/reads.bam --examples /data/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Output:. ```bash. 2021-02-05 21:03:25.094808: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.095168 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.100202 140318393947904 make_examples.py:648] Task 0/24: Preparing inputs. 2021-02-05 21:03:25.143855: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.144100 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.145697 140318393947904 make_examples.py:648] Task 0/24: Common contigs are ['contig']. I0205 21:03:25.149076 140318393947904 make_examples.py:648] Task 0/24: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-02-05 21:03:25.149922: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2021-02-05 21:03:25.162121: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.162382 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. 2021-02-05 21:03:25.163749: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.163837 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.349464 140318393947904 make_examples.py:648] Task 0/24: Writing examples to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1523,interoperability,coordinat,coordinate,1523,"enomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.100202 140318393947904 make_examples.py:648] Task 0/24: Preparing inputs. 2021-02-05 21:03:25.143855: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.144100 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.145697 140318393947904 make_examples.py:648] Task 0/24: Common contigs are ['contig']. I0205 21:03:25.149076 140318393947904 make_examples.py:648] Task 0/24: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-02-05 21:03:25.149922: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2021-02-05 21:03:25.162121: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.162382 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. 2021-02-05 21:03:25.163749: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.163837 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.349464 140318393947904 make_examples.py:648] Task 0/24: Writing examples to /data/make_examples.tfrecord-00000-of-00024.gz. I0205 21:03:25.352185 140318393947904 make_examples.py:648] Task 0/24: Overhead for preparing inputs: 0 seconds. I0205 21:03:25.385783 140318393947904 make_examples.py:648] Task 0/24: 2 candidates (2 examples) [0.03s elapsed]. I0205 21:03:25.394754 140318393947904 make_examples.py:648] Task 0/24: Found 2 candidate variants. I0205 21:03:25.394980 140318393947904 make_examples.py:648] Task 0/24: Created 2 examples. ```. Looks like something is going on with the singularity invo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1780,interoperability,coordinat,coordinate,1780,"er.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.100202 140318393947904 make_examples.py:648] Task 0/24: Preparing inputs. 2021-02-05 21:03:25.143855: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.144100 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.145697 140318393947904 make_examples.py:648] Task 0/24: Common contigs are ['contig']. I0205 21:03:25.149076 140318393947904 make_examples.py:648] Task 0/24: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-02-05 21:03:25.149922: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2021-02-05 21:03:25.162121: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.162382 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. 2021-02-05 21:03:25.163749: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.163837 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.349464 140318393947904 make_examples.py:648] Task 0/24: Writing examples to /data/make_examples.tfrecord-00000-of-00024.gz. I0205 21:03:25.352185 140318393947904 make_examples.py:648] Task 0/24: Overhead for preparing inputs: 0 seconds. I0205 21:03:25.385783 140318393947904 make_examples.py:648] Task 0/24: 2 candidates (2 examples) [0.03s elapsed]. I0205 21:03:25.394754 140318393947904 make_examples.py:648] Task 0/24: Found 2 candidate variants. I0205 21:03:25.394980 140318393947904 make_examples.py:648] Task 0/24: Created 2 examples. ```. Looks like something is going on with the singularity invocation then?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1224,modifiability,deco,decode,1224,"t_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Output:. ```bash. 2021-02-05 21:03:25.094808: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.095168 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.100202 140318393947904 make_examples.py:648] Task 0/24: Preparing inputs. 2021-02-05 21:03:25.143855: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.144100 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.145697 140318393947904 make_examples.py:648] Task 0/24: Common contigs are ['contig']. I0205 21:03:25.149076 140318393947904 make_examples.py:648] Task 0/24: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-02-05 21:03:25.149922: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2021-02-05 21:03:25.162121: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.162382 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. 2021-02-05 21:03:25.163749: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.163837 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.349464 140318393947904 make_examples.py:648] Task 0/24: Writing examples to /data/make_examples.tfrecord-00000-of-00024.gz. I0205 21:03:25.352185 140318393947904 make_examples.py:648] Task 0/24: Overhead for preparing inputs: 0 seconds. I0205 21:03:25.385783 140318393947904 make_examples.py:648] Task 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:2119,performance,Overhead,Overhead,2119,"er.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.100202 140318393947904 make_examples.py:648] Task 0/24: Preparing inputs. 2021-02-05 21:03:25.143855: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.144100 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.145697 140318393947904 make_examples.py:648] Task 0/24: Common contigs are ['contig']. I0205 21:03:25.149076 140318393947904 make_examples.py:648] Task 0/24: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-02-05 21:03:25.149922: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2021-02-05 21:03:25.162121: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.162382 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. 2021-02-05 21:03:25.163749: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.163837 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.349464 140318393947904 make_examples.py:648] Task 0/24: Writing examples to /data/make_examples.tfrecord-00000-of-00024.gz. I0205 21:03:25.352185 140318393947904 make_examples.py:648] Task 0/24: Overhead for preparing inputs: 0 seconds. I0205 21:03:25.385783 140318393947904 make_examples.py:648] Task 0/24: 2 candidates (2 examples) [0.03s elapsed]. I0205 21:03:25.394754 140318393947904 make_examples.py:648] Task 0/24: Found 2 candidate variants. I0205 21:03:25.394980 140318393947904 make_examples.py:648] Task 0/24: Created 2 examples. ```. Looks like something is going on with the singularity invocation then?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:678,safety,input,inputs,678,"This works with docker:. ```bash. docker run -v $PWD:/data google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref /data/ref.fa --reads /data/reads.bam --examples /data/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Output:. ```bash. 2021-02-05 21:03:25.094808: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.095168 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.100202 140318393947904 make_examples.py:648] Task 0/24: Preparing inputs. 2021-02-05 21:03:25.143855: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.144100 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.145697 140318393947904 make_examples.py:648] Task 0/24: Common contigs are ['contig']. I0205 21:03:25.149076 140318393947904 make_examples.py:648] Task 0/24: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-02-05 21:03:25.149922: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2021-02-05 21:03:25.162121: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.162382 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. 2021-02-05 21:03:25.163749: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.163837 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.349464 140318393947904 make_examples.py:648] Task 0/24: Writing examples to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1199,safety,input,input,1199,"mples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Output:. ```bash. 2021-02-05 21:03:25.094808: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.095168 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.100202 140318393947904 make_examples.py:648] Task 0/24: Preparing inputs. 2021-02-05 21:03:25.143855: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.144100 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.145697 140318393947904 make_examples.py:648] Task 0/24: Common contigs are ['contig']. I0205 21:03:25.149076 140318393947904 make_examples.py:648] Task 0/24: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-02-05 21:03:25.149922: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2021-02-05 21:03:25.162121: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.162382 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. 2021-02-05 21:03:25.163749: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.163837 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.349464 140318393947904 make_examples.py:648] Task 0/24: Writing examples to /data/make_examples.tfrecord-00000-of-00024.gz. I0205 21:03:25.352185 140318393947904 make_examples.py:648] Task 0/24: Overhead for preparing inputs: 0 seconds. I0205 21:03:25.385783 140318393947904 mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:2142,safety,input,inputs,2142,"er.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.100202 140318393947904 make_examples.py:648] Task 0/24: Preparing inputs. 2021-02-05 21:03:25.143855: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.144100 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.145697 140318393947904 make_examples.py:648] Task 0/24: Common contigs are ['contig']. I0205 21:03:25.149076 140318393947904 make_examples.py:648] Task 0/24: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-02-05 21:03:25.149922: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2021-02-05 21:03:25.162121: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.162382 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. 2021-02-05 21:03:25.163749: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.163837 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.349464 140318393947904 make_examples.py:648] Task 0/24: Writing examples to /data/make_examples.tfrecord-00000-of-00024.gz. I0205 21:03:25.352185 140318393947904 make_examples.py:648] Task 0/24: Overhead for preparing inputs: 0 seconds. I0205 21:03:25.385783 140318393947904 make_examples.py:648] Task 0/24: 2 candidates (2 examples) [0.03s elapsed]. I0205 21:03:25.394754 140318393947904 make_examples.py:648] Task 0/24: Found 2 candidate variants. I0205 21:03:25.394980 140318393947904 make_examples.py:648] Task 0/24: Created 2 examples. ```. Looks like something is going on with the singularity invocation then?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:678,usability,input,inputs,678,"This works with docker:. ```bash. docker run -v $PWD:/data google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref /data/ref.fa --reads /data/reads.bam --examples /data/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Output:. ```bash. 2021-02-05 21:03:25.094808: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.095168 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.100202 140318393947904 make_examples.py:648] Task 0/24: Preparing inputs. 2021-02-05 21:03:25.143855: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.144100 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.145697 140318393947904 make_examples.py:648] Task 0/24: Common contigs are ['contig']. I0205 21:03:25.149076 140318393947904 make_examples.py:648] Task 0/24: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-02-05 21:03:25.149922: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2021-02-05 21:03:25.162121: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.162382 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. 2021-02-05 21:03:25.163749: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.163837 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.349464 140318393947904 make_examples.py:648] Task 0/24: Writing examples to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1199,usability,input,input,1199,"mples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0. ```. Output:. ```bash. 2021-02-05 21:03:25.094808: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.095168 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.100202 140318393947904 make_examples.py:648] Task 0/24: Preparing inputs. 2021-02-05 21:03:25.143855: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.144100 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.145697 140318393947904 make_examples.py:648] Task 0/24: Common contigs are ['contig']. I0205 21:03:25.149076 140318393947904 make_examples.py:648] Task 0/24: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-02-05 21:03:25.149922: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2021-02-05 21:03:25.162121: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.162382 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. 2021-02-05 21:03:25.163749: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.163837 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.349464 140318393947904 make_examples.py:648] Task 0/24: Writing examples to /data/make_examples.tfrecord-00000-of-00024.gz. I0205 21:03:25.352185 140318393947904 make_examples.py:648] Task 0/24: Overhead for preparing inputs: 0 seconds. I0205 21:03:25.385783 140318393947904 mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:2142,usability,input,inputs,2142,"er.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.100202 140318393947904 make_examples.py:648] Task 0/24: Preparing inputs. 2021-02-05 21:03:25.143855: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.144100 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.145697 140318393947904 make_examples.py:648] Task 0/24: Common contigs are ['contig']. I0205 21:03:25.149076 140318393947904 make_examples.py:648] Task 0/24: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-02-05 21:03:25.149922: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2021-02-05 21:03:25.162121: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.162382 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. 2021-02-05 21:03:25.163749: W third_party/nucleus/io/sam_reader.cc:130] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0205 21:03:25.163837 140318393947904 genomics_reader.py:223] Reading /data/reads.bam with NativeSamReader. I0205 21:03:25.349464 140318393947904 make_examples.py:648] Task 0/24: Writing examples to /data/make_examples.tfrecord-00000-of-00024.gz. I0205 21:03:25.352185 140318393947904 make_examples.py:648] Task 0/24: Overhead for preparing inputs: 0 seconds. I0205 21:03:25.385783 140318393947904 make_examples.py:648] Task 0/24: 2 candidates (2 examples) [0.03s elapsed]. I0205 21:03:25.394754 140318393947904 make_examples.py:648] Task 0/24: Found 2 candidate variants. I0205 21:03:25.394980 140318393947904 make_examples.py:648] Task 0/24: Created 2 examples. ```. Looks like something is going on with the singularity invocation then?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:652,availability,down,downloaded,652,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:374,deployability,Instal,Install,374,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:461,deployability,instal,installation,461,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:479,deployability,instal,installation-on-linux,479,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:573,deployability,version,version,573,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:594,deployability,version,version,594,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:202,energy efficiency,cloud,cloud-platform,202,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:274,energy efficiency,cloud,cloud,274,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1460,energy efficiency,CPU,CPU,1460,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1647,energy efficiency,CPU,CPU,1647,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:573,integrability,version,version,573,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:594,integrability,version,version,594,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:208,interoperability,platform,platform,208,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:303,interoperability,standard,standard-,303,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:869,interoperability,bind,bind,869,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1494,interoperability,Specif,Specifically,1494,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:573,modifiability,version,version,573,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:594,modifiability,version,version,594,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:869,modifiability,bind,bind,869,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:349,performance,disk,disk-size,349,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1460,performance,CPU,CPU,1460,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1647,performance,CPU,CPU,1647,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1071,testability,unit,unittest,1071,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:151,usability,USER,USER,151,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:438,usability,guid,guides,438,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:455,usability,guid,guide,455,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1325,usability,statu,status,1325,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:1651,usability,support,supports,1651,". Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue. . # Get a CentOS 7 machine. ```. gcloud compute instances create ""${USER}-centos"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"" \. --boot-disk-size ""200"" . ```. # Install Singularity 3.5. Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:. ```. [pichuan@pichuan-centos singularity]$ singularity --version. singularity version 3.5.2. ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:. ```. singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:1.1.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --regions ""chr20:10,000,000-10,010,000"" \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --num_shards 24 -v 2. ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:12,availability,error,error,12,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash. $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py. $. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:62,interoperability,specif,specific,62,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash. $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py. $. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:12,performance,error,error,12,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash. $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py. $. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:251,reliability,doe,does,251,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash. $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py. $. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:12,safety,error,error,12,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash. $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py. $. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:223,security,command-lin,command-line,223,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash. $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py. $. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:21,testability,simpl,simpler,21,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash. $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py. $. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:12,usability,error,error,12,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash. $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py. $. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:21,usability,simpl,simpler,21,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash. $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py. $. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:223,usability,command,command-line,223,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash. $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py. $. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:336,usability,statu,status,336,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash. $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py. $. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:472,usability,help,helps,472,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash. $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py. $. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:371,usability,help,helping,371,"> Hmm, after reading the whole thread again, the AVX explanation didn't quite make sense either because @williamrowell mentioned that running with docker worked.. > . > . > . > I don't have a next suggestion yet :-/ . I had to run docker on a different node, so this is very possible. Will look into this on Monday, as I didn't save which mode I was using and might need helping looking up that information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:401,deployability,fail,failed,401,"@pgrosu the `no` prefix is used to set a flag to FALSE with the flag library we are using. These types of flags are allowed. ---. @williamrowell can you launch a singularity shell and try executing these commands manually? ```shell. singularity shell --bind /scratch:/tmp,/usr/lib/locale/ \. docker://google/deepvariant:1.1.0. ```. Then once you are in the shell, try running one of the commands that failed directly:. ```. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. ```. And let us know what the output looks like.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:253,interoperability,bind,bind,253,"@pgrosu the `no` prefix is used to set a flag to FALSE with the flag library we are using. These types of flags are allowed. ---. @williamrowell can you launch a singularity shell and try executing these commands manually? ```shell. singularity shell --bind /scratch:/tmp,/usr/lib/locale/ \. docker://google/deepvariant:1.1.0. ```. Then once you are in the shell, try running one of the commands that failed directly:. ```. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. ```. And let us know what the output looks like.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:253,modifiability,bind,bind,253,"@pgrosu the `no` prefix is used to set a flag to FALSE with the flag library we are using. These types of flags are allowed. ---. @williamrowell can you launch a singularity shell and try executing these commands manually? ```shell. singularity shell --bind /scratch:/tmp,/usr/lib/locale/ \. docker://google/deepvariant:1.1.0. ```. Then once you are in the shell, try running one of the commands that failed directly:. ```. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. ```. And let us know what the output looks like.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:401,reliability,fail,failed,401,"@pgrosu the `no` prefix is used to set a flag to FALSE with the flag library we are using. These types of flags are allowed. ---. @williamrowell can you launch a singularity shell and try executing these commands manually? ```shell. singularity shell --bind /scratch:/tmp,/usr/lib/locale/ \. docker://google/deepvariant:1.1.0. ```. Then once you are in the shell, try running one of the commands that failed directly:. ```. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. ```. And let us know what the output looks like.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:204,usability,command,commands,204,"@pgrosu the `no` prefix is used to set a flag to FALSE with the flag library we are using. These types of flags are allowed. ---. @williamrowell can you launch a singularity shell and try executing these commands manually? ```shell. singularity shell --bind /scratch:/tmp,/usr/lib/locale/ \. docker://google/deepvariant:1.1.0. ```. Then once you are in the shell, try running one of the commands that failed directly:. ```. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. ```. And let us know what the output looks like.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:387,usability,command,commands,387,"@pgrosu the `no` prefix is used to set a flag to FALSE with the flag library we are using. These types of flags are allowed. ---. @williamrowell can you launch a singularity shell and try executing these commands manually? ```shell. singularity shell --bind /scratch:/tmp,/usr/lib/locale/ \. docker://google/deepvariant:1.1.0. ```. Then once you are in the shell, try running one of the commands that failed directly:. ```. /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmpfpe8sndc/make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 16. ```. And let us know what the output looks like.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:172,availability,error,error,172,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:497,availability,monitor,monitor,497,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:280,deployability,api,apic,280,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:497,deployability,monitor,monitor,497,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:44,energy efficiency,CPU,CPU,44,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:154,energy efficiency,core,core,154,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:497,energy efficiency,monitor,monitor,497,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:178,integrability,messag,message,178,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:280,integrability,api,apic,280,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:178,interoperability,messag,message,178,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:280,interoperability,api,apic,280,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:44,performance,CPU,CPU,44,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:172,performance,error,error,172,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:384,reliability,rdt,rdtscp,384,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:497,reliability,monitor,monitor,497,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:172,safety,error,error,172,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:497,safety,monitor,monitor,497,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:497,testability,monitor,monitor,497,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:48,usability,support,supports,48,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:107,usability,support,support,107,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:172,usability,error,error,172,> @williamrowell Can you check whether your CPU supports AVX instruction? This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash. wrowell@mp0608-sge:~$ lscpu | grep Flags. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:20,usability,close,close,20,"@williamrowell I'll close out this issue, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/420:182,deployability,manag,manager,182,"Hello - you can run docker interactively to verify that your files are being mounted correctly. Try running the following command:. ```. BIN_VERSION=""1.1.0"". export OUTPUT_DIR=/home/manager/deepvariant-run/input. sudo docker run -it -v ""${OUTPUT_DIR}"":/opt/deepvariant/output dajunluo/deepvariant /bin/bash. ```. From the looks of it you can probably rectify the issue by specifying this path to `newtest.bam`:. `output/newtest.bam`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/420
https://github.com/google/deepvariant/issues/420:182,energy efficiency,manag,manager,182,"Hello - you can run docker interactively to verify that your files are being mounted correctly. Try running the following command:. ```. BIN_VERSION=""1.1.0"". export OUTPUT_DIR=/home/manager/deepvariant-run/input. sudo docker run -it -v ""${OUTPUT_DIR}"":/opt/deepvariant/output dajunluo/deepvariant /bin/bash. ```. From the looks of it you can probably rectify the issue by specifying this path to `newtest.bam`:. `output/newtest.bam`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/420
https://github.com/google/deepvariant/issues/420:372,interoperability,specif,specifying,372,"Hello - you can run docker interactively to verify that your files are being mounted correctly. Try running the following command:. ```. BIN_VERSION=""1.1.0"". export OUTPUT_DIR=/home/manager/deepvariant-run/input. sudo docker run -it -v ""${OUTPUT_DIR}"":/opt/deepvariant/output dajunluo/deepvariant /bin/bash. ```. From the looks of it you can probably rectify the issue by specifying this path to `newtest.bam`:. `output/newtest.bam`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/420
https://github.com/google/deepvariant/issues/420:182,safety,manag,manager,182,"Hello - you can run docker interactively to verify that your files are being mounted correctly. Try running the following command:. ```. BIN_VERSION=""1.1.0"". export OUTPUT_DIR=/home/manager/deepvariant-run/input. sudo docker run -it -v ""${OUTPUT_DIR}"":/opt/deepvariant/output dajunluo/deepvariant /bin/bash. ```. From the looks of it you can probably rectify the issue by specifying this path to `newtest.bam`:. `output/newtest.bam`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/420
https://github.com/google/deepvariant/issues/420:206,safety,input,input,206,"Hello - you can run docker interactively to verify that your files are being mounted correctly. Try running the following command:. ```. BIN_VERSION=""1.1.0"". export OUTPUT_DIR=/home/manager/deepvariant-run/input. sudo docker run -it -v ""${OUTPUT_DIR}"":/opt/deepvariant/output dajunluo/deepvariant /bin/bash. ```. From the looks of it you can probably rectify the issue by specifying this path to `newtest.bam`:. `output/newtest.bam`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/420
https://github.com/google/deepvariant/issues/420:44,testability,verif,verify,44,"Hello - you can run docker interactively to verify that your files are being mounted correctly. Try running the following command:. ```. BIN_VERSION=""1.1.0"". export OUTPUT_DIR=/home/manager/deepvariant-run/input. sudo docker run -it -v ""${OUTPUT_DIR}"":/opt/deepvariant/output dajunluo/deepvariant /bin/bash. ```. From the looks of it you can probably rectify the issue by specifying this path to `newtest.bam`:. `output/newtest.bam`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/420
https://github.com/google/deepvariant/issues/420:27,usability,interact,interactively,27,"Hello - you can run docker interactively to verify that your files are being mounted correctly. Try running the following command:. ```. BIN_VERSION=""1.1.0"". export OUTPUT_DIR=/home/manager/deepvariant-run/input. sudo docker run -it -v ""${OUTPUT_DIR}"":/opt/deepvariant/output dajunluo/deepvariant /bin/bash. ```. From the looks of it you can probably rectify the issue by specifying this path to `newtest.bam`:. `output/newtest.bam`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/420
https://github.com/google/deepvariant/issues/420:122,usability,command,command,122,"Hello - you can run docker interactively to verify that your files are being mounted correctly. Try running the following command:. ```. BIN_VERSION=""1.1.0"". export OUTPUT_DIR=/home/manager/deepvariant-run/input. sudo docker run -it -v ""${OUTPUT_DIR}"":/opt/deepvariant/output dajunluo/deepvariant /bin/bash. ```. From the looks of it you can probably rectify the issue by specifying this path to `newtest.bam`:. `output/newtest.bam`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/420
https://github.com/google/deepvariant/issues/420:206,usability,input,input,206,"Hello - you can run docker interactively to verify that your files are being mounted correctly. Try running the following command:. ```. BIN_VERSION=""1.1.0"". export OUTPUT_DIR=/home/manager/deepvariant-run/input. sudo docker run -it -v ""${OUTPUT_DIR}"":/opt/deepvariant/output dajunluo/deepvariant /bin/bash. ```. From the looks of it you can probably rectify the issue by specifying this path to `newtest.bam`:. `output/newtest.bam`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/420
https://github.com/google/deepvariant/issues/420:26,usability,close,close,26,"@belarus1941 I'm going to close this thread due to inactivity, but feel free to reopen of you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/420
https://github.com/google/deepvariant/issues/422:244,deployability,updat,update,244,"Hello,. The way you run it looks correct. I don't understand how running it for the second time succeeded. Could you try to run it with the intermediate_results_dir set to the location inside /wd directory? We will try to reproduce it and then update the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/422
https://github.com/google/deepvariant/issues/422:91,performance,time,time,91,"Hello,. The way you run it looks correct. I don't understand how running it for the second time succeeded. Could you try to run it with the intermediate_results_dir set to the location inside /wd directory? We will try to reproduce it and then update the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/422
https://github.com/google/deepvariant/issues/422:244,safety,updat,update,244,"Hello,. The way you run it looks correct. I don't understand how running it for the second time succeeded. Could you try to run it with the intermediate_results_dir set to the location inside /wd directory? We will try to reproduce it and then update the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/422
https://github.com/google/deepvariant/issues/422:244,security,updat,update,244,"Hello,. The way you run it looks correct. I don't understand how running it for the second time succeeded. Could you try to run it with the intermediate_results_dir set to the location inside /wd directory? We will try to reproduce it and then update the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/422
https://github.com/google/deepvariant/issues/422:50,testability,understand,understand,50,"Hello,. The way you run it looks correct. I don't understand how running it for the second time succeeded. Could you try to run it with the intermediate_results_dir set to the location inside /wd directory? We will try to reproduce it and then update the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/422
https://github.com/google/deepvariant/issues/422:16,usability,close,close,16,"@wharvey31 I'll close out this issue, but feel free to reopen if you have any remaining questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/422
https://github.com/google/deepvariant/issues/423:140,deployability,releas,release,140,"Thanks for bringing this up Brendan! I have added the white background to the diagram internally, so it will be fixed on GitHub in the next release. If this is annoying to more people, I could cherry-pick it to fix it sooner. Leave the pair-of-eyes emoji here to vote for us to prioritize this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/423
https://github.com/google/deepvariant/pull/424:13,security,sign,signed,13,@googlebot I signed it!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424
https://github.com/google/deepvariant/pull/424:172,deployability,releas,release,172,"Hi @dridk, thank you for the pull request! Unfortunately, we cannot merge pull requests through GitHub. I can fix this internally, and the change should go out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424
https://github.com/google/deepvariant/pull/424:201,availability,error,error,201,"Hi @dridk, my apologies, I didn't notice the branch for this pull request. We are no longer maintaining the r0.5 branch. However, thank you for the suggestion. We will keep an eye out for this kind of error in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424
https://github.com/google/deepvariant/pull/424:92,modifiability,maintain,maintaining,92,"Hi @dridk, my apologies, I didn't notice the branch for this pull request. We are no longer maintaining the r0.5 branch. However, thank you for the suggestion. We will keep an eye out for this kind of error in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424
https://github.com/google/deepvariant/pull/424:201,performance,error,error,201,"Hi @dridk, my apologies, I didn't notice the branch for this pull request. We are no longer maintaining the r0.5 branch. However, thank you for the suggestion. We will keep an eye out for this kind of error in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424
https://github.com/google/deepvariant/pull/424:92,safety,maintain,maintaining,92,"Hi @dridk, my apologies, I didn't notice the branch for this pull request. We are no longer maintaining the r0.5 branch. However, thank you for the suggestion. We will keep an eye out for this kind of error in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424
https://github.com/google/deepvariant/pull/424:201,safety,error,error,201,"Hi @dridk, my apologies, I didn't notice the branch for this pull request. We are no longer maintaining the r0.5 branch. However, thank you for the suggestion. We will keep an eye out for this kind of error in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424
https://github.com/google/deepvariant/pull/424:201,usability,error,error,201,"Hi @dridk, my apologies, I didn't notice the branch for this pull request. We are no longer maintaining the r0.5 branch. However, thank you for the suggestion. We will keep an eye out for this kind of error in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424
https://github.com/google/deepvariant/issues/425:105,availability,down,downsamples,105,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/425:468,deployability,depend,depending,468,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/425:187,energy efficiency,model,models,187,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/425:371,energy efficiency,model,models,371,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/425:680,energy efficiency,model,model,680,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/425:468,integrability,depend,depending,468,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/425:468,modifiability,depend,depending,468,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/425:592,performance,time,time,592,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/425:468,safety,depend,depending,468,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/425:187,security,model,models,187,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/425:371,security,model,models,371,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/425:680,security,model,model,680,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/425:468,testability,depend,depending,468,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/425:755,testability,coverag,coverage,755,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425
https://github.com/google/deepvariant/issues/426:0,safety,test,test,0,test 2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/426
https://github.com/google/deepvariant/issues/426:0,testability,test,test,0,test 2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/426
https://github.com/google/deepvariant/issues/426:29,deployability,continu,continue,29,:D I'll close this but might continue to comment on it. I'm trying to figure out email notifications...,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/426
https://github.com/google/deepvariant/issues/426:8,usability,close,close,8,:D I'll close this but might continue to comment on it. I'm trying to figure out email notifications...,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/426
https://github.com/google/deepvariant/issues/426:0,safety,Test,Test,0,Test,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/426
https://github.com/google/deepvariant/issues/426:0,testability,Test,Test,0,Test,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/426
https://github.com/google/deepvariant/issues/426:8,safety,test,test,8,another test by not-Pi-Chuan,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/426
https://github.com/google/deepvariant/issues/426:8,testability,test,test,8,another test by not-Pi-Chuan,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/426
https://github.com/google/deepvariant/issues/427:142,availability,error,error,142,"Hi @kostasgalexiou , can you provide more information like:. - What type of machines are you working on (how many cores, how much RAM). - Any error messages that you can share?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:114,energy efficiency,core,cores,114,"Hi @kostasgalexiou , can you provide more information like:. - What type of machines are you working on (how many cores, how much RAM). - Any error messages that you can share?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:148,integrability,messag,messages,148,"Hi @kostasgalexiou , can you provide more information like:. - What type of machines are you working on (how many cores, how much RAM). - Any error messages that you can share?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:148,interoperability,messag,messages,148,"Hi @kostasgalexiou , can you provide more information like:. - What type of machines are you working on (how many cores, how much RAM). - Any error messages that you can share?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:170,interoperability,share,share,170,"Hi @kostasgalexiou , can you provide more information like:. - What type of machines are you working on (how many cores, how much RAM). - Any error messages that you can share?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:142,performance,error,error,142,"Hi @kostasgalexiou , can you provide more information like:. - What type of machines are you working on (how many cores, how much RAM). - Any error messages that you can share?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:142,safety,error,error,142,"Hi @kostasgalexiou , can you provide more information like:. - What type of machines are you working on (how many cores, how much RAM). - Any error messages that you can share?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:142,usability,error,error,142,"Hi @kostasgalexiou , can you provide more information like:. - What type of machines are you working on (how many cores, how much RAM). - Any error messages that you can share?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:84,availability,error,error,84,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:144,availability,error,errors,144,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:186,deployability,log,logs,186,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:90,integrability,messag,messages,90,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:30,interoperability,standard,standard-,30,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:90,interoperability,messag,messages,90,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:59,performance,memor,memory,59,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:84,performance,error,error,84,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:144,performance,error,errors,144,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:231,performance,content,contents,231,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:84,safety,error,error,84,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:144,safety,error,errors,144,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:186,safety,log,logs,186,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:283,safety,input,input,283,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:186,security,log,logs,186,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:186,testability,log,logs,186,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:59,usability,memor,memory,59,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:84,usability,error,error,84,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:144,usability,error,errors,144,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:283,usability,input,input,283,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:.  [drwxrwxr-x 4.0K] input.   [drwxrwxr-x 4.0K] data.   [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa.   [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai.   [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam.   [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai.  [drwxrwxr-x 4.0K] output.  [drwxr-xr-x 4.0K] intermediate_results_dir.  [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz.  [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz.  [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz.  [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-0001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:30,deployability,log,log,30,"Hi @kostasgalexiou , from the log, does it look like all 16 make_examples have finished? (At the end it should have something like:. ```. Task 1/16: Created ... examples. ```. If that's hard to see, can you check how many of the make_examples are still running? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:35,reliability,doe,does,35,"Hi @kostasgalexiou , from the log, does it look like all 16 make_examples have finished? (At the end it should have something like:. ```. Task 1/16: Created ... examples. ```. If that's hard to see, can you check how many of the make_examples are still running? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:30,safety,log,log,30,"Hi @kostasgalexiou , from the log, does it look like all 16 make_examples have finished? (At the end it should have something like:. ```. Task 1/16: Created ... examples. ```. If that's hard to see, can you check how many of the make_examples are still running? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:30,security,log,log,30,"Hi @kostasgalexiou , from the log, does it look like all 16 make_examples have finished? (At the end it should have something like:. ```. Task 1/16: Created ... examples. ```. If that's hard to see, can you check how many of the make_examples are still running? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:30,testability,log,log,30,"Hi @kostasgalexiou , from the log, does it look like all 16 make_examples have finished? (At the end it should have something like:. ```. Task 1/16: Created ... examples. ```. If that's hard to see, can you check how many of the make_examples are still running? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:47,deployability,log,log,47,"Hi @pichuan . Unfortunately, I do not have the log file. None of the make_examples is running. The last update occurred on March,1 at 18:44. Can we say whether the make_example / gvcf process has finished? If yes, can I launch deevariant again and go directly to call_variants? -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 353632256 Mar 1 18:44 gvcf.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 353107968 Mar 1 18:44 gvcf.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00005-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00006-of-00016.gz. -rw-r--r-- 1 root root 353370112 Mar 1 18:44 gvcf.tfrecord-00007-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00008-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00009-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00010-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00011-of-00016.gz. -rw-r--r-- 1 root root 354418688 Mar 1 18:44 gvcf.tfrecord-00012-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00013-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00014-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00015-of-00016.gz. -rw-r--r-- 1 root root 10704650240 Mar 1 18:44 make_examples.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 10710417408 Mar 1 18:44 make_examples.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 10699931648 Mar 1 18:44 make_examples.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 10677649408 Mar 1 18:44 make_examples.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 10692329472 Mar 1 18:44 make_examples.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 107253",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:104,deployability,updat,update,104,"Hi @pichuan . Unfortunately, I do not have the log file. None of the make_examples is running. The last update occurred on March,1 at 18:44. Can we say whether the make_example / gvcf process has finished? If yes, can I launch deevariant again and go directly to call_variants? -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 353632256 Mar 1 18:44 gvcf.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 353107968 Mar 1 18:44 gvcf.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00005-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00006-of-00016.gz. -rw-r--r-- 1 root root 353370112 Mar 1 18:44 gvcf.tfrecord-00007-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00008-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00009-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00010-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00011-of-00016.gz. -rw-r--r-- 1 root root 354418688 Mar 1 18:44 gvcf.tfrecord-00012-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00013-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00014-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00015-of-00016.gz. -rw-r--r-- 1 root root 10704650240 Mar 1 18:44 make_examples.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 10710417408 Mar 1 18:44 make_examples.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 10699931648 Mar 1 18:44 make_examples.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 10677649408 Mar 1 18:44 make_examples.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 10692329472 Mar 1 18:44 make_examples.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 107253",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:47,safety,log,log,47,"Hi @pichuan . Unfortunately, I do not have the log file. None of the make_examples is running. The last update occurred on March,1 at 18:44. Can we say whether the make_example / gvcf process has finished? If yes, can I launch deevariant again and go directly to call_variants? -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 353632256 Mar 1 18:44 gvcf.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 353107968 Mar 1 18:44 gvcf.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00005-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00006-of-00016.gz. -rw-r--r-- 1 root root 353370112 Mar 1 18:44 gvcf.tfrecord-00007-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00008-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00009-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00010-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00011-of-00016.gz. -rw-r--r-- 1 root root 354418688 Mar 1 18:44 gvcf.tfrecord-00012-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00013-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00014-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00015-of-00016.gz. -rw-r--r-- 1 root root 10704650240 Mar 1 18:44 make_examples.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 10710417408 Mar 1 18:44 make_examples.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 10699931648 Mar 1 18:44 make_examples.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 10677649408 Mar 1 18:44 make_examples.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 10692329472 Mar 1 18:44 make_examples.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 107253",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:104,safety,updat,update,104,"Hi @pichuan . Unfortunately, I do not have the log file. None of the make_examples is running. The last update occurred on March,1 at 18:44. Can we say whether the make_example / gvcf process has finished? If yes, can I launch deevariant again and go directly to call_variants? -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 353632256 Mar 1 18:44 gvcf.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 353107968 Mar 1 18:44 gvcf.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00005-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00006-of-00016.gz. -rw-r--r-- 1 root root 353370112 Mar 1 18:44 gvcf.tfrecord-00007-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00008-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00009-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00010-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00011-of-00016.gz. -rw-r--r-- 1 root root 354418688 Mar 1 18:44 gvcf.tfrecord-00012-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00013-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00014-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00015-of-00016.gz. -rw-r--r-- 1 root root 10704650240 Mar 1 18:44 make_examples.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 10710417408 Mar 1 18:44 make_examples.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 10699931648 Mar 1 18:44 make_examples.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 10677649408 Mar 1 18:44 make_examples.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 10692329472 Mar 1 18:44 make_examples.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 107253",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:47,security,log,log,47,"Hi @pichuan . Unfortunately, I do not have the log file. None of the make_examples is running. The last update occurred on March,1 at 18:44. Can we say whether the make_example / gvcf process has finished? If yes, can I launch deevariant again and go directly to call_variants? -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 353632256 Mar 1 18:44 gvcf.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 353107968 Mar 1 18:44 gvcf.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00005-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00006-of-00016.gz. -rw-r--r-- 1 root root 353370112 Mar 1 18:44 gvcf.tfrecord-00007-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00008-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00009-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00010-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00011-of-00016.gz. -rw-r--r-- 1 root root 354418688 Mar 1 18:44 gvcf.tfrecord-00012-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00013-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00014-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00015-of-00016.gz. -rw-r--r-- 1 root root 10704650240 Mar 1 18:44 make_examples.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 10710417408 Mar 1 18:44 make_examples.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 10699931648 Mar 1 18:44 make_examples.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 10677649408 Mar 1 18:44 make_examples.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 10692329472 Mar 1 18:44 make_examples.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 107253",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:104,security,updat,update,104,"Hi @pichuan . Unfortunately, I do not have the log file. None of the make_examples is running. The last update occurred on March,1 at 18:44. Can we say whether the make_example / gvcf process has finished? If yes, can I launch deevariant again and go directly to call_variants? -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 353632256 Mar 1 18:44 gvcf.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 353107968 Mar 1 18:44 gvcf.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00005-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00006-of-00016.gz. -rw-r--r-- 1 root root 353370112 Mar 1 18:44 gvcf.tfrecord-00007-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00008-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00009-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00010-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00011-of-00016.gz. -rw-r--r-- 1 root root 354418688 Mar 1 18:44 gvcf.tfrecord-00012-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00013-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00014-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00015-of-00016.gz. -rw-r--r-- 1 root root 10704650240 Mar 1 18:44 make_examples.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 10710417408 Mar 1 18:44 make_examples.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 10699931648 Mar 1 18:44 make_examples.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 10677649408 Mar 1 18:44 make_examples.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 10692329472 Mar 1 18:44 make_examples.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 107253",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:47,testability,log,log,47,"Hi @pichuan . Unfortunately, I do not have the log file. None of the make_examples is running. The last update occurred on March,1 at 18:44. Can we say whether the make_example / gvcf process has finished? If yes, can I launch deevariant again and go directly to call_variants? -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 353632256 Mar 1 18:44 gvcf.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 353107968 Mar 1 18:44 gvcf.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00005-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00006-of-00016.gz. -rw-r--r-- 1 root root 353370112 Mar 1 18:44 gvcf.tfrecord-00007-of-00016.gz. -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00008-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00009-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00010-of-00016.gz. -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00011-of-00016.gz. -rw-r--r-- 1 root root 354418688 Mar 1 18:44 gvcf.tfrecord-00012-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00013-of-00016.gz. -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00014-of-00016.gz. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00015-of-00016.gz. -rw-r--r-- 1 root root 10704650240 Mar 1 18:44 make_examples.tfrecord-00000-of-00016.gz. -rw-r--r-- 1 root root 10710417408 Mar 1 18:44 make_examples.tfrecord-00001-of-00016.gz. -rw-r--r-- 1 root root 10699931648 Mar 1 18:44 make_examples.tfrecord-00002-of-00016.gz. -rw-r--r-- 1 root root 10677649408 Mar 1 18:44 make_examples.tfrecord-00003-of-00016.gz. -rw-r--r-- 1 root root 10692329472 Mar 1 18:44 make_examples.tfrecord-00004-of-00016.gz. -rw-r--r-- 1 root root 107253",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:291,availability,error,error,291,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:378,deployability,observ,observations,378,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:394,deployability,log,logs,394,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:483,integrability,pub,public,483,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:500,integrability,pub,public,500,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:560,interoperability,standard,standard-,560,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:291,performance,error,error,291,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:111,safety,compl,complete,111,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:212,safety,compl,complete,212,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:291,safety,error,error,291,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:394,safety,log,logs,394,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:461,safety,input,input,461,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:111,security,compl,complete,111,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:212,security,compl,complete,212,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:394,security,log,logs,394,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:378,testability,observ,observations,378,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:394,testability,log,logs,394,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:291,usability,error,error,291,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:461,usability,input,input,461,"Hi @kostasgalexiou ,. Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not. You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:25,integrability,pub,public,25,"Yes, the bam file is not public, but I could send you the files through a private link. Thanks for your help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:104,usability,help,help,104,"Yes, the bam file is not public, but I could send you the files through a private link. Thanks for your help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:66,availability,down,download,66,"Hi @kalexiou ,. if the file is not public, unfortunately I cannot download it or test it. Thanks for offering though. If you can share a bit more about what type of file this is (sequencing instrument, coverage, PCR-free or PCR-plus, etc) that might make this file unique, maybe I can try to see if I can reproduce the issue on another public file. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:35,integrability,pub,public,35,"Hi @kalexiou ,. if the file is not public, unfortunately I cannot download it or test it. Thanks for offering though. If you can share a bit more about what type of file this is (sequencing instrument, coverage, PCR-free or PCR-plus, etc) that might make this file unique, maybe I can try to see if I can reproduce the issue on another public file. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:336,integrability,pub,public,336,"Hi @kalexiou ,. if the file is not public, unfortunately I cannot download it or test it. Thanks for offering though. If you can share a bit more about what type of file this is (sequencing instrument, coverage, PCR-free or PCR-plus, etc) that might make this file unique, maybe I can try to see if I can reproduce the issue on another public file. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:129,interoperability,share,share,129,"Hi @kalexiou ,. if the file is not public, unfortunately I cannot download it or test it. Thanks for offering though. If you can share a bit more about what type of file this is (sequencing instrument, coverage, PCR-free or PCR-plus, etc) that might make this file unique, maybe I can try to see if I can reproduce the issue on another public file. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:81,safety,test,test,81,"Hi @kalexiou ,. if the file is not public, unfortunately I cannot download it or test it. Thanks for offering though. If you can share a bit more about what type of file this is (sequencing instrument, coverage, PCR-free or PCR-plus, etc) that might make this file unique, maybe I can try to see if I can reproduce the issue on another public file. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:81,testability,test,test,81,"Hi @kalexiou ,. if the file is not public, unfortunately I cannot download it or test it. Thanks for offering though. If you can share a bit more about what type of file this is (sequencing instrument, coverage, PCR-free or PCR-plus, etc) that might make this file unique, maybe I can try to see if I can reproduce the issue on another public file. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:190,testability,instrument,instrument,190,"Hi @kalexiou ,. if the file is not public, unfortunately I cannot download it or test it. Thanks for offering though. If you can share a bit more about what type of file this is (sequencing instrument, coverage, PCR-free or PCR-plus, etc) that might make this file unique, maybe I can try to see if I can reproduce the issue on another public file. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:202,testability,coverag,coverage,202,"Hi @kalexiou ,. if the file is not public, unfortunately I cannot download it or test it. Thanks for offering though. If you can share a bit more about what type of file this is (sequencing instrument, coverage, PCR-free or PCR-plus, etc) that might make this file unique, maybe I can try to see if I can reproduce the issue on another public file. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:459,availability,checkpoint,checkpoint,459,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:509,availability,checkpoint,checkpoint,509,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:557,deployability,depend,depending,557,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:476,energy efficiency,model,models,476,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:487,energy efficiency,model,model,487,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:557,integrability,depend,depending,557,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:550,modifiability,pac,pacbio,550,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:557,modifiability,depend,depending,557,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:459,reliability,checkpoint,checkpoint,459,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:509,reliability,checkpoint,checkpoint,509,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:557,safety,depend,depending,557,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:476,security,model,models,476,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:487,security,model,model,487,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:557,testability,depend,depending,557,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:160,usability,help,help,160,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:207,usability,command,command,207,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:. ```. docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help. ```. to look at all relevant flags. Your command should be probably something like:. ```. ... /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"". ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:488,availability,error,error,488,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:75,deployability,stage,stage,75,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:555,deployability,observ,observation,555,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:494,integrability,messag,messages,494,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:494,interoperability,messag,messages,494,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:583,modifiability,interm,intermediate,583,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:92,performance,memor,memory,92,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:488,performance,error,error,488,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:488,safety,error,error,488,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:630,safety,compl,complete,630,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:630,security,compl,complete,630,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:555,testability,observ,observation,555,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:92,usability,memor,memory,92,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:159,usability,behavi,behavior,159,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:442,usability,help,helps,442,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:488,usability,error,error,488,"Hi @kalexiou ,. I think what might have happened is that the make_examples stage ran out of memory. I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:. (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM. (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:29,deployability,updat,update,29,"Hi @pichuan,. Thanks for the update! I will try your suggestions next time I come up with a ""heavy"" bam file. Kostas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:70,performance,time,time,70,"Hi @pichuan,. Thanks for the update! I will try your suggestions next time I come up with a ""heavy"" bam file. Kostas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:29,safety,updat,update,29,"Hi @pichuan,. Thanks for the update! I will try your suggestions next time I come up with a ""heavy"" bam file. Kostas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/427:29,security,updat,update,29,"Hi @pichuan,. Thanks for the update! I will try your suggestions next time I come up with a ""heavy"" bam file. Kostas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427
https://github.com/google/deepvariant/issues/428:52,usability,help,help,52,@cdibona @dberlin @pichuan @AndrewCarroll could you help me ? thanks ~,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/428
https://github.com/google/deepvariant/issues/428:333,safety,input,inputs,333,"Hi @zhoudreames ,. You can apply DeepVariant in this case. DeepVariant is trained on human data, but we've seen it generalized to other cases before. . In addition to the BAM file you have, you'll need to provide a Pig reference file. For the command here:. https://github.com/google/deepvariant#how-to-run-deepvariant. The two main inputs that have to come from your are these:. ```. --ref=/input/YOUR_REF \. --reads=/input/YOUR_BAM \. ```. Curious to hear how it works for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/428
https://github.com/google/deepvariant/issues/428:392,safety,input,input,392,"Hi @zhoudreames ,. You can apply DeepVariant in this case. DeepVariant is trained on human data, but we've seen it generalized to other cases before. . In addition to the BAM file you have, you'll need to provide a Pig reference file. For the command here:. https://github.com/google/deepvariant#how-to-run-deepvariant. The two main inputs that have to come from your are these:. ```. --ref=/input/YOUR_REF \. --reads=/input/YOUR_BAM \. ```. Curious to hear how it works for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/428
https://github.com/google/deepvariant/issues/428:419,safety,input,input,419,"Hi @zhoudreames ,. You can apply DeepVariant in this case. DeepVariant is trained on human data, but we've seen it generalized to other cases before. . In addition to the BAM file you have, you'll need to provide a Pig reference file. For the command here:. https://github.com/google/deepvariant#how-to-run-deepvariant. The two main inputs that have to come from your are these:. ```. --ref=/input/YOUR_REF \. --reads=/input/YOUR_BAM \. ```. Curious to hear how it works for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/428
https://github.com/google/deepvariant/issues/428:243,usability,command,command,243,"Hi @zhoudreames ,. You can apply DeepVariant in this case. DeepVariant is trained on human data, but we've seen it generalized to other cases before. . In addition to the BAM file you have, you'll need to provide a Pig reference file. For the command here:. https://github.com/google/deepvariant#how-to-run-deepvariant. The two main inputs that have to come from your are these:. ```. --ref=/input/YOUR_REF \. --reads=/input/YOUR_BAM \. ```. Curious to hear how it works for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/428
https://github.com/google/deepvariant/issues/428:333,usability,input,inputs,333,"Hi @zhoudreames ,. You can apply DeepVariant in this case. DeepVariant is trained on human data, but we've seen it generalized to other cases before. . In addition to the BAM file you have, you'll need to provide a Pig reference file. For the command here:. https://github.com/google/deepvariant#how-to-run-deepvariant. The two main inputs that have to come from your are these:. ```. --ref=/input/YOUR_REF \. --reads=/input/YOUR_BAM \. ```. Curious to hear how it works for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/428
https://github.com/google/deepvariant/issues/428:392,usability,input,input,392,"Hi @zhoudreames ,. You can apply DeepVariant in this case. DeepVariant is trained on human data, but we've seen it generalized to other cases before. . In addition to the BAM file you have, you'll need to provide a Pig reference file. For the command here:. https://github.com/google/deepvariant#how-to-run-deepvariant. The two main inputs that have to come from your are these:. ```. --ref=/input/YOUR_REF \. --reads=/input/YOUR_BAM \. ```. Curious to hear how it works for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/428
https://github.com/google/deepvariant/issues/428:419,usability,input,input,419,"Hi @zhoudreames ,. You can apply DeepVariant in this case. DeepVariant is trained on human data, but we've seen it generalized to other cases before. . In addition to the BAM file you have, you'll need to provide a Pig reference file. For the command here:. https://github.com/google/deepvariant#how-to-run-deepvariant. The two main inputs that have to come from your are these:. ```. --ref=/input/YOUR_REF \. --reads=/input/YOUR_BAM \. ```. Curious to hear how it works for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/428
https://github.com/google/deepvariant/issues/429:178,deployability,releas,release,178,"Hi @jkalleberg thanks raising this issue. You're right that this actually should be optional, this seems to be bug - I've noted it in internally and we'll have a fix in the next release. For now, I would suggest just specifying some path for the output_gvcf flags, even if you don't actually need them. Also, would you be able to share why you don't want to use the gVCF file? Normally people will want to merge these callsets, which would require using the gVCFs, so I'd be curious to know if there's a specific use case where you are not needing gVCF output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:217,interoperability,specif,specifying,217,"Hi @jkalleberg thanks raising this issue. You're right that this actually should be optional, this seems to be bug - I've noted it in internally and we'll have a fix in the next release. For now, I would suggest just specifying some path for the output_gvcf flags, even if you don't actually need them. Also, would you be able to share why you don't want to use the gVCF file? Normally people will want to merge these callsets, which would require using the gVCFs, so I'd be curious to know if there's a specific use case where you are not needing gVCF output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:330,interoperability,share,share,330,"Hi @jkalleberg thanks raising this issue. You're right that this actually should be optional, this seems to be bug - I've noted it in internally and we'll have a fix in the next release. For now, I would suggest just specifying some path for the output_gvcf flags, even if you don't actually need them. Also, would you be able to share why you don't want to use the gVCF file? Normally people will want to merge these callsets, which would require using the gVCFs, so I'd be curious to know if there's a specific use case where you are not needing gVCF output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:504,interoperability,specif,specific,504,"Hi @jkalleberg thanks raising this issue. You're right that this actually should be optional, this seems to be bug - I've noted it in internally and we'll have a fix in the next release. For now, I would suggest just specifying some path for the output_gvcf flags, even if you don't actually need them. Also, would you be able to share why you don't want to use the gVCF file? Normally people will want to merge these callsets, which would require using the gVCFs, so I'd be curious to know if there's a specific use case where you are not needing gVCF output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:15,deployability,updat,update,15,"Thanks for the update, @sgoe1. Just wanted to make sure I wasn't interpreting the help page incorrectly. I went ahead and did as you recommended. As for why it's mostly for convenience. I'm wanting to contrast the per-sample output differences across GATK, DeepVariant, and DeepTrio. The fewer method-specific flags required, the cleaner my pipeline for running either (or both) across trios.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:341,deployability,pipelin,pipeline,341,"Thanks for the update, @sgoe1. Just wanted to make sure I wasn't interpreting the help page incorrectly. I went ahead and did as you recommended. As for why it's mostly for convenience. I'm wanting to contrast the per-sample output differences across GATK, DeepVariant, and DeepTrio. The fewer method-specific flags required, the cleaner my pipeline for running either (or both) across trios.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:341,integrability,pipelin,pipeline,341,"Thanks for the update, @sgoe1. Just wanted to make sure I wasn't interpreting the help page incorrectly. I went ahead and did as you recommended. As for why it's mostly for convenience. I'm wanting to contrast the per-sample output differences across GATK, DeepVariant, and DeepTrio. The fewer method-specific flags required, the cleaner my pipeline for running either (or both) across trios.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:301,interoperability,specif,specific,301,"Thanks for the update, @sgoe1. Just wanted to make sure I wasn't interpreting the help page incorrectly. I went ahead and did as you recommended. As for why it's mostly for convenience. I'm wanting to contrast the per-sample output differences across GATK, DeepVariant, and DeepTrio. The fewer method-specific flags required, the cleaner my pipeline for running either (or both) across trios.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:15,safety,updat,update,15,"Thanks for the update, @sgoe1. Just wanted to make sure I wasn't interpreting the help page incorrectly. I went ahead and did as you recommended. As for why it's mostly for convenience. I'm wanting to contrast the per-sample output differences across GATK, DeepVariant, and DeepTrio. The fewer method-specific flags required, the cleaner my pipeline for running either (or both) across trios.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:15,security,updat,update,15,"Thanks for the update, @sgoe1. Just wanted to make sure I wasn't interpreting the help page incorrectly. I went ahead and did as you recommended. As for why it's mostly for convenience. I'm wanting to contrast the per-sample output differences across GATK, DeepVariant, and DeepTrio. The fewer method-specific flags required, the cleaner my pipeline for running either (or both) across trios.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:82,usability,help,help,82,"Thanks for the update, @sgoe1. Just wanted to make sure I wasn't interpreting the help page incorrectly. I went ahead and did as you recommended. As for why it's mostly for convenience. I'm wanting to contrast the per-sample output differences across GATK, DeepVariant, and DeepTrio. The fewer method-specific flags required, the cleaner my pipeline for running either (or both) across trios.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/430:81,deployability,version,version,81,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:123,deployability,upgrad,upgrade,123,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:135,deployability,version,version,135,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:81,integrability,version,version,81,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:135,integrability,version,version,135,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:205,interoperability,specif,specific,205,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:81,modifiability,version,version,81,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:123,modifiability,upgrad,upgrade,123,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:135,modifiability,version,version,135,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:214,performance,time,timeframe,214,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:49,usability,support,support,49,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:155,usability,support,support,155,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:45,deployability,version,versions,45,does this mean we can potentially run future versions of deepvariant-GPU on Windows using Docker Desktop?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:69,energy efficiency,GPU,GPU,69,does this mean we can potentially run future versions of deepvariant-GPU on Windows using Docker Desktop?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:45,integrability,version,versions,45,does this mean we can potentially run future versions of deepvariant-GPU on Windows using Docker Desktop?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:45,modifiability,version,versions,45,does this mean we can potentially run future versions of deepvariant-GPU on Windows using Docker Desktop?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:69,performance,GPU,GPU,69,does this mean we can potentially run future versions of deepvariant-GPU on Windows using Docker Desktop?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:0,reliability,doe,does,0,does this mean we can potentially run future versions of deepvariant-GPU on Windows using Docker Desktop?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:103,deployability,version,version,103,"@husamia technically yes, I believe you should be able to run via Docker on Windows, even with current version (but not with CUDA 11, as mentioned). However, this is not something we explicitly support/test against, so if you have issues running it feel free to let us know, but we may not be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:95,energy efficiency,current,current,95,"@husamia technically yes, I believe you should be able to run via Docker on Windows, even with current version (but not with CUDA 11, as mentioned). However, this is not something we explicitly support/test against, so if you have issues running it feel free to let us know, but we may not be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:103,integrability,version,version,103,"@husamia technically yes, I believe you should be able to run via Docker on Windows, even with current version (but not with CUDA 11, as mentioned). However, this is not something we explicitly support/test against, so if you have issues running it feel free to let us know, but we may not be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:103,modifiability,version,version,103,"@husamia technically yes, I believe you should be able to run via Docker on Windows, even with current version (but not with CUDA 11, as mentioned). However, this is not something we explicitly support/test against, so if you have issues running it feel free to let us know, but we may not be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:202,safety,test,test,202,"@husamia technically yes, I believe you should be able to run via Docker on Windows, even with current version (but not with CUDA 11, as mentioned). However, this is not something we explicitly support/test against, so if you have issues running it feel free to let us know, but we may not be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:202,testability,test,test,202,"@husamia technically yes, I believe you should be able to run via Docker on Windows, even with current version (but not with CUDA 11, as mentioned). However, this is not something we explicitly support/test against, so if you have issues running it feel free to let us know, but we may not be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:194,usability,support,support,194,"@husamia technically yes, I believe you should be able to run via Docker on Windows, even with current version (but not with CUDA 11, as mentioned). However, this is not something we explicitly support/test against, so if you have issues running it feel free to let us know, but we may not be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/430:314,usability,support,support,314,"@husamia technically yes, I believe you should be able to run via Docker on Windows, even with current version (but not with CUDA 11, as mentioned). However, this is not something we explicitly support/test against, so if you have issues running it feel free to let us know, but we may not be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/431:212,deployability,contain,contain,212,"* Could you please check the intermediate directory ```../home/tmp``` to see if examples are generated for all 3 samples? . * Also, could you please check that child BAM is mapped to the right reference and does contain reads overlapping parent1 and parent2 samples?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:29,modifiability,interm,intermediate,29,"* Could you please check the intermediate directory ```../home/tmp``` to see if examples are generated for all 3 samples? . * Also, could you please check that child BAM is mapped to the right reference and does contain reads overlapping parent1 and parent2 samples?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:207,reliability,doe,does,207,"* Could you please check the intermediate directory ```../home/tmp``` to see if examples are generated for all 3 samples? . * Also, could you please check that child BAM is mapped to the right reference and does contain reads overlapping parent1 and parent2 samples?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:134,deployability,contain,contains,134,"> * Could you please check the intermediate directory `../home/tmp` to see if examples are generated for all 3 samples? `../home/tmp` contains 315 files. 105 for each sample. > * Also, could you please check that child BAM is mapped to the right reference and does contain reads overlapping parent1 and parent2 samples? I am not sure what you mean here. The child bam is mapped to the same reference and contains reads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:265,deployability,contain,contain,265,"> * Could you please check the intermediate directory `../home/tmp` to see if examples are generated for all 3 samples? `../home/tmp` contains 315 files. 105 for each sample. > * Also, could you please check that child BAM is mapped to the right reference and does contain reads overlapping parent1 and parent2 samples? I am not sure what you mean here. The child bam is mapped to the same reference and contains reads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:404,deployability,contain,contains,404,"> * Could you please check the intermediate directory `../home/tmp` to see if examples are generated for all 3 samples? `../home/tmp` contains 315 files. 105 for each sample. > * Also, could you please check that child BAM is mapped to the right reference and does contain reads overlapping parent1 and parent2 samples? I am not sure what you mean here. The child bam is mapped to the same reference and contains reads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:31,modifiability,interm,intermediate,31,"> * Could you please check the intermediate directory `../home/tmp` to see if examples are generated for all 3 samples? `../home/tmp` contains 315 files. 105 for each sample. > * Also, could you please check that child BAM is mapped to the right reference and does contain reads overlapping parent1 and parent2 samples? I am not sure what you mean here. The child bam is mapped to the same reference and contains reads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:260,reliability,doe,does,260,"> * Could you please check the intermediate directory `../home/tmp` to see if examples are generated for all 3 samples? `../home/tmp` contains 315 files. 105 for each sample. > * Also, could you please check that child BAM is mapped to the right reference and does contain reads overlapping parent1 and parent2 samples? I am not sure what you mean here. The child bam is mapped to the same reference and contains reads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:272,interoperability,share,share,272,"It looks like examples were created for all the samples but call_variants did not run for the child. . Could you please check the sizes of all files with extension ```tfrecord``` for all samples? To make sure that parents' files are not empty. Is there a chance you could share your BAM files or may be just a chr20? It would be easier to investigate the issue if we could reproduce it locally. Also, could you try to run the test on Ubuntu OS?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:154,modifiability,extens,extension,154,"It looks like examples were created for all the samples but call_variants did not run for the child. . Could you please check the sizes of all files with extension ```tfrecord``` for all samples? To make sure that parents' files are not empty. Is there a chance you could share your BAM files or may be just a chr20? It would be easier to investigate the issue if we could reproduce it locally. Also, could you try to run the test on Ubuntu OS?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:426,safety,test,test,426,"It looks like examples were created for all the samples but call_variants did not run for the child. . Could you please check the sizes of all files with extension ```tfrecord``` for all samples? To make sure that parents' files are not empty. Is there a chance you could share your BAM files or may be just a chr20? It would be easier to investigate the issue if we could reproduce it locally. Also, could you try to run the test on Ubuntu OS?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:426,testability,test,test,426,"It looks like examples were created for all the samples but call_variants did not run for the child. . Could you please check the sizes of all files with extension ```tfrecord``` for all samples? To make sure that parents' files are not empty. Is there a chance you could share your BAM files or may be just a chr20? It would be easier to investigate the issue if we could reproduce it locally. Also, could you try to run the test on Ubuntu OS?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:19647,energy efficiency,cloud,cloud,19647," 17:55 make_examples_parent2.tfrecord-00026-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00027-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00028-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00029-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00030-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00031-of-00052.gz. 1.2G Mar 9 17:56 make_examples_parent2.tfrecord-00032-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00033-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00034-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00035-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00036-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00037-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00038-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00039-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00040-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00041-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00042-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00043-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00044-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00045-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00046-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00047-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00048-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00049-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00050-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00051-of-00052.gz. > Is there a chance you could share your BAM files or may be just a chr20? It would be easier to investigate the issue if we could reproduce it locally. I don't have cloud storage for thi. > Also, could you try to run the test on Ubuntu OS? I can't run it on Ubuntu OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:19511,interoperability,share,share,19511," 17:55 make_examples_parent2.tfrecord-00026-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00027-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00028-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00029-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00030-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00031-of-00052.gz. 1.2G Mar 9 17:56 make_examples_parent2.tfrecord-00032-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00033-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00034-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00035-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00036-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00037-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00038-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00039-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00040-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00041-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00042-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00043-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00044-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00045-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00046-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00047-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00048-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00049-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00050-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00051-of-00052.gz. > Is there a chance you could share your BAM files or may be just a chr20? It would be easier to investigate the issue if we could reproduce it locally. I don't have cloud storage for thi. > Also, could you try to run the test on Ubuntu OS? I can't run it on Ubuntu OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:55,modifiability,extens,extension,55,. > Could you please check the sizes of all files with extension `tfrecord` for all samples? To make sure that parents' files are not empty. the files are non-empty . 422M Mar 10 05:32 call_variants_output_child.tfrecord.gz. 430M Mar 10 17:02 call_variants_output_parent1.tfrecord.gz. 253M Mar 10 23:58 call_variants_output_parent2.tfrecord.gz. 39M Mar 9 17:54 gvcf_child.tfrecord-00000-of-00052.gz. 38M Mar 9 17:54 gvcf_child.tfrecord-00001-of-00052.gz. 39M Mar 9 17:54 gvcf_child.tfrecord-00002-of-00052.gz. 39M Mar 9 17:53 gvcf_child.tfrecord-00003-of-00052.gz. 39M Mar 9 17:56 gvcf_child.tfrecord-00004-of-00052.gz. 38M Mar 9 17:54 gvcf_child.tfrecord-00005-of-00052.gz. 39M Mar 9 17:54 gvcf_child.tfrecord-00006-of-00052.gz. 39M Mar 9 17:54 gvcf_child.tfrecord-00007-of-00052.gz. 38M Mar 9 17:54 gvcf_child.tfrecord-00008-of-00052.gz. 39M Mar 9 17:53 gvcf_child.tfrecord-00009-of-00052.gz. 39M Mar 9 17:53 gvcf_child.tfrecord-00010-of-00052.gz. 39M Mar 9 17:53 gvcf_child.tfrecord-00011-of-00052.gz. 39M Mar 9 17:53 gvcf_child.tfrecord-00012-of-00052.gz. 38M Mar 9 17:54 gvcf_child.tfrecord-00013-of-00052.gz. 38M Mar 9 17:54 gvcf_child.tfrecord-00014-of-00052.gz. 39M Mar 9 17:54 gvcf_child.tfrecord-00015-of-00052.gz. 39M Mar 9 17:54 gvcf_child.tfrecord-00016-of-00052.gz. 38M Mar 9 17:54 gvcf_child.tfrecord-00017-of-00052.gz. 38M Mar 9 17:53 gvcf_child.tfrecord-00018-of-00052.gz. 39M Mar 9 17:53 gvcf_child.tfrecord-00019-of-00052.gz. 39M Mar 9 17:54 gvcf_child.tfrecord-00020-of-00052.gz. 39M Mar 9 17:54 gvcf_child.tfrecord-00021-of-00052.gz. 39M Mar 9 17:53 gvcf_child.tfrecord-00022-of-00052.gz. 39M Mar 9 17:53 gvcf_child.tfrecord-00023-of-00052.gz. 39M Mar 9 17:53 gvcf_child.tfrecord-00024-of-00052.gz. 38M Mar 9 17:55 gvcf_child.tfrecord-00025-of-00052.gz. 39M Mar 9 17:55 gvcf_child.tfrecord-00026-of-00052.gz. 39M Mar 9 17:53 gvcf_child.tfrecord-00027-of-00052.gz. 39M Mar 9 17:54 gvcf_child.tfrecord-00028-of-00052.gz. 38M Mar 9 17:53 gvcf_child.tfrecord-00029-of-00052.gz. 38M M,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:19703,safety,test,test,19703," 17:55 make_examples_parent2.tfrecord-00026-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00027-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00028-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00029-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00030-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00031-of-00052.gz. 1.2G Mar 9 17:56 make_examples_parent2.tfrecord-00032-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00033-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00034-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00035-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00036-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00037-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00038-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00039-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00040-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00041-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00042-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00043-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00044-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00045-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00046-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00047-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00048-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00049-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00050-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00051-of-00052.gz. > Is there a chance you could share your BAM files or may be just a chr20? It would be easier to investigate the issue if we could reproduce it locally. I don't have cloud storage for thi. > Also, could you try to run the test on Ubuntu OS? I can't run it on Ubuntu OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:19703,testability,test,test,19703," 17:55 make_examples_parent2.tfrecord-00026-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00027-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00028-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00029-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00030-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00031-of-00052.gz. 1.2G Mar 9 17:56 make_examples_parent2.tfrecord-00032-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00033-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00034-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00035-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00036-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00037-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00038-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00039-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00040-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00041-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00042-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00043-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00044-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00045-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00046-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00047-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00048-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00049-of-00052.gz. 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00050-of-00052.gz. 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00051-of-00052.gz. > Is there a chance you could share your BAM files or may be just a chr20? It would be easier to investigate the issue if we could reproduce it locally. I don't have cloud storage for thi. > Also, could you try to run the test on Ubuntu OS? I can't run it on Ubuntu OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:136,deployability,stage,stage,136,From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually. . To generate a VCF:. ```. /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa. ```. To generate a g.VCF:. ```. /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz. ```. These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:154,deployability,pipelin,pipeline,154,From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually. . To generate a VCF:. ```. /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa. ```. To generate a g.VCF:. ```. /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz. ```. These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:154,integrability,pipelin,pipeline,154,From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually. . To generate a VCF:. ```. /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa. ```. To generate a g.VCF:. ```. /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz. ```. These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:799,usability,command,commands,799,From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually. . To generate a VCF:. ```. /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa. ```. To generate a g.VCF:. ```. /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz. ```. These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:916,usability,command,command,916,From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually. . To generate a VCF:. ```. /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa. ```. To generate a g.VCF:. ```. /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz. ```. These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:138,deployability,stage,stage,138,> From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually. > . > To generate a VCF:. > . > ```. > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa. > ```. > . > To generate a g.VCF:. > . > ```. > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz. > ```. > . > These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run. This suggestion works for me. Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:156,deployability,pipelin,pipeline,156,> From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually. > . > To generate a VCF:. > . > ```. > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa. > ```. > . > To generate a g.VCF:. > . > ```. > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz. > ```. > . > These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run. This suggestion works for me. Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:156,integrability,pipelin,pipeline,156,> From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually. > . > To generate a VCF:. > . > ```. > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa. > ```. > . > To generate a g.VCF:. > . > ```. > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz. > ```. > . > These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run. This suggestion works for me. Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:837,usability,command,commands,837,> From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually. > . > To generate a VCF:. > . > ```. > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa. > ```. > . > To generate a g.VCF:. > . > ```. > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz. > ```. > . > These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run. This suggestion works for me. Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:954,usability,command,command,954,> From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually. > . > To generate a VCF:. > . > ```. > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa. > ```. > . > To generate a g.VCF:. > . > ```. > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz. > ```. > . > These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run. This suggestion works for me. Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/432:140,availability,down,down,140,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1334,availability,error,errors,1334,"--image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://gi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1717,availability,checkpoint,checkpoint,1717,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:2048,availability,checkpoint,checkpoint,2048,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:483,deployability,updat,update,483,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:505,deployability,instal,install,505,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1249,deployability,log,log,1249,"copes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, anothe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1844,deployability,version,version,1844,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:282,energy efficiency,cloud,cloud-platform,282,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:364,energy efficiency,cloud,cloud,364,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1734,energy efficiency,model,models,1734,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1745,energy efficiency,model,model,1745,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:2082,energy efficiency,model,models,2082,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:2138,energy efficiency,model,model,2138,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1844,integrability,version,version,1844,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:288,interoperability,platform,platform,288,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:2199,interoperability,specif,specific,2199,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1844,modifiability,version,version,1844,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:2315,modifiability,variab,variables,2315,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:2541,modifiability,variab,variables,2541,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1334,performance,error,errors,1334,"--image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://gi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1717,reliability,checkpoint,checkpoint,1717,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:2048,reliability,checkpoint,checkpoint,2048,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:239,safety,test,test,239,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:483,safety,updat,update,483,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:731,safety,input,input,731,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:879,safety,input,input,879,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:928,safety,input,input,928,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1249,safety,log,log,1249,"copes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, anothe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1316,safety,compl,completed,1316,"tu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variab",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1334,safety,error,errors,1334,"--image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://gi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1438,safety,input,input,1438,"nto the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether thos",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:436,security,ssh,ssh,436,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:476,security,apt,apt,476,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:483,security,updat,update,483,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:498,security,apt,apt,498,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1249,security,log,log,1249,"copes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, anothe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1316,security,compl,completed,1316,"tu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variab",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1734,security,model,models,1734,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1745,security,model,model,1745,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:2082,security,model,models,2082,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:2138,security,model,model,2138,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:239,testability,test,test,239,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:901,testability,unit,unittest,901,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1249,testability,log,log,1249,"copes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, anothe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:148,usability,command,commands,148,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:233,usability,USER,USER,233,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:670,usability,command,command,670,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:731,usability,input,input,731,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:879,usability,input,input,879,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:928,usability,input,input,928,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine . ```. gcloud compute instances create ""${USER}-test"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1334,usability,error,errors,1334,"--image-project ""ubuntu-os-cloud"" \. --machine-type ""e2-medium"" \. --zone ""us-west1-b"". ```. After ssh into the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://gi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1438,usability,input,input,1438,"nto the machine, I ran:. ```. sudo apt -y update && sudo apt -y install docker.io. ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether thos",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:1892,usability,command,command,1892,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:2231,usability,command,command,2231,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:2566,usability,help,helps,2566,"github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1 \. --call_variants_extra_args=""use_openvino=true"" \. 2>&1 | tee /tmp/deepvariant.log. ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --use_openvino. ```. which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:. `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command? And, another pointer for you:. In our Dockerfile, we set these environment variables:. https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:69,deployability,instal,installation,69,"Thanks a lot @pichuan, I am working on this - apparently my OpenVino installation wasn't good. Your answer was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:116,usability,help,helpful,116,"Thanks a lot @pichuan, I am working on this - apparently my OpenVino installation wasn't good. Your answer was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/433:962,availability,slo,slower,962,"Hi @maryawood , thanks for the question. As you probably noticed in the code, `very_sensitive_caller` is the default that we use for candidate generation. In all our released models, this is the default so there is no need to change it. `vcf_candidate_importer` is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for `--proposed_variants`, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. . Our team might be a bit slower to answer specific questions on this, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:166,deployability,releas,released,166,"Hi @maryawood , thanks for the question. As you probably noticed in the code, `very_sensitive_caller` is the default that we use for candidate generation. In all our released models, this is the default so there is no need to change it. `vcf_candidate_importer` is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for `--proposed_variants`, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. . Our team might be a bit slower to answer specific questions on this, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:175,energy efficiency,model,models,175,"Hi @maryawood , thanks for the question. As you probably noticed in the code, `very_sensitive_caller` is the default that we use for candidate generation. In all our released models, this is the default so there is no need to change it. `vcf_candidate_importer` is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for `--proposed_variants`, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. . Our team might be a bit slower to answer specific questions on this, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:756,integrability,sub,subtle,756,"Hi @maryawood , thanks for the question. As you probably noticed in the code, `very_sensitive_caller` is the default that we use for candidate generation. In all our released models, this is the default so there is no need to change it. `vcf_candidate_importer` is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for `--proposed_variants`, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. . Our team might be a bit slower to answer specific questions on this, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:979,interoperability,specif,specific,979,"Hi @maryawood , thanks for the question. As you probably noticed in the code, `very_sensitive_caller` is the default that we use for candidate generation. In all our released models, this is the default so there is no need to change it. `vcf_candidate_importer` is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for `--proposed_variants`, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. . Our team might be a bit slower to answer specific questions on this, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:962,reliability,slo,slower,962,"Hi @maryawood , thanks for the question. As you probably noticed in the code, `very_sensitive_caller` is the default that we use for candidate generation. In all our released models, this is the default so there is no need to change it. `vcf_candidate_importer` is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for `--proposed_variants`, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. . Our team might be a bit slower to answer specific questions on this, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:175,security,model,models,175,"Hi @maryawood , thanks for the question. As you probably noticed in the code, `very_sensitive_caller` is the default that we use for candidate generation. In all our released models, this is the default so there is no need to change it. `vcf_candidate_importer` is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for `--proposed_variants`, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. . Our team might be a bit slower to answer specific questions on this, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:942,security,team,team,942,"Hi @maryawood , thanks for the question. As you probably noticed in the code, `very_sensitive_caller` is the default that we use for candidate generation. In all our released models, this is the default so there is no need to change it. `vcf_candidate_importer` is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for `--proposed_variants`, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. . Our team might be a bit slower to answer specific questions on this, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:332,usability,user,users,332,"Hi @maryawood , thanks for the question. As you probably noticed in the code, `very_sensitive_caller` is the default that we use for candidate generation. In all our released models, this is the default so there is no need to change it. `vcf_candidate_importer` is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for `--proposed_variants`, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. . Our team might be a bit slower to answer specific questions on this, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:709,usability,document,documentation,709,"Hi @maryawood , thanks for the question. As you probably noticed in the code, `very_sensitive_caller` is the default that we use for candidate generation. In all our released models, this is the default so there is no need to change it. `vcf_candidate_importer` is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for `--proposed_variants`, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. . Our team might be a bit slower to answer specific questions on this, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:1034,usability,support,support,1034,"Hi @maryawood , thanks for the question. As you probably noticed in the code, `very_sensitive_caller` is the default that we use for candidate generation. In all our released models, this is the default so there is no need to change it. `vcf_candidate_importer` is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for `--proposed_variants`, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. . Our team might be a bit slower to answer specific questions on this, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:211,availability,avail,available,211,"Hi @pichuan, thanks for the explanation! I've been working on using DeepVariant to train my own sequencing-type specific model, and I'm getting to the point of wanting to mess with some of the different options available to improve performance. It sounds like `vcf_candidate_importer` would be less relevant for this purpose, but what about `unspecified_caller`? How does that option differ from `very_sensitive_caller`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:121,energy efficiency,model,model,121,"Hi @pichuan, thanks for the explanation! I've been working on using DeepVariant to train my own sequencing-type specific model, and I'm getting to the point of wanting to mess with some of the different options available to improve performance. It sounds like `vcf_candidate_importer` would be less relevant for this purpose, but what about `unspecified_caller`? How does that option differ from `very_sensitive_caller`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:112,interoperability,specif,specific,112,"Hi @pichuan, thanks for the explanation! I've been working on using DeepVariant to train my own sequencing-type specific model, and I'm getting to the point of wanting to mess with some of the different options available to improve performance. It sounds like `vcf_candidate_importer` would be less relevant for this purpose, but what about `unspecified_caller`? How does that option differ from `very_sensitive_caller`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:224,performance,improve perform,improve performance,224,"Hi @pichuan, thanks for the explanation! I've been working on using DeepVariant to train my own sequencing-type specific model, and I'm getting to the point of wanting to mess with some of the different options available to improve performance. It sounds like `vcf_candidate_importer` would be less relevant for this purpose, but what about `unspecified_caller`? How does that option differ from `very_sensitive_caller`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:211,reliability,availab,available,211,"Hi @pichuan, thanks for the explanation! I've been working on using DeepVariant to train my own sequencing-type specific model, and I'm getting to the point of wanting to mess with some of the different options available to improve performance. It sounds like `vcf_candidate_importer` would be less relevant for this purpose, but what about `unspecified_caller`? How does that option differ from `very_sensitive_caller`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:367,reliability,doe,does,367,"Hi @pichuan, thanks for the explanation! I've been working on using DeepVariant to train my own sequencing-type specific model, and I'm getting to the point of wanting to mess with some of the different options available to improve performance. It sounds like `vcf_candidate_importer` would be less relevant for this purpose, but what about `unspecified_caller`? How does that option differ from `very_sensitive_caller`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:211,safety,avail,available,211,"Hi @pichuan, thanks for the explanation! I've been working on using DeepVariant to train my own sequencing-type specific model, and I'm getting to the point of wanting to mess with some of the different options available to improve performance. It sounds like `vcf_candidate_importer` would be less relevant for this purpose, but what about `unspecified_caller`? How does that option differ from `very_sensitive_caller`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:121,security,model,model,121,"Hi @pichuan, thanks for the explanation! I've been working on using DeepVariant to train my own sequencing-type specific model, and I'm getting to the point of wanting to mess with some of the different options available to improve performance. It sounds like `vcf_candidate_importer` would be less relevant for this purpose, but what about `unspecified_caller`? How does that option differ from `very_sensitive_caller`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:211,security,availab,available,211,"Hi @pichuan, thanks for the explanation! I've been working on using DeepVariant to train my own sequencing-type specific model, and I'm getting to the point of wanting to mess with some of the different options available to improve performance. It sounds like `vcf_candidate_importer` would be less relevant for this purpose, but what about `unspecified_caller`? How does that option differ from `very_sensitive_caller`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:232,usability,perform,performance,232,"Hi @pichuan, thanks for the explanation! I've been working on using DeepVariant to train my own sequencing-type specific model, and I'm getting to the point of wanting to mess with some of the different options available to improve performance. It sounds like `vcf_candidate_importer` would be less relevant for this purpose, but what about `unspecified_caller`? How does that option differ from `very_sensitive_caller`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:170,availability,error,error,170,"`unspecified_caller` is an invalid option. I've never actually specified it, but I think the behavior should be that it should crash earlier (hopefully with a meaningful error message). . To train your own sequencing-type specific model, generally if the sequencer's base error rate is not too high, the default `very_sensitive_caller` should just work. But, if it's tricker cases like ONT reads, having a separate candidate generation process which is smarter about proposing candidates, and then using `vcf_candidate_importer` is the right way to go. @maryawood Can you say a bit more about why the default `vcf_candidate_importer` doesn't work for you? Do you get very imbalanced training data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
https://github.com/google/deepvariant/issues/433:272,availability,error,error,272,"`unspecified_caller` is an invalid option. I've never actually specified it, but I think the behavior should be that it should crash earlier (hopefully with a meaningful error message). . To train your own sequencing-type specific model, generally if the sequencer's base error rate is not too high, the default `very_sensitive_caller` should just work. But, if it's tricker cases like ONT reads, having a separate candidate generation process which is smarter about proposing candidates, and then using `vcf_candidate_importer` is the right way to go. @maryawood Can you say a bit more about why the default `vcf_candidate_importer` doesn't work for you? Do you get very imbalanced training data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433
