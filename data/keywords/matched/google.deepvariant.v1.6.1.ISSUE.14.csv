id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/491:29739,interoperability,platform,platform,29739,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038989: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039093: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039179: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038440: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039229: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039085: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039153: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038776: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039068: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:29886,interoperability,platform,platform,29886,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039093: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039179: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038440: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039229: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039085: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039153: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038776: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039068: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:30033,interoperability,platform,platform,30033,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039179: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038440: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039229: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039085: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039153: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038776: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039068: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:30180,interoperability,platform,platform,30180,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038440: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039229: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039085: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039153: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038776: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039068: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:30327,interoperability,platform,platform,30327,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039229: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039085: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039153: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038776: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039068: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:30474,interoperability,platform,platform,30474,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039085: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039153: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038776: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039068: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:30621,interoperability,platform,platform,30621,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039153: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038776: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039068: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:30768,interoperability,platform,platform,30768,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039153: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038776: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039068: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:30915,interoperability,platform,platform,30915,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038776: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039068: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:31062,interoperability,platform,platform,31062,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039068: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:31209,interoperability,platform,platform,31209,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:31356,interoperability,platform,platform,31356,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:31503,interoperability,platform,platform,31503,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:31650,interoperability,platform,platform,31650,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:31797,interoperability,platform,platform,31797,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039108: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:31944,interoperability,platform,platform,31944,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038426: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:32091,interoperability,platform,platform,32091,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039091: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:32238,interoperability,platform,platform,32238,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039091: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039235: I te,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:32385,interoperability,platform,platform,32385,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039091: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039235: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. I1103 14:40:01.136240 1402823247,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:32532,interoperability,platform,platform,32532,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039091: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039235: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. I1103 14:40:01.136240 140282324772672 make_examples_core.py:163] Task 57/64: Preparing inputs. .... I1103 14:53:05.451765 139769163196224 make_examples_core.py:163] Task 25/64: 23,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:32679,interoperability,platform,platform,32679,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039091: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039235: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. I1103 14:40:01.136240 140282324772672 make_examples_core.py:163] Task 57/64: Preparing inputs. .... I1103 14:53:05.451765 139769163196224 make_examples_core.py:163] Task 25/64: 2313 candidates (2405 examples) [0.99s elapsed]. I1103 14:53:04.698359 140152994068288 make_examples_core.py:163] Task 4/64: 2129 candidates (2243 ex,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:32826,interoperability,platform,platform,32826,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039091: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039235: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. I1103 14:40:01.136240 140282324772672 make_examples_core.py:163] Task 57/64: Preparing inputs. .... I1103 14:53:05.451765 139769163196224 make_examples_core.py:163] Task 25/64: 2313 candidates (2405 examples) [0.99s elapsed]. I1103 14:53:04.698359 140152994068288 make_examples_core.py:163] Task 4/64: 2129 candidates (2243 examples) [13.75s elapsed]. I1103 14:53:05.625952 139672941975360 make_examples_core.py:163] Task 52/64: 2418 candidates (2540 examples) [10.88s elap,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:32973,interoperability,platform,platform,32973,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039091: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039235: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. I1103 14:40:01.136240 140282324772672 make_examples_core.py:163] Task 57/64: Preparing inputs. .... I1103 14:53:05.451765 139769163196224 make_examples_core.py:163] Task 25/64: 2313 candidates (2405 examples) [0.99s elapsed]. I1103 14:53:04.698359 140152994068288 make_examples_core.py:163] Task 4/64: 2129 candidates (2243 examples) [13.75s elapsed]. I1103 14:53:05.625952 139672941975360 make_examples_core.py:163] Task 52/64: 2418 candidates (2540 examples) [10.88s elapsed]. I1103 14:53:05.330440 140126785840960 make_examples_core.py:163] Task 16/64: 2220 candidates (2282 examples) [1.44s elapsed]. I1103 14:53:05.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:33120,interoperability,platform,platform,33120,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039091: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039235: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. I1103 14:40:01.136240 140282324772672 make_examples_core.py:163] Task 57/64: Preparing inputs. .... I1103 14:53:05.451765 139769163196224 make_examples_core.py:163] Task 25/64: 2313 candidates (2405 examples) [0.99s elapsed]. I1103 14:53:04.698359 140152994068288 make_examples_core.py:163] Task 4/64: 2129 candidates (2243 examples) [13.75s elapsed]. I1103 14:53:05.625952 139672941975360 make_examples_core.py:163] Task 52/64: 2418 candidates (2540 examples) [10.88s elapsed]. I1103 14:53:05.330440 140126785840960 make_examples_core.py:163] Task 16/64: 2220 candidates (2282 examples) [1.44s elapsed]. I1103 14:53:05.493952 140529397729088 make_examples_core.py:163] Task 44/64: 2443 candidates (2523 examples) [2.77s elapsed]. I1103 14:53:06.154630 14052939772908,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:33267,interoperability,platform,platform,33267,] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039091: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039235: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. I1103 14:40:01.136240 140282324772672 make_examples_core.py:163] Task 57/64: Preparing inputs. .... I1103 14:53:05.451765 139769163196224 make_examples_core.py:163] Task 25/64: 2313 candidates (2405 examples) [0.99s elapsed]. I1103 14:53:04.698359 140152994068288 make_examples_core.py:163] Task 4/64: 2129 candidates (2243 examples) [13.75s elapsed]. I1103 14:53:05.625952 139672941975360 make_examples_core.py:163] Task 52/64: 2418 candidates (2540 examples) [10.88s elapsed]. I1103 14:53:05.330440 140126785840960 make_examples_core.py:163] Task 16/64: 2220 candidates (2282 examples) [1.44s elapsed]. I1103 14:53:05.493952 140529397729088 make_examples_core.py:163] Task 44/64: 2443 candidates (2523 examples) [2.77s elapsed]. I1103 14:53:06.154630 140529397729088 make_examples_core.py:163] Task 44/64: 2510 candidates (2594 examples) [0.66s elapsed]. I1103 14:53:06.968048 140435694630720 make_examples_core.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:396,modifiability,version,version,396,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:1621,modifiability,MODUL,MODULE,1621,"**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2632,modifiability,MODUL,MODULE,2632,"D. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25]",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:6898,modifiability,paramet,parameters,6898,"9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9849,modifiability,MODUL,MODULE,9849,"ing results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:22683,modifiability,interm,intermediate,22683,"h38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_pileup ""none"" --gvcf ""/cromwell_root/pepper_output/dv_intermediate_outputs/gvcf.tfrecord@64.gz"" --min_bas",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:22988,modifiability,Interm,Intermediate,22988,"riant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_pileup ""none"" --gvcf ""/cromwell_root/pepper_output/dv_intermediate_outputs/gvcf.tfrecord@64.gz"" --min_base_quality ""1"" --min_mapping_quality ""1"" --parse_sam_aux_fields --proposed_variants ""/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --norealign_reads --sample_name ""6061-SL-0029"" --sort_by_haplotypes --variant_caller ""vcf_candidate_importer"" --task {} ). 2021-11-03 14:39:57.038890: I tensorflow/stre",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:29,performance,disk,disk,29,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:226,performance,error,error,226,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:256,performance,disk,disk,256,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:1282,performance,cach,cacheCopy,1282,"ating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:1367,performance,resourc,resources,1367,":r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_an",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:1941,performance,time,time,1941, of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2220,performance,cach,cacheCopy,2220,"d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CON",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2305,performance,resourc,resources,2305,"14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INT",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2681,performance,PROFIL,PROFILE,2681,"OLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3376,performance,TIME,TIME,3376,"_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3460,performance,TIME,TIME,3460,"per_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PRO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3540,performance,TIME,TIME,3540,"ot/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3699,performance,TIME,TIME,3699,"ANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3946,performance,CPU,CPU,3946,"O: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: P",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4089,performance,LOAD,LOADING,4089,"hr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4134,performance,BATCH,BATCHES,4134,"hr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] IN",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4186,performance,BATCH,BATCHES,4186,"1-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUN",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4239,performance,BATCH,BATCHES,4239,"ERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4292,performance,BATCH,BATCHES,4292," 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4345,performance,BATCH,BATCHES,4345,"483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4398,performance,BATCH,BATCHES,4398,"1-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4451,performance,BATCH,BATCHES,4451,"PSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /crom",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4621,performance,TIME,TIME,4621,"21 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4741,performance,TIME,TIME,4741,"-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_roo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:5235,performance,TIME,TIME,5235,": BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:5946,performance,time,time,5946,"ROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:6211,performance,cach,cacheCopy,6211,":02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:6293,performance,resourc,resources,6293,"579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7403,performance,time,time,7403,"PPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remain",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7473,performance,time,time,7473,"ag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remainin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7543,performance,time,time,7543,"SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7614,performance,time,time,7614,"ag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7685,performance,time,time,7685,"tput/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7756,performance,time,time,7756,"-@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7827,performance,time,time,7827,"tagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. >",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7898,performance,time,time,7898,"rameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7969,performance,time,time,7969,"json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8039,performance,time,time,8039,"t/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8110,performance,time,time,8110,"115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8181,performance,time,time,8181," 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplota",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8252,performance,time,time,8252," resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8323,performance,time,time,8323,"> Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8394,performance,time,time,8394,"imated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8462,performance,time,time,8462,"stimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.hap",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8531,performance,time,time,8531,"Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resource",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8600,performance,time,time,8600,". Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8669,performance,time,time,8669,"2). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8738,performance,time,time,8738,"342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8807,performance,time,time,8807,"/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9369,performance,time,time,9369," (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9525,performance,resourc,resources,9525," time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THRE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9914,performance,PROFIL,PROFILE,9914,"ook 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10625,performance,TIME,TIME,10625,"models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED T",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10715,performance,TIME,TIME,10715,"9 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10805,performance,TIME,TIME,10805," [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPS",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10895,performance,TIME,TIME,10895," VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10986,performance,TIME,TIME,10986,"021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [EL",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11077,performance,TIME,TIME,11077,"1305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [E",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11168,performance,TIME,TIME,11168," CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11259,performance,TIME,TIME,11259,"hr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11350,performance,TIME,TIME,11350,"r20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11442,performance,TIME,TIME,11442,25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11534,performance,TIME,TIME,11534,FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11626,performance,TIME,TIME,11626,E: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11718,performance,TIME,TIME,11718, 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11810,performance,TIME,TIME,11810, Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11902,performance,TIME,TIME,11902,in 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11994,performance,TIME,TIME,11994,n 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12086,performance,TIME,TIME,12086, 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12178,performance,TIME,TIME,12178,0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12270,performance,TIME,TIME,12270, Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (6,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12362,performance,TIME,TIME,12362,Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12454,performance,TIME,TIME,12454,Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12546,performance,TIME,TIME,12546,Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12638,performance,TIME,TIME,12638,Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLET,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12730,performance,TIME,TIME,12730,Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12822,performance,TIME,TIME,12822,Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12914,performance,TIME,TIME,12914,Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13006,performance,TIME,TIME,13006,Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13099,performance,TIME,TIME,13099,ec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLET,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13192,performance,TIME,TIME,13192,c]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13285,performance,TIME,TIME,13285,]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13378,performance,TIME,TIME,13378,. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13471,performance,TIME,TIME,13471, [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (8,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13564,performance,TIME,TIME,13564,[11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13657,performance,TIME,TIME,13657,11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13750,performance,TIME,TIME,13750,1-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13842,performance,TIME,TIME,13842,1-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13934,performance,TIME,TIME,13934,1-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14026,performance,TIME,TIME,14026,11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [1,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14118,performance,TIME,TIME,14118,[11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIM,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14210,performance,TIME,TIME,14210, [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14302,performance,TIME,TIME,14302,. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14394,performance,TIME,TIME,14394,]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14486,performance,TIME,TIME,14486,c]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14578,performance,TIME,TIME,14578,ec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PR,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14670,performance,TIME,TIME,14670,Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] IN,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14762,performance,TIME,TIME,14762,Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-202,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14854,performance,TIME,TIME,14854,Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14946,performance,TIME,TIME,14946,Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15117,performance,TIME,TIME,15117,IME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 6,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15330,performance,CPU,CPU,15330,1 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THRE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15473,performance,LOAD,LOADING,15473,[ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFU,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15518,performance,BATCH,BATCHES,15518,7:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFULLY. [11-03-2021 14:31:24] INFO: THREAD 21 FI,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15570,performance,BATCH,BATCHES,15570,SED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFULLY. [11-03-2021 14:31:24] INFO: THREAD 21 FINISHED SUCCESSFULLY. [11-03-2021 14:31:25] INFO: THR,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15623,performance,BATCH,BATCHES,15623,THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFULLY. [11-03-2021 14:31:24] INFO: THREAD 21 FINISHED SUCCESSFULLY. [11-03-2021 14:31:25] INFO: THREAD 61 FINISHED SUCCESSFULLY. [11-03-2021 14:31:26] I,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15676,performance,BATCH,BATCHES,15676,n 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFULLY. [11-03-2021 14:31:24] INFO: THREAD 21 FINISHED SUCCESSFULLY. [11-03-2021 14:31:25] INFO: THREAD 61 FINISHED SUCCESSFULLY. [11-03-2021 14:31:26] INFO: THREAD 42 FINISHED SUCCESSFULLY. [11-03-2021 14:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15729,performance,BATCH,BATCHES,15729,/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFULLY. [11-03-2021 14:31:24] INFO: THREAD 21 FINISHED SUCCESSFULLY. [11-03-2021 14:31:25] INFO: THREAD 61 FINISHED SUCCESSFULLY. [11-03-2021 14:31:26] INFO: THREAD 42 FINISHED SUCCESSFULLY. [11-03-2021 14:31:29] INFO: THREAD 62 FINISHED SUCCESSFULLY. [11-03-,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15782,performance,BATCH,BATCHES,15782,03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFULLY. [11-03-2021 14:31:24] INFO: THREAD 21 FINISHED SUCCESSFULLY. [11-03-2021 14:31:25] INFO: THREAD 61 FINISHED SUCCESSFULLY. [11-03-2021 14:31:26] INFO: THREAD 42 FINISHED SUCCESSFULLY. [11-03-2021 14:31:29] INFO: THREAD 62 FINISHED SUCCESSFULLY. [11-03-2021 14:31:31] INFO: THREAD 40 FINISHED SUCCESSFULLY.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15835,performance,BATCH,BATCHES,15835,(97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFULLY. [11-03-2021 14:31:24] INFO: THREAD 21 FINISHED SUCCESSFULLY. [11-03-2021 14:31:25] INFO: THREAD 61 FINISHED SUCCESSFULLY. [11-03-2021 14:31:26] INFO: THREAD 42 FINISHED SUCCESSFULLY. [11-03-2021 14:31:29] INFO: THREAD 62 FINISHED SUCCESSFULLY. [11-03-2021 14:31:31] INFO: THREAD 40 FINISHED SUCCESSFULLY. [11-03-2021 14:31:32] INFO: THREAD 14 FINISHED SUCCE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15888,performance,BATCH,BATCHES,15888,11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFULLY. [11-03-2021 14:31:24] INFO: THREAD 21 FINISHED SUCCESSFULLY. [11-03-2021 14:31:25] INFO: THREAD 61 FINISHED SUCCESSFULLY. [11-03-2021 14:31:26] INFO: THREAD 42 FINISHED SUCCESSFULLY. [11-03-2021 14:31:29] INFO: THREAD 62 FINISHED SUCCESSFULLY. [11-03-2021 14:31:31] INFO: THREAD 40 FINISHED SUCCESSFULLY. [11-03-2021 14:31:32] INFO: THREAD 14 FINISHED SUCCESSFULLY. [11-03-2021 14:31:38] INFO: THREAD 39 FINISH,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15941,performance,BATCH,BATCHES,15941, TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFULLY. [11-03-2021 14:31:24] INFO: THREAD 21 FINISHED SUCCESSFULLY. [11-03-2021 14:31:25] INFO: THREAD 61 FINISHED SUCCESSFULLY. [11-03-2021 14:31:26] INFO: THREAD 42 FINISHED SUCCESSFULLY. [11-03-2021 14:31:29] INFO: THREAD 62 FINISHED SUCCESSFULLY. [11-03-2021 14:31:31] INFO: THREAD 40 FINISHED SUCCESSFULLY. [11-03-2021 14:31:32] INFO: THREAD 14 FINISHED SUCCESSFULLY. [11-03-2021 14:31:38] INFO: THREAD 39 FINISHED SUCCESSFULLY. [11-03-2021 14:31:39] INFO: THREAD 3,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15994,performance,BATCH,BATCHES,15994,AD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFULLY. [11-03-2021 14:31:24] INFO: THREAD 21 FINISHED SUCCESSFULLY. [11-03-2021 14:31:25] INFO: THREAD 61 FINISHED SUCCESSFULLY. [11-03-2021 14:31:26] INFO: THREAD 42 FINISHED SUCCESSFULLY. [11-03-2021 14:31:29] INFO: THREAD 62 FINISHED SUCCESSFULLY. [11-03-2021 14:31:31] INFO: THREAD 40 FINISHED SUCCESSFULLY. [11-03-2021 14:31:32] INFO: THREAD 14 FINISHED SUCCESSFULLY. [11-03-2021 14:31:38] INFO: THREAD 39 FINISHED SUCCESSFULLY. [11-03-2021 14:31:39] INFO: THREAD 36 FINISHED SUCCESSFULLY. [11-03-2021 14:31:39] INFO: ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:16047,performance,BATCH,BATCHES,16047,O: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFULLY. [11-03-2021 14:31:24] INFO: THREAD 21 FINISHED SUCCESSFULLY. [11-03-2021 14:31:25] INFO: THREAD 61 FINISHED SUCCESSFULLY. [11-03-2021 14:31:26] INFO: THREAD 42 FINISHED SUCCESSFULLY. [11-03-2021 14:31:29] INFO: THREAD 62 FINISHED SUCCESSFULLY. [11-03-2021 14:31:31] INFO: THREAD 40 FINISHED SUCCESSFULLY. [11-03-2021 14:31:32] INFO: THREAD 14 FINISHED SUCCESSFULLY. [11-03-2021 14:31:38] INFO: THREAD 39 FINISHED SUCCESSFULLY. [11-03-2021 14:31:39] INFO: THREAD 36 FINISHED SUCCESSFULLY. [11-03-2021 14:31:39] INFO: THREAD 44 FINISHED SUCCESSFULLY. [11-03-2021 14:31:39,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:16100,performance,BATCH,BATCHES,16100,NFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFULLY. [11-03-2021 14:31:24] INFO: THREAD 21 FINISHED SUCCESSFULLY. [11-03-2021 14:31:25] INFO: THREAD 61 FINISHED SUCCESSFULLY. [11-03-2021 14:31:26] INFO: THREAD 42 FINISHED SUCCESSFULLY. [11-03-2021 14:31:29] INFO: THREAD 62 FINISHED SUCCESSFULLY. [11-03-2021 14:31:31] INFO: THREAD 40 FINISHED SUCCESSFULLY. [11-03-2021 14:31:32] INFO: THREAD 14 FINISHED SUCCESSFULLY. [11-03-2021 14:31:38] INFO: THREAD 39 FINISHED SUCCESSFULLY. [11-03-2021 14:31:39] INFO: THREAD 36 FINISHED SUCCESSFULLY. [11-03-2021 14:31:39] INFO: THREAD 44 FINISHED SUCCESSFULLY. [11-03-2021 14:31:39] INFO: THREAD 31 FINISHED SUCCESSFULLY. [11-03-2021 ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:16153,performance,BATCH,BATCHES,16153, STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SUCCESSFULLY. [11-03-2021 14:31:24] INFO: THREAD 21 FINISHED SUCCESSFULLY. [11-03-2021 14:31:25] INFO: THREAD 61 FINISHED SUCCESSFULLY. [11-03-2021 14:31:26] INFO: THREAD 42 FINISHED SUCCESSFULLY. [11-03-2021 14:31:29] INFO: THREAD 62 FINISHED SUCCESSFULLY. [11-03-2021 14:31:31] INFO: THREAD 40 FINISHED SUCCESSFULLY. [11-03-2021 14:31:32] INFO: THREAD 14 FINISHED SUCCESSFULLY. [11-03-2021 14:31:38] INFO: THREAD 39 FINISHED SUCCESSFULLY. [11-03-2021 14:31:39] INFO: THREAD 36 FINISHED SUCCESSFULLY. [11-03-2021 14:31:39] INFO: THREAD 44 FINISHED SUCCESSFULLY. [11-03-2021 14:31:39] INFO: THREAD 31 FINISHED SUCCESSFULLY. [11-03-2021 14:31:39] INFO: THREAD 55 FINISHED SUCCESSFULLY. [11-,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:20157,performance,TIME,TIME,20157,"[11-03-2021 14:31:57] INFO: THREAD 25 FINISHED SUCCESSFULLY. [11-03-2021 14:31:58] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:32:00] INFO: THREAD 52 FINISHED SUCCESSFULLY. [11-03-2021 14:32:00] INFO: THREAD 32 FINISHED SUCCESSFULLY. [11-03-2021 14:32:01] INFO: THREAD 23 FINISHED SUCCESSFULLY. [11-03-2021 14:32:03] INFO: THREAD 30 FINISHED SUCCESSFULLY. [11-03-2021 14:32:03] INFO: THREAD 43 FINISHED SUCCESSFULLY. [11-03-2021 14:32:03] INFO: THREAD 11 FINISHED SUCCESSFULLY. [11-03-2021 14:32:04] INFO: THREAD 1 FINISHED SUCCESSFULLY. [11-03-2021 14:32:07] INFO: THREAD 13 FINISHED SUCCESSFULLY. [11-03-2021 14:32:07] INFO: THREAD 59 FINISHED SUCCESSFULLY. [11-03-2021 14:32:12] INFO: THREAD 57 FINISHED SUCCESSFULLY. [11-03-2021 14:32:20] INFO: THREAD 47 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 27 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 29 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: FINISHED PREDICTION. [11-03-2021 14:32:23] INFO: ELAPSED TIME: 14 Min 14 Sec. [11-03-2021 14:32:23] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 14:32:23] TOTAL ELAPSED TIME FOR INFERENCE: 14 Min 14 Sec. [11-03-2021 14:32:23] STEP 3.1: CALLING VARIANTS. [11-03-2021 14:32:23] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/. [11-03-2021 14:32:23] INFO: PROCESSING HAPLOTAG: 0. [11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10. [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec. [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:20278,performance,TIME,TIME,20278,"[11-03-2021 14:32:00] INFO: THREAD 52 FINISHED SUCCESSFULLY. [11-03-2021 14:32:00] INFO: THREAD 32 FINISHED SUCCESSFULLY. [11-03-2021 14:32:01] INFO: THREAD 23 FINISHED SUCCESSFULLY. [11-03-2021 14:32:03] INFO: THREAD 30 FINISHED SUCCESSFULLY. [11-03-2021 14:32:03] INFO: THREAD 43 FINISHED SUCCESSFULLY. [11-03-2021 14:32:03] INFO: THREAD 11 FINISHED SUCCESSFULLY. [11-03-2021 14:32:04] INFO: THREAD 1 FINISHED SUCCESSFULLY. [11-03-2021 14:32:07] INFO: THREAD 13 FINISHED SUCCESSFULLY. [11-03-2021 14:32:07] INFO: THREAD 59 FINISHED SUCCESSFULLY. [11-03-2021 14:32:12] INFO: THREAD 57 FINISHED SUCCESSFULLY. [11-03-2021 14:32:20] INFO: THREAD 47 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 27 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 29 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: FINISHED PREDICTION. [11-03-2021 14:32:23] INFO: ELAPSED TIME: 14 Min 14 Sec. [11-03-2021 14:32:23] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 14:32:23] TOTAL ELAPSED TIME FOR INFERENCE: 14 Min 14 Sec. [11-03-2021 14:32:23] STEP 3.1: CALLING VARIANTS. [11-03-2021 14:32:23] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/. [11-03-2021 14:32:23] INFO: PROCESSING HAPLOTAG: 0. [11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10. [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec. [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; . rm -rf /cromwell_r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:20638,performance,TIME,TIME,20638,"ULLY. [11-03-2021 14:32:04] INFO: THREAD 1 FINISHED SUCCESSFULLY. [11-03-2021 14:32:07] INFO: THREAD 13 FINISHED SUCCESSFULLY. [11-03-2021 14:32:07] INFO: THREAD 59 FINISHED SUCCESSFULLY. [11-03-2021 14:32:12] INFO: THREAD 57 FINISHED SUCCESSFULLY. [11-03-2021 14:32:20] INFO: THREAD 47 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 27 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 29 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: FINISHED PREDICTION. [11-03-2021 14:32:23] INFO: ELAPSED TIME: 14 Min 14 Sec. [11-03-2021 14:32:23] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 14:32:23] TOTAL ELAPSED TIME FOR INFERENCE: 14 Min 14 Sec. [11-03-2021 14:32:23] STEP 3.1: CALLING VARIANTS. [11-03-2021 14:32:23] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/. [11-03-2021 14:32:23] INFO: PROCESSING HAPLOTAG: 0. [11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10. [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec. [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_hp/. -------. [11-03-2021 14:39:53] INFO: [7/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output/dv_intermediate_outputs/; . echo ""STARTING DEEPVARIANT""; . time /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:20807,performance,TIME,TIME,20807,"SHED SUCCESSFULLY. [11-03-2021 14:32:12] INFO: THREAD 57 FINISHED SUCCESSFULLY. [11-03-2021 14:32:20] INFO: THREAD 47 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 27 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 29 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: FINISHED PREDICTION. [11-03-2021 14:32:23] INFO: ELAPSED TIME: 14 Min 14 Sec. [11-03-2021 14:32:23] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 14:32:23] TOTAL ELAPSED TIME FOR INFERENCE: 14 Min 14 Sec. [11-03-2021 14:32:23] STEP 3.1: CALLING VARIANTS. [11-03-2021 14:32:23] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/. [11-03-2021 14:32:23] INFO: PROCESSING HAPLOTAG: 0. [11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10. [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec. [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_hp/. -------. [11-03-2021 14:39:53] INFO: [7/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output/dv_intermediate_outputs/; . echo ""STARTING DEEPVARIANT""; . time /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_M",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:20869,performance,TIME,TIME,20869,"HED SUCCESSFULLY. [11-03-2021 14:32:20] INFO: THREAD 47 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 27 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 29 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: FINISHED PREDICTION. [11-03-2021 14:32:23] INFO: ELAPSED TIME: 14 Min 14 Sec. [11-03-2021 14:32:23] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 14:32:23] TOTAL ELAPSED TIME FOR INFERENCE: 14 Min 14 Sec. [11-03-2021 14:32:23] STEP 3.1: CALLING VARIANTS. [11-03-2021 14:32:23] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/. [11-03-2021 14:32:23] INFO: PROCESSING HAPLOTAG: 0. [11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10. [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec. [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_hp/. -------. [11-03-2021 14:39:53] INFO: [7/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output/dv_intermediate_outputs/; . echo ""STARTING DEEPVARIANT""; . time /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_outpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:21490,performance,time,time,21490,"[11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10. [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec. [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_hp/. -------. [11-03-2021 14:39:53] INFO: [7/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output/dv_intermediate_outputs/; . echo ""STARTING DEEPVARIANT""; . time /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:21665,performance,resourc,resources,21665,"2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_hp/. -------. [11-03-2021 14:39:53] INFO: [7/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output/dv_intermediate_outputs/; . echo ""STARTING DEEPVARIANT""; . time /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:22922,performance,load,load,22922,"output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_pileup ""none"" --gvcf ""/cromwell_root/pepper_output/dv_intermediate_outputs/gvcf.tfrecord@64.gz"" --min_base_quality ""1"" --min_mapping_quality ""1"" --parse_sam_aux_fields --proposed_variants ""/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --norealign_reads --sample_name ""6061-SL-0029"" --sort_by_haplotypes --variant_caller ""vcf_candidat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:23134,performance,time,time,23134,"ds=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_pileup ""none"" --gvcf ""/cromwell_root/pepper_output/dv_intermediate_outputs/gvcf.tfrecord@64.gz"" --min_base_quality ""1"" --min_mapping_quality ""1"" --parse_sam_aux_fields --proposed_variants ""/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --norealign_reads --sample_name ""6061-SL-0029"" --sort_by_haplotypes --variant_caller ""vcf_candidate_importer"" --task {} ). 2021-11-03 14:39:57.038890: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038671: I tensorflow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:23150,performance,parallel,parallel,23150,"les_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_pileup ""none"" --gvcf ""/cromwell_root/pepper_output/dv_intermediate_outputs/gvcf.tfrecord@64.gz"" --min_base_quality ""1"" --min_mapping_quality ""1"" --parse_sam_aux_fields --proposed_variants ""/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --norealign_reads --sample_name ""6061-SL-0029"" --sort_by_haplotypes --variant_caller ""vcf_candidate_importer"" --task {} ). 2021-11-03 14:39:57.038890: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038671: I tensorflow/stream_executor/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:23287,performance,resourc,resources,23287,"_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_pileup ""none"" --gvcf ""/cromwell_root/pepper_output/dv_intermediate_outputs/gvcf.tfrecord@64.gz"" --min_base_quality ""1"" --min_mapping_quality ""1"" --parse_sam_aux_fields --proposed_variants ""/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --norealign_reads --sample_name ""6061-SL-0029"" --sort_by_haplotypes --variant_caller ""vcf_candidate_importer"" --task {} ). 2021-11-03 14:39:57.038890: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038671: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039009: I tensorflow/stream_e",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35820,performance,disk,disk,35820,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35966,performance,error,error,35966,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:36188,performance,disk,disk,36188,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:705,reliability,fail,failed,705,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35765,reliability,fail,failed,35765,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35842,reliability,Doe,Does,35842,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35925,reliability,fail,fail,35925,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:36103,reliability,fail,failed,36103,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:18,safety,predict,predicting,18,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:226,safety,error,error,226,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:688,safety,log,log,688,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:951,safety,log,log,951,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:1367,safety,resourc,resources,1367,":r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_an",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:1621,safety,MODUL,MODULE,1621,"**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:1791,safety,log,logs,1791,". -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2305,safety,resourc,resources,2305,"14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INT",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2559,safety,log,logs,2559,"d_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2577,safety,log,log,2577,"1-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:4",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2632,safety,MODUL,MODULE,2632,"D. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25]",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3353,safety,COMPL,COMPLETE,3353,"RCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROC",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3436,safety,COMPL,COMPLETE,3436," -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3516,safety,COMPL,COMPLETE,3516,">&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3817,safety,PREDICT,PREDICTION,3817,"t/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CAND",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4573,safety,PREDICT,PREDICTION,4573,"49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4669,safety,PREDICT,PREDICTION,4669," 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4858,safety,PREDICT,PREDICTION,4858,"t/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:6293,safety,resourc,resources,6293,"579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:6596,safety,log,logs,6596,"utput/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:6619,safety,log,log,6619,"vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7333,safety,compl,complete,7333,"405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7373,safety,compl,complete,7373,"omwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7443,safety,compl,complete,7443,"ms/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7512,safety,compl,complete,7512,"_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7583,safety,compl,complete,7583,"r_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7654,safety,compl,complete,7654,".bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7725,safety,compl,complete,7725,"tagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). E",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7796,safety,compl,complete,7796,"HASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Est",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7867,safety,compl,complete,7867,"4 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7938,safety,compl,complete,7938,"/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8008,safety,compl,complete,8008,"m /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotype",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8079,safety,compl,complete,8079,"egion, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8150,safety,compl,complete,8150,"NDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8221,safety,compl,complete,8221,"000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_outpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8292,safety,compl,complete,8292,"g chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8363,safety,compl,complete,8363,"ng 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8431,safety,compl,complete,8431,"hing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8499,safety,compl,complete,8499,"shing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-ds",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8568,safety,compl,complete,8568,"ishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8637,safety,compl,complete,8637,"olishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8706,safety,compl,complete,8706," Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8775,safety,compl,complete,8775," > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/lo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9525,safety,resourc,resources,9525," time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THRE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9777,safety,log,logs,9777,"lete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/4",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9794,safety,log,log,9794," Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9849,safety,MODUL,MODULE,9849,"ing results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10602,safety,COMPL,COMPLETE,10602,"-t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMP",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10692,safety,COMPL,COMPLETE,10692,"er_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 CO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10782,safety,COMPL,COMPLETE,10782,"pper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10872,safety,COMPL,COMPLETE,10872,"1 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/48",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10962,safety,COMPL,COMPLETE,10962,"32021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11053,safety,COMPL,COMPLETE,11053,"_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11144,safety,COMPL,COMPLETE,11144,"14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 18",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11235,safety,COMPL,COMPLETE,11235,"7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11326,safety,COMPL,COMPLETE,11326," 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11418,safety,COMPL,COMPLETE,11418, INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11510,safety,COMPL,COMPLETE,11510,5] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11602,safety,COMPL,COMPLETE,11602,LETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11694,safety,COMPL,COMPLETE,11694,TE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11786,safety,COMPL,COMPLETE,11786, (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11878,safety,COMPL,COMPLETE,11878,8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11970,safety,COMPL,COMPLETE,11970,%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12062,safety,COMPL,COMPLETE,12062,) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00],MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12154,safety,COMPL,COMPLETE,12154, [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12246,safety,COMPL,COMPLETE,12246,[ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 0,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12338,safety,COMPL,COMPLETE,12338,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12430,safety,COMPL,COMPLETE,12430,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12522,safety,COMPL,COMPLETE,12522,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREA,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12614,safety,COMPL,COMPLETE,12614,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THRE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12706,safety,COMPL,COMPLETE,12706,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THR,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12798,safety,COMPL,COMPLETE,12798,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THR,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12890,safety,COMPL,COMPLETE,12890,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THR,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12982,safety,COMPL,COMPLETE,12982,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THR,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13075,safety,COMPL,COMPLETE,13075,LAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THRE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13168,safety,COMPL,COMPLETE,13168,APSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREA,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13261,safety,COMPL,COMPLETE,13261,PSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13354,safety,COMPL,COMPLETE,13354,SED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13447,safety,COMPL,COMPLETE,13447,ED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 0,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13540,safety,COMPL,COMPLETE,13540,D TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13633,safety,COMPL,COMPLETE,13633, TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00],MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13726,safety,COMPL,COMPLETE,13726,TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13818,safety,COMPL,COMPLETE,13818,TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13910,safety,COMPL,COMPLETE,13910,TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14002,safety,COMPL,COMPLETE,14002,TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FIN,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14094,safety,COMPL,COMPLETE,14094, TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14186,safety,COMPL,COMPLETE,14186,D TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-0,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14278,safety,COMPL,COMPLETE,14278,ED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_1,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14370,safety,COMPL,COMPLETE,14370,SED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14462,safety,COMPL,COMPLETE,14462,PSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14554,safety,COMPL,COMPLETE,14554,APSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:2,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14646,safety,COMPL,COMPLETE,14646,LAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [1,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14738,safety,COMPL,COMPLETE,14738,ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14830,safety,COMPL,COMPLETE,14830,ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14922,safety,COMPL,COMPLETE,14922,ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 1,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:20109,safety,PREDICT,PREDICTION,20109,"1:55] INFO: THREAD 50 FINISHED SUCCESSFULLY. [11-03-2021 14:31:57] INFO: THREAD 25 FINISHED SUCCESSFULLY. [11-03-2021 14:31:58] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:32:00] INFO: THREAD 52 FINISHED SUCCESSFULLY. [11-03-2021 14:32:00] INFO: THREAD 32 FINISHED SUCCESSFULLY. [11-03-2021 14:32:01] INFO: THREAD 23 FINISHED SUCCESSFULLY. [11-03-2021 14:32:03] INFO: THREAD 30 FINISHED SUCCESSFULLY. [11-03-2021 14:32:03] INFO: THREAD 43 FINISHED SUCCESSFULLY. [11-03-2021 14:32:03] INFO: THREAD 11 FINISHED SUCCESSFULLY. [11-03-2021 14:32:04] INFO: THREAD 1 FINISHED SUCCESSFULLY. [11-03-2021 14:32:07] INFO: THREAD 13 FINISHED SUCCESSFULLY. [11-03-2021 14:32:07] INFO: THREAD 59 FINISHED SUCCESSFULLY. [11-03-2021 14:32:12] INFO: THREAD 57 FINISHED SUCCESSFULLY. [11-03-2021 14:32:20] INFO: THREAD 47 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 27 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 29 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: FINISHED PREDICTION. [11-03-2021 14:32:23] INFO: ELAPSED TIME: 14 Min 14 Sec. [11-03-2021 14:32:23] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 14:32:23] TOTAL ELAPSED TIME FOR INFERENCE: 14 Min 14 Sec. [11-03-2021 14:32:23] STEP 3.1: CALLING VARIANTS. [11-03-2021 14:32:23] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/. [11-03-2021 14:32:23] INFO: PROCESSING HAPLOTAG: 0. [11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10. [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec. [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/P",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:20206,safety,PREDICT,PREDICTION,20206,"SSFULLY. [11-03-2021 14:31:58] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:32:00] INFO: THREAD 52 FINISHED SUCCESSFULLY. [11-03-2021 14:32:00] INFO: THREAD 32 FINISHED SUCCESSFULLY. [11-03-2021 14:32:01] INFO: THREAD 23 FINISHED SUCCESSFULLY. [11-03-2021 14:32:03] INFO: THREAD 30 FINISHED SUCCESSFULLY. [11-03-2021 14:32:03] INFO: THREAD 43 FINISHED SUCCESSFULLY. [11-03-2021 14:32:03] INFO: THREAD 11 FINISHED SUCCESSFULLY. [11-03-2021 14:32:04] INFO: THREAD 1 FINISHED SUCCESSFULLY. [11-03-2021 14:32:07] INFO: THREAD 13 FINISHED SUCCESSFULLY. [11-03-2021 14:32:07] INFO: THREAD 59 FINISHED SUCCESSFULLY. [11-03-2021 14:32:12] INFO: THREAD 57 FINISHED SUCCESSFULLY. [11-03-2021 14:32:20] INFO: THREAD 47 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 27 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 29 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: FINISHED PREDICTION. [11-03-2021 14:32:23] INFO: ELAPSED TIME: 14 Min 14 Sec. [11-03-2021 14:32:23] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 14:32:23] TOTAL ELAPSED TIME FOR INFERENCE: 14 Min 14 Sec. [11-03-2021 14:32:23] STEP 3.1: CALLING VARIANTS. [11-03-2021 14:32:23] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/. [11-03-2021 14:32:23] INFO: PROCESSING HAPLOTAG: 0. [11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10. [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec. [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . tabix -p vcf /cro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:21665,safety,resourc,resources,21665,"2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_hp/. -------. [11-03-2021 14:39:53] INFO: [7/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output/dv_intermediate_outputs/; . echo ""STARTING DEEPVARIANT""; . time /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:22539,safety,log,logs,22539,"ype=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfreco",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:22558,safety,log,log,22558,"d_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_h",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:23287,safety,resourc,resources,23287,"_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_pileup ""none"" --gvcf ""/cromwell_root/pepper_output/dv_intermediate_outputs/gvcf.tfrecord@64.gz"" --min_base_quality ""1"" --min_mapping_quality ""1"" --parse_sam_aux_fields --proposed_variants ""/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --norealign_reads --sample_name ""6061-SL-0029"" --sort_by_haplotypes --variant_caller ""vcf_candidate_importer"" --task {} ). 2021-11-03 14:39:57.038890: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038671: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039009: I tensorflow/stream_e",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:33444,safety,input,inputs,33444, library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039091: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039235: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. I1103 14:40:01.136240 140282324772672 make_examples_core.py:163] Task 57/64: Preparing inputs. .... I1103 14:53:05.451765 139769163196224 make_examples_core.py:163] Task 25/64: 2313 candidates (2405 examples) [0.99s elapsed]. I1103 14:53:04.698359 140152994068288 make_examples_core.py:163] Task 4/64: 2129 candidates (2243 examples) [13.75s elapsed]. I1103 14:53:05.625952 139672941975360 make_examples_core.py:163] Task 52/64: 2418 candidates (2540 examples) [10.88s elapsed]. I1103 14:53:05.330440 140126785840960 make_examples_core.py:163] Task 16/64: 2220 candidates (2282 examples) [1.44s elapsed]. I1103 14:53:05.493952 140529397729088 make_examples_core.py:163] Task 44/64: 2443 candidates (2523 examples) [2.77s elapsed]. I1103 14:53:06.154630 140529397729088 make_examples_core.py:163] Task 44/64: 2510 candidates (2594 examples) [0.66s elapsed]. I1103 14:53:06.968048 140435694630720 make_examples_core.py:163] Task 26/64: 2303 candidates (2427 examples) [4.70s elapsed]. I1103 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35782,safety,input,input,35782,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35863,safety,test,test,35863,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35897,safety,input,inputs,35897,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35966,safety,error,error,35966,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:36074,safety,input,inputs,36074,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:36177,safety,predict,predicting,36177,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:36264,safety,log,log,36264,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:688,security,log,log,688,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:951,security,log,log,951,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:1791,security,log,logs,1791,". -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2559,security,log,logs,2559,"d_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2577,security,log,log,2577,"1-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:4",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3353,security,COMPL,COMPLETE,3353,"RCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROC",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3436,security,COMPL,COMPLETE,3436," -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:3516,security,COMPL,COMPLETE,3516,">&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4083,security,MODEL,MODEL,4083,"14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:44:25] FINISHED IMAGE GENERATION. [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec. [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE. [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/. [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64. [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1. [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX. [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35. [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35. [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35. [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:6596,security,log,logs,6596,"utput/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:6619,security,log,log,6619,"vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:6892,security,model,model,6892,"NFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7333,security,compl,complete,7333,"405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7373,security,compl,complete,7373,"omwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7443,security,compl,complete,7443,"ms/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7512,security,compl,complete,7512,"_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7583,security,compl,complete,7583,"r_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7654,security,compl,complete,7654,".bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7725,security,compl,complete,7725,"tagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). E",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7796,security,compl,complete,7796,"HASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Est",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7867,security,compl,complete,7867,"4 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:7938,security,compl,complete,7938,"/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8008,security,compl,complete,8008,"m /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotype",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8079,security,compl,complete,8079,"egion, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8150,security,compl,complete,8150,"NDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8221,security,compl,complete,8221,"000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_outpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8292,security,compl,complete,8292,"g chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8363,security,compl,complete,8363,"ng 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8431,security,compl,complete,8431,"hing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8499,security,compl,complete,8499,"shing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-ds",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8568,security,compl,complete,8568,"ishing 14% complete (191/1342). Estimated time remaining: 2m 39s. > Polishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8637,security,compl,complete,8637,"olishing 17% complete (233/1342). Estimated time remaining: 2m 36s. > Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8706,security,compl,complete,8706," Polishing 19% complete (259/1342). Estimated time remaining: 2m 33s. > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:8775,security,compl,complete,8775," > Polishing 25% complete (338/1342). Estimated time remaining: 2m 24s. > Polishing 31% complete (418/1342). Estimated time remaining: 2m 15s. > Polishing 36% complete (487/1342). Estimated time remaining: 2m 2s. > Polishing 40% complete (547/1342). Estimated time remaining: 1m 57s. > Polishing 45% complete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/lo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9777,security,log,logs,9777,"lete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/4",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9794,security,log,log,9794," Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10602,security,COMPL,COMPLETE,10602,"-t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMP",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10692,security,COMPL,COMPLETE,10692,"er_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 CO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10782,security,COMPL,COMPLETE,10782,"pper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10872,security,COMPL,COMPLETE,10872,"1 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/48",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:10962,security,COMPL,COMPLETE,10962,"32021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11053,security,COMPL,COMPLETE,11053,"_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11144,security,COMPL,COMPLETE,11144,"14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 18",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11235,security,COMPL,COMPLETE,11235,"7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11326,security,COMPL,COMPLETE,11326," 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11418,security,COMPL,COMPLETE,11418, INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11510,security,COMPL,COMPLETE,11510,5] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11602,security,COMPL,COMPLETE,11602,LETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11694,security,COMPL,COMPLETE,11694,TE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11786,security,COMPL,COMPLETE,11786, (6%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 40/483 COMPLETE (8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11878,security,COMPL,COMPLETE,11878,8%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 50/483 COMPLETE (10%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:11970,security,COMPL,COMPLETE,11970,%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 60/483 COMPLETE (12%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12062,security,COMPL,COMPLETE,12062,) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 70/483 COMPLETE (14%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00],MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12154,security,COMPL,COMPLETE,12154, [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 80/483 COMPLETE (16%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12246,security,COMPL,COMPLETE,12246,[ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 90/483 COMPLETE (18%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 0,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12338,security,COMPL,COMPLETE,12338,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 100/483 COMPLETE (20%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12430,security,COMPL,COMPLETE,12430,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 110/483 COMPLETE (22%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12522,security,COMPL,COMPLETE,12522,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 120/483 COMPLETE (24%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREA,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12614,security,COMPL,COMPLETE,12614,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 130/483 COMPLETE (26%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THRE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12706,security,COMPL,COMPLETE,12706,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 140/483 COMPLETE (28%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THR,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12798,security,COMPL,COMPLETE,12798,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 150/483 COMPLETE (31%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THR,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12890,security,COMPL,COMPLETE,12890,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 160/483 COMPLETE (33%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THR,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:12982,security,COMPL,COMPLETE,12982,ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 170/483 COMPLETE (35%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THR,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13075,security,COMPL,COMPLETE,13075,LAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 180/483 COMPLETE (37%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THRE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13168,security,COMPL,COMPLETE,13168,APSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 190/483 COMPLETE (39%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREA,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13261,security,COMPL,COMPLETE,13261,PSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 200/483 COMPLETE (41%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13354,security,COMPL,COMPLETE,13354,SED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 210/483 COMPLETE (43%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13447,security,COMPL,COMPLETE,13447,ED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 220/483 COMPLETE (45%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 0,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13540,security,COMPL,COMPLETE,13540,D TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 230/483 COMPLETE (47%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13633,security,COMPL,COMPLETE,13633, TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 240/483 COMPLETE (49%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00],MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13726,security,COMPL,COMPLETE,13726,TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 250/483 COMPLETE (51%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13818,security,COMPL,COMPLETE,13818,TIME: 0 Min 1 Sec]. [11-03-2021 14:13:07] INFO: [THREAD 00] 260/483 COMPLETE (53%) [ELAPSED TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:13910,security,COMPL,COMPLETE,13910,TIME: 0 Min 1 Sec]. [11-03-2021 14:14:45] INFO: [THREAD 00] 270/483 COMPLETE (55%) [ELAPSED TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14002,security,COMPL,COMPLETE,14002,TIME: 1 Min 39 Sec]. [11-03-2021 14:16:39] INFO: [THREAD 00] 280/483 COMPLETE (57%) [ELAPSED TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FIN,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14094,security,COMPL,COMPLETE,14094, TIME: 3 Min 33 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 290/483 COMPLETE (60%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14186,security,COMPL,COMPLETE,14186,D TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 300/483 COMPLETE (62%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-0,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14278,security,COMPL,COMPLETE,14278,ED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 310/483 COMPLETE (64%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_1,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14370,security,COMPL,COMPLETE,14370,SED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 320/483 COMPLETE (66%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14462,security,COMPL,COMPLETE,14462,PSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 330/483 COMPLETE (68%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14554,security,COMPL,COMPLETE,14554,APSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:02] INFO: [THREAD 00] 340/483 COMPLETE (70%) [ELAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:2,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14646,security,COMPL,COMPLETE,14646,LAPSED TIME: 3 Min 56 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 350/483 COMPLETE (72%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [1,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14738,security,COMPL,COMPLETE,14738,ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 360/483 COMPLETE (74%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCE,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14830,security,COMPL,COMPLETE,14830,ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 370/483 COMPLETE (76%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:14922,security,COMPL,COMPLETE,14922,ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 380/483 COMPLETE (78%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 390/483 COMPLETE (80%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 400/483 COMPLETE (82%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 410/483 COMPLETE (84%) [ELAPSED TIME: 4 Min 4 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 420/483 COMPLETE (86%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:10] INFO: [THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 1,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15467,security,MODEL,MODEL,15467, (89%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]. [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION. [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec. [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE. [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/. [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP. [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64. [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1. [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX. [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66. [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66. [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66. [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66. [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66. [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66. [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66. [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66. [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66. [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66. [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66. [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66. [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66. [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY. [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY. [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY. [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY. [11-03-2021 14:31:22] INFO: THREAD 15 FINISHED SU,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:21597,security,model,model,21597," TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec. [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_hp/. -------. [11-03-2021 14:39:53] INFO: [7/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output/dv_intermediate_outputs/; . echo ""STARTING DEEPVARIANT""; . time /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:22539,security,log,logs,22539,"ype=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfreco",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:22558,security,log,log,22558,"d_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_h",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:22881,security,model,model,22881,"ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_pileup ""none"" --gvcf ""/cromwell_root/pepper_output/dv_intermediate_outputs/gvcf.tfrecord@64.gz"" --min_base_quality ""1"" --min_mapping_quality ""1"" --parse_sam_aux_fields --proposed_variants ""/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --norealign_reads --sample_name ""6061-SL-0029"" --sort_by_h",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:22956,security,model,model,22956,"utput/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_pileup ""none"" --gvcf ""/cromwell_root/pepper_output/dv_intermediate_outputs/gvcf.tfrecord@64.gz"" --min_base_quality ""1"" --min_mapping_quality ""1"" --parse_sam_aux_fields --proposed_variants ""/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --norealign_reads --sample_name ""6061-SL-0029"" --sort_by_haplotypes --variant_caller ""vcf_candidate_importer"" --task {} ). 2021-11-03",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:36264,security,log,log,36264,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:688,testability,log,log,688,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:951,testability,log,log,951,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:1367,testability,resourc,resources,1367,":r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_an",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:1791,testability,log,logs,1791,". -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2305,testability,resourc,resources,2305,"14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INT",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2559,testability,log,logs,2559,"d_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2577,testability,log,log,2577,"1-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895. [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. ... [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]. [11-03-2021 13:42:4",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:6293,testability,resourc,resources,6293,"579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:6596,testability,log,logs,6596,"utput/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:6619,testability,log,log,6619,"vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json. > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL. > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks. > Ordering chunks by estimated depth. > Setup complete, beginning run. > Polishing 3% complete (46/1342). Estimated time remaining: unknown. > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s. > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s. > Polishing 14% complete (191/1342). Estimated time re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9525,testability,resourc,resources,9525," time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THRE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9777,testability,log,logs,9777,"lete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/4",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9794,testability,log,log,9794," Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']. [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376. [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS. [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]. [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:21665,testability,resourc,resources,21665,"2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_hp/. -------. [11-03-2021 14:39:53] INFO: [7/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output/dv_intermediate_outputs/; . echo ""STARTING DEEPVARIANT""; . time /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:22539,testability,log,logs,22539,"ype=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfreco",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:22558,testability,log,log,22558,"d_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_h",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:23287,testability,resourc,resources,23287,"_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_pileup ""none"" --gvcf ""/cromwell_root/pepper_output/dv_intermediate_outputs/gvcf.tfrecord@64.gz"" --min_base_quality ""1"" --min_mapping_quality ""1"" --parse_sam_aux_fields --proposed_variants ""/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --norealign_reads --sample_name ""6061-SL-0029"" --sort_by_haplotypes --variant_caller ""vcf_candidate_importer"" --task {} ). 2021-11-03 14:39:57.038890: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038671: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039009: I tensorflow/stream_e",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35863,testability,test,test,35863,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:36033,testability,context,context,36033,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:36264,testability,log,log,36264,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:187,usability,workflow,workflows,187,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:226,usability,error,error,226,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:242,usability,indicat,indicates,242,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:647,usability,command,command,647,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:697,usability,command,command,697,"Empirical formula predicting disk space usage?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**. - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`. - Installation method (Docker, built from source, etc.): Docker. - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```. # This is the command from Pepper, but judged from the log, the command failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:1694,usability,COMMAND,COMMAND,1694,"ommand failed during the DV stage. run_pepper_margin_deepvariant \. call_variant \. -b ~{bam} \. -f ~{ref_fasta} \. -t ""${num_core}"" \. -s ""${SM}"" \. -o ""~{output_root}"" \. -p ""~{prefix}"" \. --gvcf \. --phased_output \. --ont. ```. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:1923,usability,COMMAND,COMMAND,1923,`. Relevant part of the log file (which is over 200MB):. ```. run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont. [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED. [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output; . mkdir -p /cromwell_root/pepper_output/logs; . mkdir -p /cromwell_root/pepper_output/intermediate_files;. -------. [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log. -------. [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING. [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041. [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/. [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES. [11-,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:5293,usability,user,user,5293,"CHES PROCESSED 20/35. [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35. [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:5383,usability,COMMAND,COMMAND,5383,":59] INFO: BATCHES PROCESSED 30/35. [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35. [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY. [11-03-2021 13:50:44] INFO: FINISHED PREDICTION. [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec. [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec. [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES. [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/. [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_roo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:5928,usability,COMMAND,COMMAND,5928,"3:50:44] INFO: PROCESSING CONTIG: chr10. [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013. [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14. [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092. [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s. user	579m29.953s. sys	11m32.825s. [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_snp/; . echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; . zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq. -------. CONTIGS FOUND IN PEPPER SNP VCF:. chr10. chr14. [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND. -------. time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;. mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; . samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam. -------. Running OpenMP with 64 threads. > Parsing model parameters from file: /opt/margin_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9078,usability,user,user,9078,"omplete (605/1342). Estimated time remaining: 1m 45s. > Polishing 48% complete (650/1342). Estimated time remaining: 1m 39s. > Polishing 54% complete (725/1342). Estimated time remaining: 1m 27s. > Polishing 60% complete (812/1342). Estimated time remaining: 1m 16s. > Polishing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:9351,usability,COMMAND,COMMAND,9351,"ing 68% complete (913/1342). Estimated time remaining: 59s. > Polishing 73% complete (984/1342). Estimated time remaining: 50s. > Polishing 75% complete (1015/1342). Estimated time remaining: 47s. > Polishing 77% complete (1036/1342). Estimated time remaining: 43s. > Polishing 78% complete (1058/1342). Estimated time remaining: 41s. > Polishing 84% complete (1138/1342). Estimated time remaining: 30s. > Polishing 92% complete (1235/1342). Estimated time remaining: 14s. > Starting merge. > Merging results from 1342 chunks. > Merging took 7s. > Merge cleanup took 0s. Separated reads with divisions: H1 475116, H2 453908, and H0 159194. > Wrote haplotyped bams in 1m 43s. > Finished phasing in 18m 46s. real	18m47.373s. user	245m51.236s. sys	2m8.903s. mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file. [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND. -------. time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log. -------. [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED. [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET. [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305. [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/. [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:. [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:20927,usability,user,user,20927,"NISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 27 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: THREAD 29 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: FINISHED PREDICTION. [11-03-2021 14:32:23] INFO: ELAPSED TIME: 14 Min 14 Sec. [11-03-2021 14:32:23] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 14:32:23] TOTAL ELAPSED TIME FOR INFERENCE: 14 Min 14 Sec. [11-03-2021 14:32:23] STEP 3.1: CALLING VARIANTS. [11-03-2021 14:32:23] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/. [11-03-2021 14:32:23] INFO: PROCESSING HAPLOTAG: 0. [11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10. [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec. [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_hp/. -------. [11-03-2021 14:39:53] INFO: [7/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output/dv_intermediate_outputs/; . echo ""STARTING DEEPVARIANT""; . time /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --outpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:21018,usability,COMMAND,COMMAND,21018," 14:32:23] INFO: THREAD 29 FINISHED SUCCESSFULLY. [11-03-2021 14:32:23] INFO: FINISHED PREDICTION. [11-03-2021 14:32:23] INFO: ELAPSED TIME: 14 Min 14 Sec. [11-03-2021 14:32:23] INFO: PREDICTION FINISHED SUCCESSFULLY. . [11-03-2021 14:32:23] TOTAL ELAPSED TIME FOR INFERENCE: 14 Min 14 Sec. [11-03-2021 14:32:23] STEP 3.1: CALLING VARIANTS. [11-03-2021 14:32:23] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/. [11-03-2021 14:32:23] INFO: PROCESSING HAPLOTAG: 0. [11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10. [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec. [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_hp/. -------. [11-03-2021 14:39:53] INFO: [7/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output/dv_intermediate_outputs/; . echo ""STARTING DEEPVARIANT""; . time /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --samp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:21375,usability,COMMAND,COMMAND,21375,"2:23] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/. [11-03-2021 14:32:23] INFO: PROCESSING HAPLOTAG: 0. [11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10. [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec. [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14. [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec. [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s. user	1220m53.668s. sys	15m35.409s. [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND. -------. mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; . tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; . rm -rf /cromwell_root/pepper_output/pepper_hp/. -------. [11-03-2021 14:39:53] INFO: [7/9] RUNNING THE FOLLOWING COMMAND. -------. mkdir -p /cromwell_root/pepper_output/dv_intermediate_outputs/; . echo ""STARTING DEEPVARIANT""; . time /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_varian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:23117,usability,command,command,23117,"uts/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log. -------. STARTING DEEPVARIANT. I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/. I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****. ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_pileup ""none"" --gvcf ""/cromwell_root/pepper_output/dv_intermediate_outputs/gvcf.tfrecord@64.gz"" --min_base_quality ""1"" --min_mapping_quality ""1"" --parse_sam_aux_fields --proposed_variants ""/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --norealign_reads --sample_name ""6061-SL-0029"" --sort_by_haplotypes --variant_caller ""vcf_candidate_importer"" --task {} ). 2021-11-03 14:39:57.038890: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.03867",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:33444,usability,input,inputs,33444, library libcudart.so.10.1. 2021-11-03 14:39:57.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.038426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039091: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. 2021-11-03 14:39:57.039235: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1. I1103 14:40:01.136240 140282324772672 make_examples_core.py:163] Task 57/64: Preparing inputs. .... I1103 14:53:05.451765 139769163196224 make_examples_core.py:163] Task 25/64: 2313 candidates (2405 examples) [0.99s elapsed]. I1103 14:53:04.698359 140152994068288 make_examples_core.py:163] Task 4/64: 2129 candidates (2243 examples) [13.75s elapsed]. I1103 14:53:05.625952 139672941975360 make_examples_core.py:163] Task 52/64: 2418 candidates (2540 examples) [10.88s elapsed]. I1103 14:53:05.330440 140126785840960 make_examples_core.py:163] Task 16/64: 2220 candidates (2282 examples) [1.44s elapsed]. I1103 14:53:05.493952 140529397729088 make_examples_core.py:163] Task 44/64: 2443 candidates (2523 examples) [2.77s elapsed]. I1103 14:53:06.154630 140529397729088 make_examples_core.py:163] Task 44/64: 2510 candidates (2594 examples) [0.66s elapsed]. I1103 14:53:06.968048 140435694630720 make_examples_core.py:163] Task 26/64: 2303 candidates (2427 examples) [4.70s elapsed]. I1103 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35782,usability,input,input,35782,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35897,usability,input,inputs,35897,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35951,usability,workflow,workflow,35951,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35966,usability,error,error,35966,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:36074,usability,input,inputs,36074,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]. I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]. I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]. I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]. I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]. I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]. I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]. I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]. I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]. I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]. I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]. # the program died here. ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks! Steve.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/492:604,availability,Error,Error,604,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:492,deployability,version,version,492,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10801,deployability,modul,module,10801,"M input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12194,deployability,Fail,Failed,12194,"ner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:13045,deployability,modul,module,13045,"files_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:14438,deployability,Fail,Failed,14438,"ner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:15289,deployability,modul,module,15289,"files_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:16682,deployability,Fail,Failed,16682,"ner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:17533,deployability,modul,module,17533,"files_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:18926,deployability,Fail,Failed,18926,"4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1.asm.hic.hap2.p_ctg.fa --reads /input/G1_hap2.bam --examples /tmp/tmp6zw47x4_/make_examples.tfrecord@4.gz --task 3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:18997,deployability,fail,failed,18997,"4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1.asm.hic.hap2.p_ctg.fa --reads /input/G1_hap2.bam --examples /tmp/tmp6zw47x4_/make_examples.tfrecord@4.gz --task 3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:492,integrability,version,version,492,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:610,integrability,Messag,Message,610,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:578,interoperability,specif,specified,578,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:610,interoperability,Messag,Message,610,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:492,modifiability,version,version,492,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:716,modifiability,interm,intermediate,716,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:8769,modifiability,deco,decode,8769,"', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.965431 139750224725824 make_examples_core.py:236] Task 3/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.965795: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.979464 140243238704960 make_examples_core.py:236] Task 2/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is defau",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:9123,modifiability,deco,decode,9123,"00047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.965431 139750224725824 make_examples_core.py:236] Task 3/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.965795: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.979464 140243238704960 make_examples_core.py:236] Task 2/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomic",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:9477,modifiability,deco,decode,9477,"'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.965431 139750224725824 make_examples_core.py:236] Task 3/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.965795: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.979464 140243238704960 make_examples_core.py:236] Task 2/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tm",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:9831,modifiability,deco,decode,9831,"-04 14:34:02.965795: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.979464 140243238704960 make_examples_core.py:236] Task 2/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tm",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10801,modifiability,modul,module,10801,"M input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:11828,modifiability,exten,extend,11828,"/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:13045,modifiability,modul,module,13045,"files_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:14072,modifiability,exten,extend,14072,"/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:15289,modifiability,modul,module,15289,"files_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:16316,modifiability,exten,extend,16316,"/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:17533,modifiability,modul,module,17533,"files_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:18560,modifiability,exten,extend,18560,"4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1.asm.hic.hap2.p_ctg.fa --reads /input/G1_hap2.bam --examples /tmp/tmp6zw47x4_/make_examples.tfrecord@4.gz --task 3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:604,performance,Error,Error,604,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10611,performance,Overhead,Overhead,10611,"HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12855,performance,Overhead,Overhead,12855,"). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:15099,performance,Overhead,Overhead,15099,"). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:17343,performance,Overhead,Overhead,17343,"). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:18978,performance,parallel,parallel,18978,"4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1.asm.hic.hap2.p_ctg.fa --reads /input/G1_hap2.bam --examples /tmp/tmp6zw47x4_/make_examples.tfrecord@4.gz --task 3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12194,reliability,Fail,Failed,12194,"ner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:14438,reliability,Fail,Failed,14438,"ner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:16682,reliability,Fail,Failed,16682,"ner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:18926,reliability,Fail,Failed,18926,"4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1.asm.hic.hap2.p_ctg.fa --reads /input/G1_hap2.bam --examples /tmp/tmp6zw47x4_/make_examples.tfrecord@4.gz --task 3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:18997,reliability,fail,failed,18997,"4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1.asm.hic.hap2.p_ctg.fa --reads /input/G1_hap2.bam --examples /tmp/tmp6zw47x4_/make_examples.tfrecord@4.gz --task 3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:185,safety,input,input,185,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:358,safety,input,input,358,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:401,safety,input,input,401,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:604,safety,Error,Error,604,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:817,safety,input,input,817,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:908,safety,input,input,908,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1051,safety,input,input,1051,"I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Coul",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1240,safety,input,input,1240,"ker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 1396",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1331,safety,input,input,1331,odel_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. D,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1474,safety,input,input,1474,variant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 13970,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1663,safety,input,input,1663,un_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomi,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1754,safety,input,input,1754, [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 13,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1897,safety,input,input,1897,"eading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2086,safety,input,input,2086,"ult as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2177,safety,input,input,2177," [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg00",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2320,safety,input,input,2320,"eading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2535,safety,input,inputs,2535,"u can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2602,safety,input,input,2602,"E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2693,safety,input,input,2693,"2.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4069,safety,input,inputs,4069,"g000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.568957 139750224725824 make_examples_core.py:236] Task 3/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571760 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.611608 139750224725824 make_examples_core.py:236] Task 3/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4136,safety,input,input,4136,"l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.568957 139750224725824 make_examples_core.py:236] Task 3/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571760 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.611608 139750224725824 make_examples_core.py:236] Task 3/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4227,safety,input,input,4227,"', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.568957 139750224725824 make_examples_core.py:236] Task 3/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571760 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.611608 139750224725824 make_examples_core.py:236] Task 3/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:5603,safety,input,inputs,5603,"g000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.569077 140243238704960 make_examples_core.py:236] Task 2/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571856 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.613078 140243238704960 make_examples_core.py:236] Task 2/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:5670,safety,input,input,5670,"l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.569077 140243238704960 make_examples_core.py:236] Task 2/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571856 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.613078 140243238704960 make_examples_core.py:236] Task 2/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:5761,safety,input,input,5761,"', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.569077 140243238704960 make_examples_core.py:236] Task 2/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571856 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.613078 140243238704960 make_examples_core.py:236] Task 2/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7137,safety,input,inputs,7137,"g000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.568975 139685650237248 make_examples_core.py:236] Task 0/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571821 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.613319 139685650237248 make_examples_core.py:236] Task 0/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7204,safety,input,input,7204,"l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.568975 139685650237248 make_examples_core.py:236] Task 0/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571821 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.613319 139685650237248 make_examples_core.py:236] Task 0/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7295,safety,input,input,7295,"', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.568975 139685650237248 make_examples_core.py:236] Task 0/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571821 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.613319 139685650237248 make_examples_core.py:236] Task 0/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:8744,safety,input,input,8744,"2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.965431 139750224725824 make_examples_core.py:236] Task 3/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.965795: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.979464 140243238704960 make_examples_core.py:236] Task 2/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:9098,safety,input,input,9098,"l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.965431 139750224725824 make_examples_core.py:236] Task 3/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.965795: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.979464 140243238704960 make_examples_core.py:236] Task 2/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.10025",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:9452,safety,input,input,9452,"000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.965431 139750224725824 make_examples_core.py:236] Task 3/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.965795: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.979464 140243238704960 make_examples_core.py:236] Task 2/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:9806,safety,input,input,9806,"ed in with --ref. 2021-11-04 14:34:02.965795: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.979464 140243238704960 make_examples_core.py:236] Task 2/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10061,safety,input,input,10061,"lt to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10152,safety,input,input,10152,"you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10251,safety,input,input,10251,"etting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10342,safety,input,input,10342,"_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10634,safety,input,inputs,10634,"134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_exam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10801,safety,modul,module,10801,"M input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12305,safety,input,input,12305,".py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12396,safety,input,input,12396,"e ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12495,safety,input,input,12495," line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12586,safety,input,input,12586,"les/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12878,safety,input,inputs,12878,"nfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_exam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:13045,safety,modul,module,13045,"files_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:14549,safety,input,input,14549,".py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:14640,safety,input,input,14640,"e ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:14739,safety,input,input,14739," line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:14830,safety,input,input,14830,"les/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:15122,safety,input,inputs,15122,"nfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_exam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:15289,safety,modul,module,15289,"files_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:16793,safety,input,input,16793,".py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:16884,safety,input,input,16884,"e ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:16983,safety,input,input,16983," line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:17074,safety,input,input,17074,"les/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:17366,safety,input,inputs,17366,"nfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_exam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:17533,safety,modul,module,17533,"files_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:19063,safety,input,input,19063,"4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1.asm.hic.hap2.p_ctg.fa --reads /input/G1_hap2.bam --examples /tmp/tmp6zw47x4_/make_examples.tfrecord@4.gz --task 3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:19103,safety,input,input,19103,"4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1.asm.hic.hap2.p_ctg.fa --reads /input/G1_hap2.bam --examples /tmp/tmp6zw47x4_/make_examples.tfrecord@4.gz --task 3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10653,testability,Trace,Traceback,10653,"4:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12897,testability,Trace,Traceback,12897,"les/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:15141,testability,Trace,Traceback,15141,"les/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:17385,testability,Trace,Traceback,17385,"les/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:80,usability,command,commandline,80,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:185,usability,input,input,185,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:358,usability,input,input,358,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:401,usability,input,input,401,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:604,usability,Error,Error,604,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:817,usability,input,input,817,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:908,usability,input,input,908,"[E::idx_find_and_load] Could not retrieve index file; I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1051,usability,input,input,1051,"I have used the following commandline for running deepvariant. sudo docker run \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Coul",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1240,usability,input,input,1240,"ker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 1396",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1331,usability,input,input,1331,odel_type=WGS \. --ref=/input/G1.asm.hic.hap2.p_ctg.fa \. --reads=/input/G1_hap2.bam \. --output_vcf=/output/output.vcf \. --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. D,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1474,usability,input,input,1474,variant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******. I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 13970,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1663,usability,input,input,1663,un_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomi,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1754,usability,input,input,1754, [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 13,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:1897,usability,input,input,1897,"eading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2086,usability,input,input,2086,"ult as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2177,usability,input,input,2177," [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg00",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2320,usability,input,input,2320,"eading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2535,usability,input,inputs,2535,"u can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2602,usability,input,input,2602,"E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:2693,usability,input,input,2693,"2.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.363935 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. W1104 14:34:02.364088 139685650237248 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I1104 14:34:02.587771 139708777084736 make_examples_core.py:236] Task 1/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.590152 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.630764 139708777084736 make_examples_core.py:236] Task 1/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4069,usability,input,inputs,4069,"g000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.568957 139750224725824 make_examples_core.py:236] Task 3/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571760 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.611608 139750224725824 make_examples_core.py:236] Task 3/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4136,usability,input,input,4136,"l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.568957 139750224725824 make_examples_core.py:236] Task 3/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571760 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.611608 139750224725824 make_examples_core.py:236] Task 3/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:4227,usability,input,input,4227,"', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.568957 139750224725824 make_examples_core.py:236] Task 3/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571760 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.611608 139750224725824 make_examples_core.py:236] Task 3/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:5603,usability,input,inputs,5603,"g000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.569077 140243238704960 make_examples_core.py:236] Task 2/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571856 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.613078 140243238704960 make_examples_core.py:236] Task 2/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:5670,usability,input,input,5670,"l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.569077 140243238704960 make_examples_core.py:236] Task 2/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571856 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.613078 140243238704960 make_examples_core.py:236] Task 2/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:5761,usability,input,input,5761,"', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.569077 140243238704960 make_examples_core.py:236] Task 2/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571856 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.613078 140243238704960 make_examples_core.py:236] Task 2/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7137,usability,input,inputs,7137,"g000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.568975 139685650237248 make_examples_core.py:236] Task 0/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571821 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.613319 139685650237248 make_examples_core.py:236] Task 0/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7204,usability,input,input,7204,"l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.568975 139685650237248 make_examples_core.py:236] Task 0/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571821 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.613319 139685650237248 make_examples_core.py:236] Task 0/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:7295,usability,input,input,7295,"', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.568975 139685650237248 make_examples_core.py:236] Task 0/4: Preparing inputs. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:02.571821 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:02.613319 139685650237248 make_examples_core.py:236] Task 0/4: Common contigs are ['h2tg000001l', 'h2tg000002l', 'h2tg000003l', 'h2tg000004l', 'h2tg000005l', 'h2tg000006l', 'h2tg000007l', 'h2tg000008l', 'h2tg000009l', 'h2tg000010l', 'h2tg000011l', 'h2tg000012l', 'h2tg000013l', 'h2tg000014l', 'h2tg000015l', 'h2tg000016l', 'h2tg000017l', 'h2tg000018l', 'h2tg000019l', 'h2tg000020l', 'h2tg000021l', 'h2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:8744,usability,input,input,8744,"2tg000022l', 'h2tg000023l', 'h2tg000024l', 'h2tg000025l', 'h2tg000026l', 'h2tg000027l', 'h2tg000028l', 'h2tg000029l', 'h2tg000030l', 'h2tg000031l', 'h2tg000032c', 'h2tg000033l', 'h2tg000034l', 'h2tg000035l', 'h2tg000036l', 'h2tg000037l', 'h2tg000038l', 'h2tg000039l', 'h2tg000040l', 'h2tg000041l', 'h2tg000042l', 'h2tg000043l', 'h2tg000044l', 'h2tg000045l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.965431 139750224725824 make_examples_core.py:236] Task 3/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.965795: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.979464 140243238704960 make_examples_core.py:236] Task 2/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:9098,usability,input,input,9098,"l', 'h2tg000046l', 'h2tg000047l', 'h2tg000048l', 'h2tg000049l', 'h2tg000050l', 'h2tg000051l', 'h2tg000052c', 'h2tg000053l', 'h2tg000054l', 'h2tg000055l', 'h2tg000056l', 'h2tg000057l', 'h2tg000058l', 'h2tg000059l', 'h2tg000060l', 'h2tg000061l', 'h2tg000062l', 'h2tg000063l', 'h2tg000064l', 'h2tg000065l', 'h2tg000066l', 'h2tg000067l', 'h2tg000068l', 'h2tg000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.965431 139750224725824 make_examples_core.py:236] Task 3/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.965795: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.979464 140243238704960 make_examples_core.py:236] Task 2/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.10025",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:9452,usability,input,input,9452,"000069l', 'h2tg000070l', 'h2tg000071l', 'h2tg000072l', 'h2tg000073l', 'h2tg000074l', 'h2tg000075l', 'h2tg000076l', 'h2tg000077l']. I1104 14:34:02.965431 139750224725824 make_examples_core.py:236] Task 3/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.965795: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.979464 140243238704960 make_examples_core.py:236] Task 2/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:9806,usability,input,input,9806,"ed in with --ref. 2021-11-04 14:34:02.965795: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.979464 140243238704960 make_examples_core.py:236] Task 2/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10061,usability,input,input,10061,"lt to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10152,usability,input,input,10152,"you passed in with --ref. 2021-11-04 14:34:02.979810: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10251,usability,input,input,10251,"etting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.976352 139685650237248 make_examples_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10342,usability,input,input,10342,"_core.py:236] Task 0/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.976696: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:10634,usability,input,inputs,10634,"134217728. I1104 14:34:02.996676 139708777084736 make_examples_core.py:236] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-11-04 14:34:02.997065: I third_party/nucleus/io/sam_reader.cc:705] Setting HTS_OPT_BLOCK_SIZE to 134217728. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.100254 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.225395 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.225888 139708777084736 make_examples_core.py:236] Task 1/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00001-of-00004.gz. I1104 14:34:03.225993 139708777084736 make_examples_core.py:236] Task 1/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_exam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12305,usability,input,input,12305,".py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12396,usability,input,input,12396,"e ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12495,usability,input,input,12495," line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12586,usability,input,input,12586,"les/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:12878,usability,input,inputs,12878,"nfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_o60bhxse/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.068893 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.193795 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.194288 139750224725824 make_examples_core.py:236] Task 3/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00003-of-00004.gz. I1104 14:34:03.194392 139750224725824 make_examples_core.py:236] Task 3/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_exam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:14549,usability,input,input,14549,".py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:14640,usability,input,input,14640,"e ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:14739,usability,input,input,14739," line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:14830,usability,input,input,14830,"les/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:15122,usability,input,inputs,15122,"nfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_j563zta4/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079088 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.208736 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.209183 140243238704960 make_examples_core.py:236] Task 2/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00002-of-00004.gz. I1104 14:34:03.209277 140243238704960 make_examples_core.py:236] Task 2/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_exam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:16793,usability,input,input,16793,".py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:16884,usability,input,input,16884,"e ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:16983,usability,input,input,16983," line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:17074,usability,input,input,17074,"les/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:17366,usability,input,inputs,17366,"nfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_fkyy0r3x/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.079069 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'. I1104 14:34:03.205049 139685650237248 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader. I1104 14:34:03.205483 139685650237248 make_examples_core.py:236] Task 0/4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_exam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:19063,usability,input,input,19063,"4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1.asm.hic.hap2.p_ctg.fa --reads /input/G1_hap2.bam --examples /tmp/tmp6zw47x4_/make_examples.tfrecord@4.gz --task 3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/492:19103,usability,input,input,19103,"4: Writing examples to /tmp/tmp6zw47x4_/make_examples.tfrecord-00000-of-00004.gz. I1104 14:34:03.205575 139685650237248 make_examples_core.py:236] Task 0/4: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 163, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1569, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 918, in process. reads = self.region_reads(. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1024, in region_reads. raise err. File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1002, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_z12b2b_u/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 245, in query. return self._reader.query(region). ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/G1.asm.hic.hap2.p_ctg.fa --reads /input/G1_hap2.bam --examples /tmp/tmp6zw47x4_/make_examples.tfrecord@4.gz --task 3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492
https://github.com/google/deepvariant/issues/493:70,energy efficiency,frequenc,frequency,70,"Deepvariant output html file ; output html file of deepvariant allele frequency plot is bizarre-looking, I don't know how to read that or either I made a mistake while running deep variant . Main genotypes plot Het (0/X) have one big peak and another one has. [output.visual_report.pdf](https://github.com/google/deepvariant/files/7499896/output.visual_report.pdf). a very small peak. I am having a hard time in understanding that . Commands i used to run on my fungal dataset. ~/minimap2-2.22_x64-linux/minimap2 --MD -ax map-hifi G1_cleaned.fasta G1_HiFi_reads.fastq -t 20 > G1_align.sam. samtools view -bS -@ 20 G1_align.sam | samtools sort -@ 20 -o G1_sorted.bam. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1_cleaned.fasta \. --reads=/input/G1_HiFi_reads.fastq \. --output_vcf=/output/output.vcf \. --num_shards=4. Output file is attached.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:404,performance,time,time,404,"Deepvariant output html file ; output html file of deepvariant allele frequency plot is bizarre-looking, I don't know how to read that or either I made a mistake while running deep variant . Main genotypes plot Het (0/X) have one big peak and another one has. [output.visual_report.pdf](https://github.com/google/deepvariant/files/7499896/output.visual_report.pdf). a very small peak. I am having a hard time in understanding that . Commands i used to run on my fungal dataset. ~/minimap2-2.22_x64-linux/minimap2 --MD -ax map-hifi G1_cleaned.fasta G1_HiFi_reads.fastq -t 20 > G1_align.sam. samtools view -bS -@ 20 G1_align.sam | samtools sort -@ 20 -o G1_sorted.bam. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1_cleaned.fasta \. --reads=/input/G1_HiFi_reads.fastq \. --output_vcf=/output/output.vcf \. --num_shards=4. Output file is attached.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:716,safety,input,input,716,"Deepvariant output html file ; output html file of deepvariant allele frequency plot is bizarre-looking, I don't know how to read that or either I made a mistake while running deep variant . Main genotypes plot Het (0/X) have one big peak and another one has. [output.visual_report.pdf](https://github.com/google/deepvariant/files/7499896/output.visual_report.pdf). a very small peak. I am having a hard time in understanding that . Commands i used to run on my fungal dataset. ~/minimap2-2.22_x64-linux/minimap2 --MD -ax map-hifi G1_cleaned.fasta G1_HiFi_reads.fastq -t 20 > G1_align.sam. samtools view -bS -@ 20 G1_align.sam | samtools sort -@ 20 -o G1_sorted.bam. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1_cleaned.fasta \. --reads=/input/G1_HiFi_reads.fastq \. --output_vcf=/output/output.vcf \. --num_shards=4. Output file is attached.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:889,safety,input,input,889,"Deepvariant output html file ; output html file of deepvariant allele frequency plot is bizarre-looking, I don't know how to read that or either I made a mistake while running deep variant . Main genotypes plot Het (0/X) have one big peak and another one has. [output.visual_report.pdf](https://github.com/google/deepvariant/files/7499896/output.visual_report.pdf). a very small peak. I am having a hard time in understanding that . Commands i used to run on my fungal dataset. ~/minimap2-2.22_x64-linux/minimap2 --MD -ax map-hifi G1_cleaned.fasta G1_HiFi_reads.fastq -t 20 > G1_align.sam. samtools view -bS -@ 20 G1_align.sam | samtools sort -@ 20 -o G1_sorted.bam. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1_cleaned.fasta \. --reads=/input/G1_HiFi_reads.fastq \. --output_vcf=/output/output.vcf \. --num_shards=4. Output file is attached.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:924,safety,input,input,924,"Deepvariant output html file ; output html file of deepvariant allele frequency plot is bizarre-looking, I don't know how to read that or either I made a mistake while running deep variant . Main genotypes plot Het (0/X) have one big peak and another one has. [output.visual_report.pdf](https://github.com/google/deepvariant/files/7499896/output.visual_report.pdf). a very small peak. I am having a hard time in understanding that . Commands i used to run on my fungal dataset. ~/minimap2-2.22_x64-linux/minimap2 --MD -ax map-hifi G1_cleaned.fasta G1_HiFi_reads.fastq -t 20 > G1_align.sam. samtools view -bS -@ 20 G1_align.sam | samtools sort -@ 20 -o G1_sorted.bam. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1_cleaned.fasta \. --reads=/input/G1_HiFi_reads.fastq \. --output_vcf=/output/output.vcf \. --num_shards=4. Output file is attached.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:412,testability,understand,understanding,412,"Deepvariant output html file ; output html file of deepvariant allele frequency plot is bizarre-looking, I don't know how to read that or either I made a mistake while running deep variant . Main genotypes plot Het (0/X) have one big peak and another one has. [output.visual_report.pdf](https://github.com/google/deepvariant/files/7499896/output.visual_report.pdf). a very small peak. I am having a hard time in understanding that . Commands i used to run on my fungal dataset. ~/minimap2-2.22_x64-linux/minimap2 --MD -ax map-hifi G1_cleaned.fasta G1_HiFi_reads.fastq -t 20 > G1_align.sam. samtools view -bS -@ 20 G1_align.sam | samtools sort -@ 20 -o G1_sorted.bam. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1_cleaned.fasta \. --reads=/input/G1_HiFi_reads.fastq \. --output_vcf=/output/output.vcf \. --num_shards=4. Output file is attached.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:433,usability,Command,Commands,433,"Deepvariant output html file ; output html file of deepvariant allele frequency plot is bizarre-looking, I don't know how to read that or either I made a mistake while running deep variant . Main genotypes plot Het (0/X) have one big peak and another one has. [output.visual_report.pdf](https://github.com/google/deepvariant/files/7499896/output.visual_report.pdf). a very small peak. I am having a hard time in understanding that . Commands i used to run on my fungal dataset. ~/minimap2-2.22_x64-linux/minimap2 --MD -ax map-hifi G1_cleaned.fasta G1_HiFi_reads.fastq -t 20 > G1_align.sam. samtools view -bS -@ 20 G1_align.sam | samtools sort -@ 20 -o G1_sorted.bam. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1_cleaned.fasta \. --reads=/input/G1_HiFi_reads.fastq \. --output_vcf=/output/output.vcf \. --num_shards=4. Output file is attached.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:716,usability,input,input,716,"Deepvariant output html file ; output html file of deepvariant allele frequency plot is bizarre-looking, I don't know how to read that or either I made a mistake while running deep variant . Main genotypes plot Het (0/X) have one big peak and another one has. [output.visual_report.pdf](https://github.com/google/deepvariant/files/7499896/output.visual_report.pdf). a very small peak. I am having a hard time in understanding that . Commands i used to run on my fungal dataset. ~/minimap2-2.22_x64-linux/minimap2 --MD -ax map-hifi G1_cleaned.fasta G1_HiFi_reads.fastq -t 20 > G1_align.sam. samtools view -bS -@ 20 G1_align.sam | samtools sort -@ 20 -o G1_sorted.bam. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1_cleaned.fasta \. --reads=/input/G1_HiFi_reads.fastq \. --output_vcf=/output/output.vcf \. --num_shards=4. Output file is attached.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:889,usability,input,input,889,"Deepvariant output html file ; output html file of deepvariant allele frequency plot is bizarre-looking, I don't know how to read that or either I made a mistake while running deep variant . Main genotypes plot Het (0/X) have one big peak and another one has. [output.visual_report.pdf](https://github.com/google/deepvariant/files/7499896/output.visual_report.pdf). a very small peak. I am having a hard time in understanding that . Commands i used to run on my fungal dataset. ~/minimap2-2.22_x64-linux/minimap2 --MD -ax map-hifi G1_cleaned.fasta G1_HiFi_reads.fastq -t 20 > G1_align.sam. samtools view -bS -@ 20 G1_align.sam | samtools sort -@ 20 -o G1_sorted.bam. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1_cleaned.fasta \. --reads=/input/G1_HiFi_reads.fastq \. --output_vcf=/output/output.vcf \. --num_shards=4. Output file is attached.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/493:924,usability,input,input,924,"Deepvariant output html file ; output html file of deepvariant allele frequency plot is bizarre-looking, I don't know how to read that or either I made a mistake while running deep variant . Main genotypes plot Het (0/X) have one big peak and another one has. [output.visual_report.pdf](https://github.com/google/deepvariant/files/7499896/output.visual_report.pdf). a very small peak. I am having a hard time in understanding that . Commands i used to run on my fungal dataset. ~/minimap2-2.22_x64-linux/minimap2 --MD -ax map-hifi G1_cleaned.fasta G1_HiFi_reads.fastq -t 20 > G1_align.sam. samtools view -bS -@ 20 G1_align.sam | samtools sort -@ 20 -o G1_sorted.bam. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \. -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \. google/deepvariant:1.2.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/G1_cleaned.fasta \. --reads=/input/G1_HiFi_reads.fastq \. --output_vcf=/output/output.vcf \. --num_shards=4. Output file is attached.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/493
https://github.com/google/deepvariant/issues/494:107,usability,support,supporting,107,"confused for SNP INDEL result; Confused while checking SNP VCF file from deepvariant. the number of reads supporting ALT is more than REF, but the genotype is 0/0. WHY? . ![image](https://user-images.githubusercontent.com/32149026/143169794-d398c8de-0dff-496f-91f3-ff088d309814.png). even, the number of reads supporting ALT is 9 and REF is 0, but the genotype is 0/0. ![image](https://user-images.githubusercontent.com/32149026/143170163-228821a1-509c-48ec-8528-57c1b8af5e5b.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:189,usability,user,user-images,189,"confused for SNP INDEL result; Confused while checking SNP VCF file from deepvariant. the number of reads supporting ALT is more than REF, but the genotype is 0/0. WHY? . ![image](https://user-images.githubusercontent.com/32149026/143169794-d398c8de-0dff-496f-91f3-ff088d309814.png). even, the number of reads supporting ALT is 9 and REF is 0, but the genotype is 0/0. ![image](https://user-images.githubusercontent.com/32149026/143170163-228821a1-509c-48ec-8528-57c1b8af5e5b.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:311,usability,support,supporting,311,"confused for SNP INDEL result; Confused while checking SNP VCF file from deepvariant. the number of reads supporting ALT is more than REF, but the genotype is 0/0. WHY? . ![image](https://user-images.githubusercontent.com/32149026/143169794-d398c8de-0dff-496f-91f3-ff088d309814.png). even, the number of reads supporting ALT is 9 and REF is 0, but the genotype is 0/0. ![image](https://user-images.githubusercontent.com/32149026/143170163-228821a1-509c-48ec-8528-57c1b8af5e5b.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/494:387,usability,user,user-images,387,"confused for SNP INDEL result; Confused while checking SNP VCF file from deepvariant. the number of reads supporting ALT is more than REF, but the genotype is 0/0. WHY? . ![image](https://user-images.githubusercontent.com/32149026/143169794-d398c8de-0dff-496f-91f3-ff088d309814.png). even, the number of reads supporting ALT is 9 and REF is 0, but the genotype is 0/0. ![image](https://user-images.githubusercontent.com/32149026/143170163-228821a1-509c-48ec-8528-57c1b8af5e5b.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494
https://github.com/google/deepvariant/issues/495:108,energy efficiency,model,model,108,"Support Request: retraining final layer; I am looking to retrain just the final classification layer of the model (DeepVariant WGS) and add an extra class, so the model outputs 4 probabilities instead of the usual 3. . I have been looking at these instructions: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md but they don't quite correspond to my use case scenario. Would someone be able to point me towards the scripts/functions of relevance please? Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:163,energy efficiency,model,model,163,"Support Request: retraining final layer; I am looking to retrain just the final classification layer of the model (DeepVariant WGS) and add an extra class, so the model outputs 4 probabilities instead of the usual 3. . I have been looking at these instructions: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md but they don't quite correspond to my use case scenario. Would someone be able to point me towards the scripts/functions of relevance please? Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:34,modifiability,layer,layer,34,"Support Request: retraining final layer; I am looking to retrain just the final classification layer of the model (DeepVariant WGS) and add an extra class, so the model outputs 4 probabilities instead of the usual 3. . I have been looking at these instructions: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md but they don't quite correspond to my use case scenario. Would someone be able to point me towards the scripts/functions of relevance please? Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:95,modifiability,layer,layer,95,"Support Request: retraining final layer; I am looking to retrain just the final classification layer of the model (DeepVariant WGS) and add an extra class, so the model outputs 4 probabilities instead of the usual 3. . I have been looking at these instructions: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md but they don't quite correspond to my use case scenario. Would someone be able to point me towards the scripts/functions of relevance please? Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:398,modifiability,scenario,scenario,398,"Support Request: retraining final layer; I am looking to retrain just the final classification layer of the model (DeepVariant WGS) and add an extra class, so the model outputs 4 probabilities instead of the usual 3. . I have been looking at these instructions: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md but they don't quite correspond to my use case scenario. Would someone be able to point me towards the scripts/functions of relevance please? Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:108,security,model,model,108,"Support Request: retraining final layer; I am looking to retrain just the final classification layer of the model (DeepVariant WGS) and add an extra class, so the model outputs 4 probabilities instead of the usual 3. . I have been looking at these instructions: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md but they don't quite correspond to my use case scenario. Would someone be able to point me towards the scripts/functions of relevance please? Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:163,security,model,model,163,"Support Request: retraining final layer; I am looking to retrain just the final classification layer of the model (DeepVariant WGS) and add an extra class, so the model outputs 4 probabilities instead of the usual 3. . I have been looking at these instructions: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md but they don't quite correspond to my use case scenario. Would someone be able to point me towards the scripts/functions of relevance please? Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/495:0,usability,Support,Support,0,"Support Request: retraining final layer; I am looking to retrain just the final classification layer of the model (DeepVariant WGS) and add an extra class, so the model outputs 4 probabilities instead of the usual 3. . I have been looking at these instructions: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md but they don't quite correspond to my use case scenario. Would someone be able to point me towards the scripts/functions of relevance please? Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495
https://github.com/google/deepvariant/issues/496:621,availability,Operat,Operating,621,"how to find call_variants args; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**. Hello,. I would like to run deepvariant on a very depth datasets (up to 10K reads at the same locus). When I am looking to the VCF, I can see depth per allele at max 1.5K. Is it possible to modify the call_variant parameter to allow more reads per bp position ? I am also wondering how I can see the full list of parameters for call_variant (the allowed flags to put in ""--call_variants_extra_args"" like min_fraction_snps, min_fraction_indels, ...). Thanks . **Setup**. - Operating system:CentOS. - DeepVariant version:1.2. - Installation method (Docker, built from source, etc.):docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)PacBio.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:660,deployability,version,version,660,"how to find call_variants args; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**. Hello,. I would like to run deepvariant on a very depth datasets (up to 10K reads at the same locus). When I am looking to the VCF, I can see depth per allele at max 1.5K. Is it possible to modify the call_variant parameter to allow more reads per bp position ? I am also wondering how I can see the full list of parameters for call_variant (the allowed flags to put in ""--call_variants_extra_args"" like min_fraction_snps, min_fraction_indels, ...). Thanks . **Setup**. - Operating system:CentOS. - DeepVariant version:1.2. - Installation method (Docker, built from source, etc.):docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)PacBio.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:675,deployability,Instal,Installation,675,"how to find call_variants args; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**. Hello,. I would like to run deepvariant on a very depth datasets (up to 10K reads at the same locus). When I am looking to the VCF, I can see depth per allele at max 1.5K. Is it possible to modify the call_variant parameter to allow more reads per bp position ? I am also wondering how I can see the full list of parameters for call_variant (the allowed flags to put in ""--call_variants_extra_args"" like min_fraction_snps, min_fraction_indels, ...). Thanks . **Setup**. - Operating system:CentOS. - DeepVariant version:1.2. - Installation method (Docker, built from source, etc.):docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)PacBio.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:660,integrability,version,version,660,"how to find call_variants args; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**. Hello,. I would like to run deepvariant on a very depth datasets (up to 10K reads at the same locus). When I am looking to the VCF, I can see depth per allele at max 1.5K. Is it possible to modify the call_variant parameter to allow more reads per bp position ? I am also wondering how I can see the full list of parameters for call_variant (the allowed flags to put in ""--call_variants_extra_args"" like min_fraction_snps, min_fraction_indels, ...). Thanks . **Setup**. - Operating system:CentOS. - DeepVariant version:1.2. - Installation method (Docker, built from source, etc.):docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)PacBio.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:363,modifiability,paramet,parameter,363,"how to find call_variants args; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**. Hello,. I would like to run deepvariant on a very depth datasets (up to 10K reads at the same locus). When I am looking to the VCF, I can see depth per allele at max 1.5K. Is it possible to modify the call_variant parameter to allow more reads per bp position ? I am also wondering how I can see the full list of parameters for call_variant (the allowed flags to put in ""--call_variants_extra_args"" like min_fraction_snps, min_fraction_indels, ...). Thanks . **Setup**. - Operating system:CentOS. - DeepVariant version:1.2. - Installation method (Docker, built from source, etc.):docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)PacBio.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:462,modifiability,paramet,parameters,462,"how to find call_variants args; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**. Hello,. I would like to run deepvariant on a very depth datasets (up to 10K reads at the same locus). When I am looking to the VCF, I can see depth per allele at max 1.5K. Is it possible to modify the call_variant parameter to allow more reads per bp position ? I am also wondering how I can see the full list of parameters for call_variant (the allowed flags to put in ""--call_variants_extra_args"" like min_fraction_snps, min_fraction_indels, ...). Thanks . **Setup**. - Operating system:CentOS. - DeepVariant version:1.2. - Installation method (Docker, built from source, etc.):docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)PacBio.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:660,modifiability,version,version,660,"how to find call_variants args; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**. Hello,. I would like to run deepvariant on a very depth datasets (up to 10K reads at the same locus). When I am looking to the VCF, I can see depth per allele at max 1.5K. Is it possible to modify the call_variant parameter to allow more reads per bp position ? I am also wondering how I can see the full list of parameters for call_variant (the allowed flags to put in ""--call_variants_extra_args"" like min_fraction_snps, min_fraction_indels, ...). Thanks . **Setup**. - Operating system:CentOS. - DeepVariant version:1.2. - Installation method (Docker, built from source, etc.):docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)PacBio.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:845,modifiability,Pac,PacBio,845,"how to find call_variants args; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**. Hello,. I would like to run deepvariant on a very depth datasets (up to 10K reads at the same locus). When I am looking to the VCF, I can see depth per allele at max 1.5K. Is it possible to modify the call_variant parameter to allow more reads per bp position ? I am also wondering how I can see the full list of parameters for call_variant (the allowed flags to put in ""--call_variants_extra_args"" like min_fraction_snps, min_fraction_indels, ...). Thanks . **Setup**. - Operating system:CentOS. - DeepVariant version:1.2. - Installation method (Docker, built from source, etc.):docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)PacBio.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:339,security,modif,modify,339,"how to find call_variants args; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**. Hello,. I would like to run deepvariant on a very depth datasets (up to 10K reads at the same locus). When I am looking to the VCF, I can see depth per allele at max 1.5K. Is it possible to modify the call_variant parameter to allow more reads per bp position ? I am also wondering how I can see the full list of parameters for call_variant (the allowed flags to put in ""--call_variants_extra_args"" like min_fraction_snps, min_fraction_indels, ...). Thanks . **Setup**. - Operating system:CentOS. - DeepVariant version:1.2. - Installation method (Docker, built from source, etc.):docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)PacBio.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/496:765,testability,instrument,instrument,765,"how to find call_variants args; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**. Hello,. I would like to run deepvariant on a very depth datasets (up to 10K reads at the same locus). When I am looking to the VCF, I can see depth per allele at max 1.5K. Is it possible to modify the call_variant parameter to allow more reads per bp position ? I am also wondering how I can see the full list of parameters for call_variant (the allowed flags to put in ""--call_variants_extra_args"" like min_fraction_snps, min_fraction_indels, ...). Thanks . **Setup**. - Operating system:CentOS. - DeepVariant version:1.2. - Installation method (Docker, built from source, etc.):docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)PacBio.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496
https://github.com/google/deepvariant/issues/497:0,deployability,fail,failed,0,"failed after PEPPER step; Hello,. Thanks very much this amazing software. I have been trying to use Pepper-deepvariant on HPC (under singularity container) with the example dataset you have provided i.e. HG002_guppy_507_2_GRCh38_pass.chr20.30x.bam . However, it failed after PEPPER step ""BASIC CANDIDATE FINDER SELECT"" and doesn't seem to produce respective vcf as expected. Images generation are completed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:145,deployability,contain,container,145,"failed after PEPPER step; Hello,. Thanks very much this amazing software. I have been trying to use Pepper-deepvariant on HPC (under singularity container) with the example dataset you have provided i.e. HG002_guppy_507_2_GRCh38_pass.chr20.30x.bam . However, it failed after PEPPER step ""BASIC CANDIDATE FINDER SELECT"" and doesn't seem to produce respective vcf as expected. Images generation are completed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:262,deployability,fail,failed,262,"failed after PEPPER step; Hello,. Thanks very much this amazing software. I have been trying to use Pepper-deepvariant on HPC (under singularity container) with the example dataset you have provided i.e. HG002_guppy_507_2_GRCh38_pass.chr20.30x.bam . However, it failed after PEPPER step ""BASIC CANDIDATE FINDER SELECT"" and doesn't seem to produce respective vcf as expected. Images generation are completed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:0,reliability,fail,failed,0,"failed after PEPPER step; Hello,. Thanks very much this amazing software. I have been trying to use Pepper-deepvariant on HPC (under singularity container) with the example dataset you have provided i.e. HG002_guppy_507_2_GRCh38_pass.chr20.30x.bam . However, it failed after PEPPER step ""BASIC CANDIDATE FINDER SELECT"" and doesn't seem to produce respective vcf as expected. Images generation are completed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:262,reliability,fail,failed,262,"failed after PEPPER step; Hello,. Thanks very much this amazing software. I have been trying to use Pepper-deepvariant on HPC (under singularity container) with the example dataset you have provided i.e. HG002_guppy_507_2_GRCh38_pass.chr20.30x.bam . However, it failed after PEPPER step ""BASIC CANDIDATE FINDER SELECT"" and doesn't seem to produce respective vcf as expected. Images generation are completed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:323,reliability,doe,doesn,323,"failed after PEPPER step; Hello,. Thanks very much this amazing software. I have been trying to use Pepper-deepvariant on HPC (under singularity container) with the example dataset you have provided i.e. HG002_guppy_507_2_GRCh38_pass.chr20.30x.bam . However, it failed after PEPPER step ""BASIC CANDIDATE FINDER SELECT"" and doesn't seem to produce respective vcf as expected. Images generation are completed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:397,safety,compl,completed,397,"failed after PEPPER step; Hello,. Thanks very much this amazing software. I have been trying to use Pepper-deepvariant on HPC (under singularity container) with the example dataset you have provided i.e. HG002_guppy_507_2_GRCh38_pass.chr20.30x.bam . However, it failed after PEPPER step ""BASIC CANDIDATE FINDER SELECT"" and doesn't seem to produce respective vcf as expected. Images generation are completed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/497:397,security,compl,completed,397,"failed after PEPPER step; Hello,. Thanks very much this amazing software. I have been trying to use Pepper-deepvariant on HPC (under singularity container) with the example dataset you have provided i.e. HG002_guppy_507_2_GRCh38_pass.chr20.30x.bam . However, it failed after PEPPER step ""BASIC CANDIDATE FINDER SELECT"" and doesn't seem to produce respective vcf as expected. Images generation are completed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497
https://github.com/google/deepvariant/issues/498:23,safety,Compl,Complexity,23,Variant Calling in Low-Complexity Regions; there are some variants where located in Low-Complexity Regions can not be identified by using DeepVariant Illmina and hifi mode . how to solve it  thanks ~. ![image](https://user-images.githubusercontent.com/54714639/145783707-493a6c3d-c1ca-4f4e-9981-1b7631e165cb.png).,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498
https://github.com/google/deepvariant/issues/498:88,safety,Compl,Complexity,88,Variant Calling in Low-Complexity Regions; there are some variants where located in Low-Complexity Regions can not be identified by using DeepVariant Illmina and hifi mode . how to solve it  thanks ~. ![image](https://user-images.githubusercontent.com/54714639/145783707-493a6c3d-c1ca-4f4e-9981-1b7631e165cb.png).,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498
https://github.com/google/deepvariant/issues/498:23,security,Compl,Complexity,23,Variant Calling in Low-Complexity Regions; there are some variants where located in Low-Complexity Regions can not be identified by using DeepVariant Illmina and hifi mode . how to solve it  thanks ~. ![image](https://user-images.githubusercontent.com/54714639/145783707-493a6c3d-c1ca-4f4e-9981-1b7631e165cb.png).,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498
https://github.com/google/deepvariant/issues/498:88,security,Compl,Complexity,88,Variant Calling in Low-Complexity Regions; there are some variants where located in Low-Complexity Regions can not be identified by using DeepVariant Illmina and hifi mode . how to solve it  thanks ~. ![image](https://user-images.githubusercontent.com/54714639/145783707-493a6c3d-c1ca-4f4e-9981-1b7631e165cb.png).,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498
https://github.com/google/deepvariant/issues/498:118,security,ident,identified,118,Variant Calling in Low-Complexity Regions; there are some variants where located in Low-Complexity Regions can not be identified by using DeepVariant Illmina and hifi mode . how to solve it  thanks ~. ![image](https://user-images.githubusercontent.com/54714639/145783707-493a6c3d-c1ca-4f4e-9981-1b7631e165cb.png).,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498
https://github.com/google/deepvariant/issues/498:219,usability,user,user-images,219,Variant Calling in Low-Complexity Regions; there are some variants where located in Low-Complexity Regions can not be identified by using DeepVariant Illmina and hifi mode . how to solve it  thanks ~. ![image](https://user-images.githubusercontent.com/54714639/145783707-493a6c3d-c1ca-4f4e-9981-1b7631e165cb.png).,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498
https://github.com/google/deepvariant/issues/499:9,availability,error,error,9,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:234,availability,error,error,234,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:349,availability,Operat,Operating,349,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1027,availability,Error,Error,1027,"or('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.461844",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:266,deployability,contain,containers,266,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:281,deployability,version,version,281,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:323,deployability,contain,container,323,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:390,deployability,version,version,390,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:423,deployability,Instal,Installation,423,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:485,deployability,contain,container,485,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1830,deployability,modul,module,1830,".bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:2867,deployability,modul,module,2867,"lurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:3904,deployability,modul,module,3904,"lurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:4782,deployability,fail,failed,4782,"/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:5171,deployability,fail,failed,5171,"/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:240,integrability,messag,message,240,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:281,integrability,version,version,281,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:390,integrability,version,version,390,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:240,interoperability,messag,message,240,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:281,modifiability,version,version,281,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:390,modifiability,version,version,390,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1830,modifiability,modul,module,1830,".bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:2867,modifiability,modul,module,2867,"lurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:3904,modifiability,modul,module,3904,"lurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:9,performance,error,error,9,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:234,performance,error,error,234,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1027,performance,Error,Error,1027,"or('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.461844",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:4763,performance,parallel,parallel,4763,"""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any addi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:5152,performance,parallel,parallel,5152,"/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:4782,reliability,fail,failed,4782,"/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:5171,reliability,fail,failed,5171,"/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:5542,reliability,Doe,Does,5542,"/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:9,safety,error,error,9,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:234,safety,error,error,234,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1027,safety,Error,Error,1027,"or('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.461844",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1830,safety,modul,module,1830,".bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:2867,safety,modul,module,2867,"lurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:3904,safety,modul,module,3904,"lurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:5563,safety,test,test,5563,"/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:5599,safety,test,test,5599,"/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:551,testability,instrument,instrument,551,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1033,testability,trace,trace,1033,"pected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Baze",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1652,testability,Trace,Traceback,1652,"0. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Typ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:2689,testability,Trace,Traceback,2689,"/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Typ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:3726,testability,Trace,Traceback,3726,"/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_vyk_xim_/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Typ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:5563,testability,test,test,5563,"/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:5599,testability,test,test,5599,"/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:5774,testability,context,context,5774,"/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:9,usability,error,error,9,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:234,usability,error,error,234,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:687,usability,Command,Command,687,"Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1027,usability,Error,Error,1027,"or('Expected to be using C++ protobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.461844",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/499:1057,usability,Command,Command,1057,"otobuf implementation '; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**. - Operating system: Centos7. - DeepVariant version: 1.2.0, 1.3.0, latest. - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16. - . - Error trace: (if applicable). Command output:. sys.exit(main(argv)). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main. proto_utils.uses_fast_cpp_protos_or_die(). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die. raise ValueError('Expected to be using C++ protobuf implementation '. ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python. Traceback (most recent call last):. File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499
https://github.com/google/deepvariant/issues/500:293,availability,error,error,293,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1346,availability,error,error,1346,"nd works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1454,availability,checkpoint,checkpoint,1454,".3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4646,availability,checkpoint,checkpoint,4646,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1676,deployability,modul,module,1676,"ng"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). Fil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4087,deployability,api,api,4087,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:171,energy efficiency,model,model,171,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:241,energy efficiency,model,model,241,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:408,energy efficiency,model,models,408,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:482,energy efficiency,model,model,482,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:541,energy efficiency,gpu,gpu,541,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:905,energy efficiency,model,models,905,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:982,energy efficiency,model,model,982,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1041,energy efficiency,gpu,gpu,1041,"3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/ap",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1446,energy efficiency,model,model,1446,"ption_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:2479,energy efficiency,estimat,estimator,2479,"arting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/us",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:2570,energy efficiency,estimat,estimator,2570,"p/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:2580,energy efficiency,estimat,estimator,2580,"nfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in ini",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:2749,energy efficiency,estimat,estimator,2749,"tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:2759,energy efficiency,estimat,estimator,2759,"/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:2944,energy efficiency,estimat,estimator,2944,"99, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _me",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:2954,energy efficiency,estimat,estimator,2954,". _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:3148,energy efficiency,estimat,estimator,3148,"epvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:3158,energy efficiency,estimat,estimator,3158,"deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/loca",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4087,integrability,api,api,4087,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4109,integrability,wrap,wrapper,4109,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1772,interoperability,platform,platform,1772,"t_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:3732,interoperability,distribut,distribute,3732,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:3901,interoperability,distribut,distribute,3901,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4087,interoperability,api,api,4087,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4109,interoperability,wrapper,wrapper,4109,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4683,interoperability,specif,specify,4683,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:159,modifiability,Pac,PacBio,159,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:234,modifiability,Pac,PacBio,234,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1439,modifiability,Pac,PacBio,1439,"nt-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1491,modifiability,variab,variables,1491,"ingularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1676,modifiability,modul,module,1676,"ng"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). Fil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1745,modifiability,pac,packages,1745,"interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:2533,modifiability,pac,packages,2533,"ck (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:2712,modifiability,pac,packages,2712,"usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:2907,modifiability,pac,packages,2907,"/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:3111,modifiability,pac,packages,3111,"nfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrappe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:3310,modifiability,pac,packages,3310,"rain.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_ch",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:3511,modifiability,pac,packages,3511,"ib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:3705,modifiability,pac,packages,3705,"st-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `mode",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:3874,modifiability,pac,packages,3874,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4045,modifiability,pac,packages,4045,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4184,modifiability,pac,packages,4184,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4361,modifiability,pac,packages,4361,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4495,modifiability,variab,variable,4495,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:293,performance,error,error,293,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:541,performance,gpu,gpu,541,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1041,performance,gpu,gpu,1041,"3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/ap",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1346,performance,error,error,1346,"nd works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1396,reliability,doe,does,1396,"epvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4e",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1454,reliability,checkpoint,checkpoint,1454,".3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4556,reliability,doe,doesn,4556,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4646,reliability,checkpoint,checkpoint,4646,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:4674,reliability,doe,does,4674,"es/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call. return merge_fn(self._strategy, *args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper. return func(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>. init_from_checkpoint_fn = lambda _: _init_from_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint. raise ValueError(. ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader. ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:293,safety,error,error,293,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1346,safety,error,error,1346,"nd works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1676,safety,modul,module,1676,"ng"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). Fil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:171,security,model,model,171,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:241,security,model,model,241,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:408,security,model,models,408,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:482,security,model,model,482,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:905,security,model,models,905,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:982,security,model,model,982,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1446,security,model,model,1446,"ption_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:2615,security,loss,loss,2615,"gle_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1530,testability,Trace,Traceback,1530,"1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-pa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:2650,testability,hook,hooks,2650,"ain.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:2845,testability,hook,hooks,2845,"parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_run. return run(. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 191, in run. estimator.train(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train. loss = self._train_model(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model. return self._train_model_default(input_fn, hooks, saving_listeners). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default. return self._train_with_estimator_spec(estimator_spec, worker_hooks,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec. tf.compat.v1.train.warm_start(*self._warm_start_settings). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start. checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint. distribution_strategy_context.get_replica_context().merge_call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call. return self._merge_call(merge_fn, args, kwargs). File ""/usr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:293,usability,error,error,293,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:344,usability,command,command,344,"Training works with DeepVariant-inception_v3-1.3.0+data-wgs_standard but not DeepVariant-inception_v3-1.3.0+data-pacbio_standard; Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/500:1346,usability,error,error,1346,"nd works:. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. But, when I try the following. ```bash. GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \. --train_dir=""training"" \. --model_name=""inception_v3"" \. --number_of_steps=900 \. --save_interval_secs=300 \. --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}"". ```. I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint? ```python. Warm-starting variables only in TRAINABLE_VARIABLES. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main. parse_and_run(). File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 229, in parse_and_ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/500
https://github.com/google/deepvariant/issues/501:348,availability,Operat,Operating,348,"A low number of calls generated; **Describe the issue:**. When calling I get a low number of records in a vcf file (20-40k variants). In comparison with GATK there are at last 10 times less variants generated. Also, it is much less than from 1 thousand genomes deepvariant dataset truncated to the same exome region - 1.7 mln records. **Setup**. - Operating system:. CentoOS 7. - DeepVariant version:. 1.3.0. - Installation method (Docker, built from source, etc.):. podman/singularity. - Type of data:. The data are human's whole exome sequences from Illumina. As a reference I use hg38 with alt contigs. I use truseq v1.2 exome bed file. Besides using bwa-mem2, samtools merge and sort there is not much preprocessing. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=\$PWD/$idxbase \. --reads=\$PWD/${sample_id}.bam \. --regions=\$PWD/$bed_file \. --output_vcf=\$PWD/${sample_id}.vcf.gz \. --output_gvcf=\$PWD/${sample_id}.g.vcf.gz \. --num_shards=${task.cpus}. ```. I also used glnexus. ```. glnexus_cli . --bed /in/truseq-dna-exome-targeted-regions-manifest-v1-2.bed . --config DeepVariantWES /in/vcf_deepvariant/*.g.vcf.gz > merged.bcf. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:392,deployability,version,version,392,"A low number of calls generated; **Describe the issue:**. When calling I get a low number of records in a vcf file (20-40k variants). In comparison with GATK there are at last 10 times less variants generated. Also, it is much less than from 1 thousand genomes deepvariant dataset truncated to the same exome region - 1.7 mln records. **Setup**. - Operating system:. CentoOS 7. - DeepVariant version:. 1.3.0. - Installation method (Docker, built from source, etc.):. podman/singularity. - Type of data:. The data are human's whole exome sequences from Illumina. As a reference I use hg38 with alt contigs. I use truseq v1.2 exome bed file. Besides using bwa-mem2, samtools merge and sort there is not much preprocessing. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=\$PWD/$idxbase \. --reads=\$PWD/${sample_id}.bam \. --regions=\$PWD/$bed_file \. --output_vcf=\$PWD/${sample_id}.vcf.gz \. --output_gvcf=\$PWD/${sample_id}.g.vcf.gz \. --num_shards=${task.cpus}. ```. I also used glnexus. ```. glnexus_cli . --bed /in/truseq-dna-exome-targeted-regions-manifest-v1-2.bed . --config DeepVariantWES /in/vcf_deepvariant/*.g.vcf.gz > merged.bcf. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:411,deployability,Instal,Installation,411,"A low number of calls generated; **Describe the issue:**. When calling I get a low number of records in a vcf file (20-40k variants). In comparison with GATK there are at last 10 times less variants generated. Also, it is much less than from 1 thousand genomes deepvariant dataset truncated to the same exome region - 1.7 mln records. **Setup**. - Operating system:. CentoOS 7. - DeepVariant version:. 1.3.0. - Installation method (Docker, built from source, etc.):. podman/singularity. - Type of data:. The data are human's whole exome sequences from Illumina. As a reference I use hg38 with alt contigs. I use truseq v1.2 exome bed file. Besides using bwa-mem2, samtools merge and sort there is not much preprocessing. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=\$PWD/$idxbase \. --reads=\$PWD/${sample_id}.bam \. --regions=\$PWD/$bed_file \. --output_vcf=\$PWD/${sample_id}.vcf.gz \. --output_gvcf=\$PWD/${sample_id}.g.vcf.gz \. --num_shards=${task.cpus}. ```. I also used glnexus. ```. glnexus_cli . --bed /in/truseq-dna-exome-targeted-regions-manifest-v1-2.bed . --config DeepVariantWES /in/vcf_deepvariant/*.g.vcf.gz > merged.bcf. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:1017,energy efficiency,cpu,cpus,1017,"A low number of calls generated; **Describe the issue:**. When calling I get a low number of records in a vcf file (20-40k variants). In comparison with GATK there are at last 10 times less variants generated. Also, it is much less than from 1 thousand genomes deepvariant dataset truncated to the same exome region - 1.7 mln records. **Setup**. - Operating system:. CentoOS 7. - DeepVariant version:. 1.3.0. - Installation method (Docker, built from source, etc.):. podman/singularity. - Type of data:. The data are human's whole exome sequences from Illumina. As a reference I use hg38 with alt contigs. I use truseq v1.2 exome bed file. Besides using bwa-mem2, samtools merge and sort there is not much preprocessing. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=\$PWD/$idxbase \. --reads=\$PWD/${sample_id}.bam \. --regions=\$PWD/$bed_file \. --output_vcf=\$PWD/${sample_id}.vcf.gz \. --output_gvcf=\$PWD/${sample_id}.g.vcf.gz \. --num_shards=${task.cpus}. ```. I also used glnexus. ```. glnexus_cli . --bed /in/truseq-dna-exome-targeted-regions-manifest-v1-2.bed . --config DeepVariantWES /in/vcf_deepvariant/*.g.vcf.gz > merged.bcf. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:392,integrability,version,version,392,"A low number of calls generated; **Describe the issue:**. When calling I get a low number of records in a vcf file (20-40k variants). In comparison with GATK there are at last 10 times less variants generated. Also, it is much less than from 1 thousand genomes deepvariant dataset truncated to the same exome region - 1.7 mln records. **Setup**. - Operating system:. CentoOS 7. - DeepVariant version:. 1.3.0. - Installation method (Docker, built from source, etc.):. podman/singularity. - Type of data:. The data are human's whole exome sequences from Illumina. As a reference I use hg38 with alt contigs. I use truseq v1.2 exome bed file. Besides using bwa-mem2, samtools merge and sort there is not much preprocessing. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=\$PWD/$idxbase \. --reads=\$PWD/${sample_id}.bam \. --regions=\$PWD/$bed_file \. --output_vcf=\$PWD/${sample_id}.vcf.gz \. --output_gvcf=\$PWD/${sample_id}.g.vcf.gz \. --num_shards=${task.cpus}. ```. I also used glnexus. ```. glnexus_cli . --bed /in/truseq-dna-exome-targeted-regions-manifest-v1-2.bed . --config DeepVariantWES /in/vcf_deepvariant/*.g.vcf.gz > merged.bcf. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:392,modifiability,version,version,392,"A low number of calls generated; **Describe the issue:**. When calling I get a low number of records in a vcf file (20-40k variants). In comparison with GATK there are at last 10 times less variants generated. Also, it is much less than from 1 thousand genomes deepvariant dataset truncated to the same exome region - 1.7 mln records. **Setup**. - Operating system:. CentoOS 7. - DeepVariant version:. 1.3.0. - Installation method (Docker, built from source, etc.):. podman/singularity. - Type of data:. The data are human's whole exome sequences from Illumina. As a reference I use hg38 with alt contigs. I use truseq v1.2 exome bed file. Besides using bwa-mem2, samtools merge and sort there is not much preprocessing. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=\$PWD/$idxbase \. --reads=\$PWD/${sample_id}.bam \. --regions=\$PWD/$bed_file \. --output_vcf=\$PWD/${sample_id}.vcf.gz \. --output_gvcf=\$PWD/${sample_id}.g.vcf.gz \. --num_shards=${task.cpus}. ```. I also used glnexus. ```. glnexus_cli . --bed /in/truseq-dna-exome-targeted-regions-manifest-v1-2.bed . --config DeepVariantWES /in/vcf_deepvariant/*.g.vcf.gz > merged.bcf. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:179,performance,time,times,179,"A low number of calls generated; **Describe the issue:**. When calling I get a low number of records in a vcf file (20-40k variants). In comparison with GATK there are at last 10 times less variants generated. Also, it is much less than from 1 thousand genomes deepvariant dataset truncated to the same exome region - 1.7 mln records. **Setup**. - Operating system:. CentoOS 7. - DeepVariant version:. 1.3.0. - Installation method (Docker, built from source, etc.):. podman/singularity. - Type of data:. The data are human's whole exome sequences from Illumina. As a reference I use hg38 with alt contigs. I use truseq v1.2 exome bed file. Besides using bwa-mem2, samtools merge and sort there is not much preprocessing. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=\$PWD/$idxbase \. --reads=\$PWD/${sample_id}.bam \. --regions=\$PWD/$bed_file \. --output_vcf=\$PWD/${sample_id}.vcf.gz \. --output_gvcf=\$PWD/${sample_id}.g.vcf.gz \. --num_shards=${task.cpus}. ```. I also used glnexus. ```. glnexus_cli . --bed /in/truseq-dna-exome-targeted-regions-manifest-v1-2.bed . --config DeepVariantWES /in/vcf_deepvariant/*.g.vcf.gz > merged.bcf. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:1017,performance,cpu,cpus,1017,"A low number of calls generated; **Describe the issue:**. When calling I get a low number of records in a vcf file (20-40k variants). In comparison with GATK there are at last 10 times less variants generated. Also, it is much less than from 1 thousand genomes deepvariant dataset truncated to the same exome region - 1.7 mln records. **Setup**. - Operating system:. CentoOS 7. - DeepVariant version:. 1.3.0. - Installation method (Docker, built from source, etc.):. podman/singularity. - Type of data:. The data are human's whole exome sequences from Illumina. As a reference I use hg38 with alt contigs. I use truseq v1.2 exome bed file. Besides using bwa-mem2, samtools merge and sort there is not much preprocessing. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=\$PWD/$idxbase \. --reads=\$PWD/${sample_id}.bam \. --regions=\$PWD/$bed_file \. --output_vcf=\$PWD/${sample_id}.vcf.gz \. --output_gvcf=\$PWD/${sample_id}.g.vcf.gz \. --num_shards=${task.cpus}. ```. I also used glnexus. ```. glnexus_cli . --bed /in/truseq-dna-exome-targeted-regions-manifest-v1-2.bed . --config DeepVariantWES /in/vcf_deepvariant/*.g.vcf.gz > merged.bcf. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/501:748,usability,Command,Command,748,"A low number of calls generated; **Describe the issue:**. When calling I get a low number of records in a vcf file (20-40k variants). In comparison with GATK there are at last 10 times less variants generated. Also, it is much less than from 1 thousand genomes deepvariant dataset truncated to the same exome region - 1.7 mln records. **Setup**. - Operating system:. CentoOS 7. - DeepVariant version:. 1.3.0. - Installation method (Docker, built from source, etc.):. podman/singularity. - Type of data:. The data are human's whole exome sequences from Illumina. As a reference I use hg38 with alt contigs. I use truseq v1.2 exome bed file. Besides using bwa-mem2, samtools merge and sort there is not much preprocessing. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=\$PWD/$idxbase \. --reads=\$PWD/${sample_id}.bam \. --regions=\$PWD/$bed_file \. --output_vcf=\$PWD/${sample_id}.vcf.gz \. --output_gvcf=\$PWD/${sample_id}.g.vcf.gz \. --num_shards=${task.cpus}. ```. I also used glnexus. ```. glnexus_cli . --bed /in/truseq-dna-exome-targeted-regions-manifest-v1-2.bed . --config DeepVariantWES /in/vcf_deepvariant/*.g.vcf.gz > merged.bcf. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501
https://github.com/google/deepvariant/issues/502:72,availability,state,states,72,"Is deepvariant free to use?; Hi, . I can't see anywhere that explicitly states if deepvariant is free to use for the user? Also, if possible, because I am new to deepvariant (and running complex tools) - if I could possibly have some help in the initial running of it for my sequences from yourselves? I have looked at the documentation but still have a few questions and can't find anyone to ask for help. My email is HousemanA@cardiff.ac.uk. Thanks! Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/502:72,integrability,state,states,72,"Is deepvariant free to use?; Hi, . I can't see anywhere that explicitly states if deepvariant is free to use for the user? Also, if possible, because I am new to deepvariant (and running complex tools) - if I could possibly have some help in the initial running of it for my sequences from yourselves? I have looked at the documentation but still have a few questions and can't find anyone to ask for help. My email is HousemanA@cardiff.ac.uk. Thanks! Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/502:187,safety,compl,complex,187,"Is deepvariant free to use?; Hi, . I can't see anywhere that explicitly states if deepvariant is free to use for the user? Also, if possible, because I am new to deepvariant (and running complex tools) - if I could possibly have some help in the initial running of it for my sequences from yourselves? I have looked at the documentation but still have a few questions and can't find anyone to ask for help. My email is HousemanA@cardiff.ac.uk. Thanks! Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/502:187,security,compl,complex,187,"Is deepvariant free to use?; Hi, . I can't see anywhere that explicitly states if deepvariant is free to use for the user? Also, if possible, because I am new to deepvariant (and running complex tools) - if I could possibly have some help in the initial running of it for my sequences from yourselves? I have looked at the documentation but still have a few questions and can't find anyone to ask for help. My email is HousemanA@cardiff.ac.uk. Thanks! Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/502:117,usability,user,user,117,"Is deepvariant free to use?; Hi, . I can't see anywhere that explicitly states if deepvariant is free to use for the user? Also, if possible, because I am new to deepvariant (and running complex tools) - if I could possibly have some help in the initial running of it for my sequences from yourselves? I have looked at the documentation but still have a few questions and can't find anyone to ask for help. My email is HousemanA@cardiff.ac.uk. Thanks! Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/502:195,usability,tool,tools,195,"Is deepvariant free to use?; Hi, . I can't see anywhere that explicitly states if deepvariant is free to use for the user? Also, if possible, because I am new to deepvariant (and running complex tools) - if I could possibly have some help in the initial running of it for my sequences from yourselves? I have looked at the documentation but still have a few questions and can't find anyone to ask for help. My email is HousemanA@cardiff.ac.uk. Thanks! Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/502:234,usability,help,help,234,"Is deepvariant free to use?; Hi, . I can't see anywhere that explicitly states if deepvariant is free to use for the user? Also, if possible, because I am new to deepvariant (and running complex tools) - if I could possibly have some help in the initial running of it for my sequences from yourselves? I have looked at the documentation but still have a few questions and can't find anyone to ask for help. My email is HousemanA@cardiff.ac.uk. Thanks! Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/502:323,usability,document,documentation,323,"Is deepvariant free to use?; Hi, . I can't see anywhere that explicitly states if deepvariant is free to use for the user? Also, if possible, because I am new to deepvariant (and running complex tools) - if I could possibly have some help in the initial running of it for my sequences from yourselves? I have looked at the documentation but still have a few questions and can't find anyone to ask for help. My email is HousemanA@cardiff.ac.uk. Thanks! Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/502:401,usability,help,help,401,"Is deepvariant free to use?; Hi, . I can't see anywhere that explicitly states if deepvariant is free to use for the user? Also, if possible, because I am new to deepvariant (and running complex tools) - if I could possibly have some help in the initial running of it for my sequences from yourselves? I have looked at the documentation but still have a few questions and can't find anyone to ask for help. My email is HousemanA@cardiff.ac.uk. Thanks! Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502
https://github.com/google/deepvariant/issues/503:12,integrability,filter,filter-QUAL,12,"DeepVariant filter-QUAL/GQ; I ran DeepVariant(r1.1.0) and merged 1305 gvcfs by GLnexus (WES data). I found one important site. - QUALthe 6th column) 25. - AQthe 8th column) 25. - the genotype of 4 samples(controls and cases) : ./. - the genotype of 1296 samples(controls and cases) : 0/0. - the genotype other 5 samples(cases) : 0/1. - the DP and alt allele ratio in 0/1 samples:. 11,0.18 10,0.20 5,0.60 5,0.40 11,0.18. - the GQ in 0/1 samples:. 14 23 20 23 25. - the PL in 0/1 samples:. 14,0,46 23,0,43 21,1,0 23,0,30 25,0,49. Should I consider the 0/1 variant may a true site in the 5 samples? In some articles,the authors think the QUAL threshold should be set to 30(QUAL>30). How can I set thresholds for QUAL,AQ and GQ to make sure the genotype(0/1) is true ? Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:208,security,control,controls,208,"DeepVariant filter-QUAL/GQ; I ran DeepVariant(r1.1.0) and merged 1305 gvcfs by GLnexus (WES data). I found one important site. - QUALthe 6th column) 25. - AQthe 8th column) 25. - the genotype of 4 samples(controls and cases) : ./. - the genotype of 1296 samples(controls and cases) : 0/0. - the genotype other 5 samples(cases) : 0/1. - the DP and alt allele ratio in 0/1 samples:. 11,0.18 10,0.20 5,0.60 5,0.40 11,0.18. - the GQ in 0/1 samples:. 14 23 20 23 25. - the PL in 0/1 samples:. 14,0,46 23,0,43 21,1,0 23,0,30 25,0,49. Should I consider the 0/1 variant may a true site in the 5 samples? In some articles,the authors think the QUAL threshold should be set to 30(QUAL>30). How can I set thresholds for QUAL,AQ and GQ to make sure the genotype(0/1) is true ? Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:265,security,control,controls,265,"DeepVariant filter-QUAL/GQ; I ran DeepVariant(r1.1.0) and merged 1305 gvcfs by GLnexus (WES data). I found one important site. - QUALthe 6th column) 25. - AQthe 8th column) 25. - the genotype of 4 samples(controls and cases) : ./. - the genotype of 1296 samples(controls and cases) : 0/0. - the genotype other 5 samples(cases) : 0/1. - the DP and alt allele ratio in 0/1 samples:. 11,0.18 10,0.20 5,0.60 5,0.40 11,0.18. - the GQ in 0/1 samples:. 14 23 20 23 25. - the PL in 0/1 samples:. 14,0,46 23,0,43 21,1,0 23,0,30 25,0,49. Should I consider the 0/1 variant may a true site in the 5 samples? In some articles,the authors think the QUAL threshold should be set to 30(QUAL>30). How can I set thresholds for QUAL,AQ and GQ to make sure the genotype(0/1) is true ? Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:620,security,auth,authors,620,"DeepVariant filter-QUAL/GQ; I ran DeepVariant(r1.1.0) and merged 1305 gvcfs by GLnexus (WES data). I found one important site. - QUALthe 6th column) 25. - AQthe 8th column) 25. - the genotype of 4 samples(controls and cases) : ./. - the genotype of 1296 samples(controls and cases) : 0/0. - the genotype other 5 samples(cases) : 0/1. - the DP and alt allele ratio in 0/1 samples:. 11,0.18 10,0.20 5,0.60 5,0.40 11,0.18. - the GQ in 0/1 samples:. 14 23 20 23 25. - the PL in 0/1 samples:. 14,0,46 23,0,43 21,1,0 23,0,30 25,0,49. Should I consider the 0/1 variant may a true site in the 5 samples? In some articles,the authors think the QUAL threshold should be set to 30(QUAL>30). How can I set thresholds for QUAL,AQ and GQ to make sure the genotype(0/1) is true ? Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:208,testability,control,controls,208,"DeepVariant filter-QUAL/GQ; I ran DeepVariant(r1.1.0) and merged 1305 gvcfs by GLnexus (WES data). I found one important site. - QUALthe 6th column) 25. - AQthe 8th column) 25. - the genotype of 4 samples(controls and cases) : ./. - the genotype of 1296 samples(controls and cases) : 0/0. - the genotype other 5 samples(cases) : 0/1. - the DP and alt allele ratio in 0/1 samples:. 11,0.18 10,0.20 5,0.60 5,0.40 11,0.18. - the GQ in 0/1 samples:. 14 23 20 23 25. - the PL in 0/1 samples:. 14,0,46 23,0,43 21,1,0 23,0,30 25,0,49. Should I consider the 0/1 variant may a true site in the 5 samples? In some articles,the authors think the QUAL threshold should be set to 30(QUAL>30). How can I set thresholds for QUAL,AQ and GQ to make sure the genotype(0/1) is true ? Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/503:265,testability,control,controls,265,"DeepVariant filter-QUAL/GQ; I ran DeepVariant(r1.1.0) and merged 1305 gvcfs by GLnexus (WES data). I found one important site. - QUALthe 6th column) 25. - AQthe 8th column) 25. - the genotype of 4 samples(controls and cases) : ./. - the genotype of 1296 samples(controls and cases) : 0/0. - the genotype other 5 samples(cases) : 0/1. - the DP and alt allele ratio in 0/1 samples:. 11,0.18 10,0.20 5,0.60 5,0.40 11,0.18. - the GQ in 0/1 samples:. 14 23 20 23 25. - the PL in 0/1 samples:. 14,0,46 23,0,43 21,1,0 23,0,30 25,0,49. Should I consider the 0/1 variant may a true site in the 5 samples? In some articles,the authors think the QUAL threshold should be set to 30(QUAL>30). How can I set thresholds for QUAL,AQ and GQ to make sure the genotype(0/1) is true ? Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503
https://github.com/google/deepvariant/issues/504:3,safety,input,input,3,"Do input BAM files still need to be sorted?; Hello, . I saw a 2018 post that mentioned about whether DeepVariant requires the input bam files to be sorted. I was wondering if that was still the case! . Thank you, . Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/504
https://github.com/google/deepvariant/issues/504:126,safety,input,input,126,"Do input BAM files still need to be sorted?; Hello, . I saw a 2018 post that mentioned about whether DeepVariant requires the input bam files to be sorted. I was wondering if that was still the case! . Thank you, . Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/504
https://github.com/google/deepvariant/issues/504:3,usability,input,input,3,"Do input BAM files still need to be sorted?; Hello, . I saw a 2018 post that mentioned about whether DeepVariant requires the input bam files to be sorted. I was wondering if that was still the case! . Thank you, . Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/504
https://github.com/google/deepvariant/issues/504:126,usability,input,input,126,"Do input BAM files still need to be sorted?; Hello, . I saw a 2018 post that mentioned about whether DeepVariant requires the input bam files to be sorted. I was wondering if that was still the case! . Thank you, . Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/504
https://github.com/google/deepvariant/issues/505:333,modifiability,paramet,parameter,333,"SNP calls missing from the terminal parts of the reference; Hi,. I am analysing long amplicons, but I do not get calls on known positions towards the end of the amplicon. The missing positions are less than 110 bp from the end, but as it is an amplicon analysis I have full coverage to the last base. I tried with setting the region parameter to start and end of the amplicon, but I still do not get the calls. Is there a way to get these calls towards the end of the reference? Best regards,. Christian.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/505
https://github.com/google/deepvariant/issues/505:274,testability,coverag,coverage,274,"SNP calls missing from the terminal parts of the reference; Hi,. I am analysing long amplicons, but I do not get calls on known positions towards the end of the amplicon. The missing positions are less than 110 bp from the end, but as it is an amplicon analysis I have full coverage to the last base. I tried with setting the region parameter to start and end of the amplicon, but I still do not get the calls. Is there a way to get these calls towards the end of the reference? Best regards,. Christian.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/505
https://github.com/google/deepvariant/issues/506:64,deployability,contain,container,64,"What does the -B mean when running deepvariant as a singularity container?; This might be an obvious question but i cannot work it out, what does the -B mean? For example from your singularity guide: . ```. singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```. wget https://containers/deepvariant_1.3.0.sif . module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna. --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam. --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz . --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz. --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/. ```. Sorry, still figuring it out! Thanks, Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:389,deployability,contain,container,389,"What does the -B mean when running deepvariant as a singularity container?; This might be an obvious question but i cannot work it out, what does the -B mean? For example from your singularity guide: . ```. singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```. wget https://containers/deepvariant_1.3.0.sif . module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna. --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam. --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz . --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz. --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/. ```. Sorry, still figuring it out! Thanks, Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:444,deployability,contain,container,444,"What does the -B mean when running deepvariant as a singularity container?; This might be an obvious question but i cannot work it out, what does the -B mean? For example from your singularity guide: . ```. singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```. wget https://containers/deepvariant_1.3.0.sif . module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna. --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam. --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz . --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz. --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/. ```. Sorry, still figuring it out! Thanks, Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:475,deployability,contain,containers,475,"What does the -B mean when running deepvariant as a singularity container?; This might be an obvious question but i cannot work it out, what does the -B mean? For example from your singularity guide: . ```. singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```. wget https://containers/deepvariant_1.3.0.sif . module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna. --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam. --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz . --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz. --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/. ```. Sorry, still figuring it out! Thanks, Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:510,deployability,modul,module,510,"What does the -B mean when running deepvariant as a singularity container?; This might be an obvious question but i cannot work it out, what does the -B mean? For example from your singularity guide: . ```. singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```. wget https://containers/deepvariant_1.3.0.sif . module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna. --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam. --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz . --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz. --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/. ```. Sorry, still figuring it out! Thanks, Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:517,energy efficiency,load,load,517,"What does the -B mean when running deepvariant as a singularity container?; This might be an obvious question but i cannot work it out, what does the -B mean? For example from your singularity guide: . ```. singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```. wget https://containers/deepvariant_1.3.0.sif . module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna. --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam. --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz . --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz. --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/. ```. Sorry, still figuring it out! Thanks, Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:510,modifiability,modul,module,510,"What does the -B mean when running deepvariant as a singularity container?; This might be an obvious question but i cannot work it out, what does the -B mean? For example from your singularity guide: . ```. singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```. wget https://containers/deepvariant_1.3.0.sif . module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna. --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam. --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz . --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz. --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/. ```. Sorry, still figuring it out! Thanks, Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:944,modifiability,interm,intermediateresults,944,"What does the -B mean when running deepvariant as a singularity container?; This might be an obvious question but i cannot work it out, what does the -B mean? For example from your singularity guide: . ```. singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```. wget https://containers/deepvariant_1.3.0.sif . module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna. --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam. --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz . --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz. --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/. ```. Sorry, still figuring it out! Thanks, Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:517,performance,load,load,517,"What does the -B mean when running deepvariant as a singularity container?; This might be an obvious question but i cannot work it out, what does the -B mean? For example from your singularity guide: . ```. singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```. wget https://containers/deepvariant_1.3.0.sif . module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna. --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam. --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz . --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz. --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/. ```. Sorry, still figuring it out! Thanks, Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:5,reliability,doe,does,5,"What does the -B mean when running deepvariant as a singularity container?; This might be an obvious question but i cannot work it out, what does the -B mean? For example from your singularity guide: . ```. singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```. wget https://containers/deepvariant_1.3.0.sif . module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna. --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam. --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz . --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz. --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/. ```. Sorry, still figuring it out! Thanks, Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:141,reliability,doe,does,141,"What does the -B mean when running deepvariant as a singularity container?; This might be an obvious question but i cannot work it out, what does the -B mean? For example from your singularity guide: . ```. singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```. wget https://containers/deepvariant_1.3.0.sif . module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna. --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam. --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz . --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz. --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/. ```. Sorry, still figuring it out! Thanks, Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:510,safety,modul,module,510,"What does the -B mean when running deepvariant as a singularity container?; This might be an obvious question but i cannot work it out, what does the -B mean? For example from your singularity guide: . ```. singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```. wget https://containers/deepvariant_1.3.0.sif . module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna. --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam. --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz . --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz. --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/. ```. Sorry, still figuring it out! Thanks, Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/issues/506:193,usability,guid,guide,193,"What does the -B mean when running deepvariant as a singularity container?; This might be an obvious question but i cannot work it out, what does the -B mean? For example from your singularity guide: . ```. singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```. wget https://containers/deepvariant_1.3.0.sif . module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna. --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam. --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz . --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz. --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/. ```. Sorry, still figuring it out! Thanks, Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506
https://github.com/google/deepvariant/pull/507:67,performance,time,time,67,Add the Keyword blog link; We are not taking pull requests at this time. This is an internal pull request from the DeepVariant team.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/507
https://github.com/google/deepvariant/pull/507:127,security,team,team,127,Add the Keyword blog link; We are not taking pull requests at this time. This is an internal pull request from the DeepVariant team.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/507
https://github.com/google/deepvariant/pull/508:68,performance,time,time,68,Add the AI blog from 2021.; We are not taking pull requests at this time. This is an internal pull request from the DeepVariant team.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/508
https://github.com/google/deepvariant/pull/508:128,security,team,team,128,Add the AI blog from 2021.; We are not taking pull requests at this time. This is an internal pull request from the DeepVariant team.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/508
https://github.com/google/deepvariant/pull/509:72,performance,time,time,72,Add the Open Source blog link.; We are not taking pull requests at this time. This is an internal pull request from the DeepVariant team.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/509
https://github.com/google/deepvariant/pull/509:132,security,team,team,132,Add the Open Source blog link.; We are not taking pull requests at this time. This is an internal pull request from the DeepVariant team.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/509
https://github.com/google/deepvariant/issues/510:313,interoperability,specif,specific-intervals,313,Question: Usage WGS with intervals; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Not an issue but a usage question:. GATK recommends [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035889551-When-should-I-restrict-my-analysis-to-specific-intervals-) to use intervals also for whole genome sequencing analysis to blacklist certain areas but also allowing to process each interval in parallel and merging the vcfs later. I was wondering if it is also possible to do the same with deepvariant by using the `--regions` parameter for each region in parallel or if you would rather recommend to input all intervals in a single bed file and providing them all at once. .,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:599,modifiability,paramet,parameter,599,Question: Usage WGS with intervals; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Not an issue but a usage question:. GATK recommends [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035889551-When-should-I-restrict-my-analysis-to-specific-intervals-) to use intervals also for whole genome sequencing analysis to blacklist certain areas but also allowing to process each interval in parallel and merging the vcfs later. I was wondering if it is also possible to do the same with deepvariant by using the `--regions` parameter for each region in parallel or if you would rather recommend to input all intervals in a single bed file and providing them all at once. .,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:466,performance,parallel,parallel,466,Question: Usage WGS with intervals; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Not an issue but a usage question:. GATK recommends [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035889551-When-should-I-restrict-my-analysis-to-specific-intervals-) to use intervals also for whole genome sequencing analysis to blacklist certain areas but also allowing to process each interval in parallel and merging the vcfs later. I was wondering if it is also possible to do the same with deepvariant by using the `--regions` parameter for each region in parallel or if you would rather recommend to input all intervals in a single bed file and providing them all at once. .,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:628,performance,parallel,parallel,628,Question: Usage WGS with intervals; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Not an issue but a usage question:. GATK recommends [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035889551-When-should-I-restrict-my-analysis-to-specific-intervals-) to use intervals also for whole genome sequencing analysis to blacklist certain areas but also allowing to process each interval in parallel and merging the vcfs later. I was wondering if it is also possible to do the same with deepvariant by using the `--regions` parameter for each region in parallel or if you would rather recommend to input all intervals in a single bed file and providing them all at once. .,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:673,safety,input,input,673,Question: Usage WGS with intervals; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Not an issue but a usage question:. GATK recommends [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035889551-When-should-I-restrict-my-analysis-to-specific-intervals-) to use intervals also for whole genome sequencing analysis to blacklist certain areas but also allowing to process each interval in parallel and merging the vcfs later. I was wondering if it is also possible to do the same with deepvariant by using the `--regions` parameter for each region in parallel or if you would rather recommend to input all intervals in a single bed file and providing them all at once. .,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/510:673,usability,input,input,673,Question: Usage WGS with intervals; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. Not an issue but a usage question:. GATK recommends [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035889551-When-should-I-restrict-my-analysis-to-specific-intervals-) to use intervals also for whole genome sequencing analysis to blacklist certain areas but also allowing to process each interval in parallel and merging the vcfs later. I was wondering if it is also possible to do the same with deepvariant by using the `--regions` parameter for each region in parallel or if you would rather recommend to input all intervals in a single bed file and providing them all at once. .,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510
https://github.com/google/deepvariant/issues/511:214,availability,error,error,214,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:0,deployability,Updat,Update,0,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:39,deployability,build,build,39,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:128,deployability,patch,patch,128,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:159,deployability,build,build,159,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:457,deployability,build,build,457,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:544,deployability,toolchain,toolchains,544,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:616,deployability,toolchain,toolchains,616,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:1231,deployability,build,build,1231,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:1284,deployability,updat,update,1284,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:1302,deployability,version,version,1302,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:1302,integrability,version,version,1302,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:446,modifiability,Inherit,Inherited,446,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:1302,modifiability,version,version,1302,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:214,performance,error,error,214,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:0,safety,Updat,Update,0,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:128,safety,patch,patch,128,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:214,safety,error,error,214,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:235,safety,test,test,235,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:407,safety,test,test,407,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:1284,safety,updat,update,1284,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:0,security,Updat,Update,0,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:128,security,patch,patch,128,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:280,security,sign,sign-compare,280,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:1284,security,updat,update,1284,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:235,testability,test,test,235,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:407,testability,test,test,407,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:61,usability,support,support,61,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:214,usability,error,error,214,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:544,usability,tool,toolchains,544,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/511:616,usability,tool,toolchains,616,"Update TensorFlow; Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:. ```. + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/... [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2. [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1. ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511
https://github.com/google/deepvariant/issues/512:358,availability,Error,Error,358,"Question about 'Could not create PileupImage' warning; Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it? **Setup**. - DeepVariant version: 1.1.0 and 1.3.0. - Installation method: Singularity. - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** . ```. W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 . W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 . W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 . W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804. W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 . W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 . ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/512
https://github.com/google/deepvariant/issues/512:225,deployability,version,version,225,"Question about 'Could not create PileupImage' warning; Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it? **Setup**. - DeepVariant version: 1.1.0 and 1.3.0. - Installation method: Singularity. - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** . ```. W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 . W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 . W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 . W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804. W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 . W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 . ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/512
https://github.com/google/deepvariant/issues/512:253,deployability,Instal,Installation,253,"Question about 'Could not create PileupImage' warning; Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it? **Setup**. - DeepVariant version: 1.1.0 and 1.3.0. - Installation method: Singularity. - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** . ```. W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 . W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 . W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 . W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804. W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 . W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 . ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/512
https://github.com/google/deepvariant/issues/512:225,integrability,version,version,225,"Question about 'Could not create PileupImage' warning; Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it? **Setup**. - DeepVariant version: 1.1.0 and 1.3.0. - Installation method: Singularity. - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** . ```. W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 . W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 . W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 . W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804. W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 . W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 . ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/512
https://github.com/google/deepvariant/issues/512:225,modifiability,version,version,225,"Question about 'Could not create PileupImage' warning; Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it? **Setup**. - DeepVariant version: 1.1.0 and 1.3.0. - Installation method: Singularity. - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** . ```. W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 . W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 . W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 . W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804. W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 . W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 . ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/512
https://github.com/google/deepvariant/issues/512:358,performance,Error,Error,358,"Question about 'Could not create PileupImage' warning; Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it? **Setup**. - DeepVariant version: 1.1.0 and 1.3.0. - Installation method: Singularity. - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** . ```. W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 . W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 . W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 . W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804. W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 . W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 . ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/512
https://github.com/google/deepvariant/issues/512:101,reliability,doe,does,101,"Question about 'Could not create PileupImage' warning; Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it? **Setup**. - DeepVariant version: 1.1.0 and 1.3.0. - Installation method: Singularity. - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** . ```. W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 . W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 . W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 . W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804. W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 . W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 . ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/512
https://github.com/google/deepvariant/issues/512:358,safety,Error,Error,358,"Question about 'Could not create PileupImage' warning; Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it? **Setup**. - DeepVariant version: 1.1.0 and 1.3.0. - Installation method: Singularity. - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** . ```. W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 . W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 . W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 . W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804. W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 . W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 . ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/512
https://github.com/google/deepvariant/issues/512:364,testability,trace,trace,364,"Question about 'Could not create PileupImage' warning; Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it? **Setup**. - DeepVariant version: 1.1.0 and 1.3.0. - Installation method: Singularity. - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** . ```. W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 . W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 . W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 . W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804. W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 . W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 . ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/512
https://github.com/google/deepvariant/issues/512:358,usability,Error,Error,358,"Question about 'Could not create PileupImage' warning; Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it? **Setup**. - DeepVariant version: 1.1.0 and 1.3.0. - Installation method: Singularity. - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** . ```. W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 . W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 . W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 . W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804. W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 . W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 . ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/512
https://github.com/google/deepvariant/issues/513:195,availability,error,error,195,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:384,availability,ERROR,ERROR,384,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:405,availability,error,error,405,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:436,availability,ERROR,ERROR,436,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:451,deployability,contain,container,451,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:461,deployability,fail,failed,461,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:523,deployability,instal,install,523,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:320,interoperability,Registr,Registry,320,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:195,performance,error,error,195,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:384,performance,ERROR,ERROR,384,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:405,performance,error,error,405,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:436,performance,ERROR,ERROR,436,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:461,reliability,fail,failed,461,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:195,safety,error,error,195,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:384,safety,ERROR,ERROR,384,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:405,safety,error,error,405,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:436,safety,ERROR,ERROR,436,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:509,safety,permiss,permission,509,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:390,security,Authenticat,Authentication,390,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:107,usability,command,command,107,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:195,usability,error,error,195,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:384,usability,ERROR,ERROR,384,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:405,usability,error,error,405,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:436,usability,ERROR,ERROR,436,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/513:486,usability,help,help,486,"singularity pull ; Hi. when I try to pull the deepvariant docker image via singularity using the following command:. singularity pull docker://google/deepvariant:""1.3.0"". it return the following error:. WARNING: pull for Docker Hub is not guaranteed to produce the. WARNING: same image on repeated pull. Use Singularity Registry. WARNING: (shub://) to pull exactly equivalent images. ERROR Authentication error, exiting. Cleaning up... ERROR: pulling container failed! could you please help. I don't have the permission to install docker. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513
https://github.com/google/deepvariant/issues/514:1632,availability,error,error,1632,"ckstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2491,availability,error,error,2491,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2520,availability,error,error,2520,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:59,deployability,instal,installation,59,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:95,deployability,version,version,95,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:163,deployability,version,version,163,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:344,deployability,modul,modules,344,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:708,deployability,modul,modules,708,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:717,deployability,modul,module,717,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:742,deployability,modul,module,742,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:773,deployability,modul,module,773,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:803,deployability,modul,module,803,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:830,deployability,modul,module,830,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:859,deployability,modul,module,859,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:890,deployability,modul,module,890,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1889,deployability,modul,module,1889,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2031,deployability,modul,module,2031,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:23,energy efficiency,gpu,gpu,23,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:91,energy efficiency,GPU,GPU,91,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:119,energy efficiency,GPU,GPU,119,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:159,energy efficiency,CPU,CPU,159,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:242,energy efficiency,GPU,GPU,242,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:352,energy efficiency,load,loaded,352,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:703,energy efficiency,Load,Load,703,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:724,energy efficiency,load,load,724,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:749,energy efficiency,load,load,749,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:780,energy efficiency,load,load,780,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:810,energy efficiency,load,load,810,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:837,energy efficiency,load,load,837,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:866,energy efficiency,load,load,866,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:880,energy efficiency,profil,profiler,880,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:897,energy efficiency,load,load,897,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:975,energy efficiency,profil,profile,975,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1105,energy efficiency,gpu,gpu,1105,"a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.loc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1226,energy efficiency,gpu,gpu,1226," the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2343,energy efficiency,core,core,2343,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2624,energy efficiency,GPU,GPU,2624,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:95,integrability,version,version,95,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:163,integrability,version,version,163,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1702,interoperability,platform,platform,1702,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:95,modifiability,version,version,95,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:163,modifiability,version,version,163,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:344,modifiability,modul,modules,344,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:708,modifiability,modul,modules,708,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:717,modifiability,modul,module,717,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:742,modifiability,modul,module,742,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:773,modifiability,modul,module,773,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:803,modifiability,modul,module,803,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:830,modifiability,modul,module,830,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:859,modifiability,modul,module,859,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:890,modifiability,modul,module,890,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1889,modifiability,modul,module,1889,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1983,modifiability,pac,packages,1983,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2031,modifiability,modul,module,2031,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2129,modifiability,pac,packages,2129,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2323,modifiability,pac,packages,2323,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:23,performance,gpu,gpu,23,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:91,performance,GPU,GPU,91,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:119,performance,GPU,GPU,119,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:159,performance,CPU,CPU,159,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:242,performance,GPU,GPU,242,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:352,performance,load,loaded,352,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:703,performance,Load,Load,703,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:724,performance,load,load,724,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:749,performance,load,load,749,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:780,performance,load,load,780,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:810,performance,load,load,810,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:837,performance,load,load,837,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:866,performance,load,load,866,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:880,performance,profil,profiler,880,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:897,performance,load,load,897,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:975,performance,profil,profile,975,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1105,performance,gpu,gpu,1105,"a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.loc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1226,performance,gpu,gpu,1226," the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1632,performance,error,error,1632,"ckstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2491,performance,error,error,2491,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2520,performance,error,error,2520,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2624,performance,GPU,GPU,2624,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:344,safety,modul,modules,344,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:643,safety,test,testdata,643,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:708,safety,modul,modules,708,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:717,safety,modul,module,717,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:742,safety,modul,module,742,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:773,safety,modul,module,773,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:803,safety,modul,module,803,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:830,safety,modul,module,830,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:859,safety,modul,module,859,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:890,safety,modul,module,890,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1632,safety,error,error,1632,"ckstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1889,safety,modul,module,1889,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2031,safety,modul,module,2031,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2491,safety,error,error,2491,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2520,safety,error,error,2520,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2588,safety,test,test,2588,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:643,testability,test,testdata,643,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1331,testability,unit,unittest,1331,"ral CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1792,testability,Trace,Traceback,1792,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2588,testability,test,test,2588,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:794,usability,tool,toolkit,794,"Issue with singularity gpu; Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? . I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)? Here's the code I'm running from the Quickstart:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:1632,usability,error,error,1632,"ckstart-testdata"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2491,usability,error,error,2491,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2501,usability,help,help,2501,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2520,usability,error,error,2520,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/514:2530,usability,experien,experiencing,2530,"ta"". mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules. module load singularity. module load cuda-dcgm/2.2.9.1. module load cuda11.4/toolkit. module load cuda11.4/blas. module load cuda11.4/nsight. module load cuda11.4/profiler. module load cuda11.4/fft. source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh. conda activate TensorFlow_GPU. # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. --nv \. docker://google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"". ```. And here's my error:. ```. 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb. ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server? Thanks! Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514
https://github.com/google/deepvariant/issues/515:45,availability,Cluster,Cluster,45,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:340,availability,cluster,cluster,340,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:665,availability,error,error,665,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:2622,availability,error,error,2622,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:2679,availability,error,error,2679,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:45,deployability,Cluster,Cluster,45,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:74,deployability,version,version,74,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:89,deployability,Instal,Installation,89,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:340,deployability,cluster,cluster,340,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:1309,deployability,contain,container,1309," the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:1378,deployability,contain,container,1378,"job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:1416,deployability,contain,container,1416,"cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_resul",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:1534,deployability,contain,container,1534," --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. Aft",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:1679,deployability,contain,containers,1679,"dir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:421,energy efficiency,cpu,cpus-per-task,421,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:74,integrability,version,version,74,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:74,modifiability,version,version,74,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:1099,modifiability,variab,variables,1099,"ingularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_Comprehensive",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:421,performance,cpu,cpus-per-task,421,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:665,performance,error,error,665,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:2622,performance,error,error,2622,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:2679,performance,error,error,2679,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:2892,reliability,doe,does,2892,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:572,safety,test,testing,572,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:665,safety,error,error,665,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:1050,safety,test,testdata,1050,"Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:1856,safety,test,testdata,1856,"xist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such fil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:1972,safety,test,testdata,1972," INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:2076,safety,test,testdata,2076,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:2603,safety,input,input,2603,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:2622,safety,error,error,2622,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:2679,safety,error,error,2679,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:116,testability,Instrument,Instrument,116,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:572,testability,test,testing,572,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:1050,testability,test,testdata,1050,"Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:1856,testability,test,testdata,1856,"xist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such fil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:1972,testability,test,testdata,1972," INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:2076,testability,test,testdata,2076,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:665,usability,error,error,665,"Output files not created; Hello. OS: Scicore Cluster, Linux. Deep Variant version:1.2.0. Installation: Singularity. Instrument: Ilumina. Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:1159,usability,command,command,1159,"uencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:. ```. #!/bin/bash. #SBATCH --job-name=Deepvariant_debug. #SBATCH --cpus-per-task=2 # change this according to your needs. #SBATCH --mem=8G # change this according to your needs. #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes. #SBATCH --output=myrun.o%j. #SBATCH --error=myrun.e%j. mkdir -p output. mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000. BIN_VERSION=""1.2.0"". # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder. export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output . export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:2603,usability,input,input,2603,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:2622,usability,error,error,2622,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/515:2679,usability,error,error,2679,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata . # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add. # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container. # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif). singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \. /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \. --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \. --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed. --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \. --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \. --call_variants_extra_args=""use_openvino=true"" \. --num_shards=$(nproc) \. --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \. --dry_run=true. ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515
https://github.com/google/deepvariant/issues/516:365,deployability,log,logically,365,"Adjacent deletions > 100% allele depth; I have a sample with a 4bp deletion at pos 775 and 1bp del at 779. Looking at the raw reads they seem to always be separate, and I confirmed there are no 5bp deletions called in this region in the alignment (via analyzing cigarStrings). DeepVariant is calling the 4bp del at 40% AD and the 1bp del at 77% AD. How could these logically add up to > 100%? Wouldn't that imply instances of 5bp deletion, but that would be reported separately, no? If someone could help me interoperate this that would be great. <img width=""113"" alt=""ComplimentaryIndelsP01-24"" src=""https://user-images.githubusercontent.com/65191963/153235606-e34c7406-d339-4b20-8ba7-fb29b42c713d.PNG"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/516
https://github.com/google/deepvariant/issues/516:508,interoperability,interop,interoperate,508,"Adjacent deletions > 100% allele depth; I have a sample with a 4bp deletion at pos 775 and 1bp del at 779. Looking at the raw reads they seem to always be separate, and I confirmed there are no 5bp deletions called in this region in the alignment (via analyzing cigarStrings). DeepVariant is calling the 4bp del at 40% AD and the 1bp del at 77% AD. How could these logically add up to > 100%? Wouldn't that imply instances of 5bp deletion, but that would be reported separately, no? If someone could help me interoperate this that would be great. <img width=""113"" alt=""ComplimentaryIndelsP01-24"" src=""https://user-images.githubusercontent.com/65191963/153235606-e34c7406-d339-4b20-8ba7-fb29b42c713d.PNG"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/516
https://github.com/google/deepvariant/issues/516:508,modifiability,interop,interoperate,508,"Adjacent deletions > 100% allele depth; I have a sample with a 4bp deletion at pos 775 and 1bp del at 779. Looking at the raw reads they seem to always be separate, and I confirmed there are no 5bp deletions called in this region in the alignment (via analyzing cigarStrings). DeepVariant is calling the 4bp del at 40% AD and the 1bp del at 77% AD. How could these logically add up to > 100%? Wouldn't that imply instances of 5bp deletion, but that would be reported separately, no? If someone could help me interoperate this that would be great. <img width=""113"" alt=""ComplimentaryIndelsP01-24"" src=""https://user-images.githubusercontent.com/65191963/153235606-e34c7406-d339-4b20-8ba7-fb29b42c713d.PNG"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/516
https://github.com/google/deepvariant/issues/516:365,safety,log,logically,365,"Adjacent deletions > 100% allele depth; I have a sample with a 4bp deletion at pos 775 and 1bp del at 779. Looking at the raw reads they seem to always be separate, and I confirmed there are no 5bp deletions called in this region in the alignment (via analyzing cigarStrings). DeepVariant is calling the 4bp del at 40% AD and the 1bp del at 77% AD. How could these logically add up to > 100%? Wouldn't that imply instances of 5bp deletion, but that would be reported separately, no? If someone could help me interoperate this that would be great. <img width=""113"" alt=""ComplimentaryIndelsP01-24"" src=""https://user-images.githubusercontent.com/65191963/153235606-e34c7406-d339-4b20-8ba7-fb29b42c713d.PNG"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/516
https://github.com/google/deepvariant/issues/516:365,security,log,logically,365,"Adjacent deletions > 100% allele depth; I have a sample with a 4bp deletion at pos 775 and 1bp del at 779. Looking at the raw reads they seem to always be separate, and I confirmed there are no 5bp deletions called in this region in the alignment (via analyzing cigarStrings). DeepVariant is calling the 4bp del at 40% AD and the 1bp del at 77% AD. How could these logically add up to > 100%? Wouldn't that imply instances of 5bp deletion, but that would be reported separately, no? If someone could help me interoperate this that would be great. <img width=""113"" alt=""ComplimentaryIndelsP01-24"" src=""https://user-images.githubusercontent.com/65191963/153235606-e34c7406-d339-4b20-8ba7-fb29b42c713d.PNG"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/516
https://github.com/google/deepvariant/issues/516:365,testability,log,logically,365,"Adjacent deletions > 100% allele depth; I have a sample with a 4bp deletion at pos 775 and 1bp del at 779. Looking at the raw reads they seem to always be separate, and I confirmed there are no 5bp deletions called in this region in the alignment (via analyzing cigarStrings). DeepVariant is calling the 4bp del at 40% AD and the 1bp del at 77% AD. How could these logically add up to > 100%? Wouldn't that imply instances of 5bp deletion, but that would be reported separately, no? If someone could help me interoperate this that would be great. <img width=""113"" alt=""ComplimentaryIndelsP01-24"" src=""https://user-images.githubusercontent.com/65191963/153235606-e34c7406-d339-4b20-8ba7-fb29b42c713d.PNG"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/516
https://github.com/google/deepvariant/issues/516:171,usability,confirm,confirmed,171,"Adjacent deletions > 100% allele depth; I have a sample with a 4bp deletion at pos 775 and 1bp del at 779. Looking at the raw reads they seem to always be separate, and I confirmed there are no 5bp deletions called in this region in the alignment (via analyzing cigarStrings). DeepVariant is calling the 4bp del at 40% AD and the 1bp del at 77% AD. How could these logically add up to > 100%? Wouldn't that imply instances of 5bp deletion, but that would be reported separately, no? If someone could help me interoperate this that would be great. <img width=""113"" alt=""ComplimentaryIndelsP01-24"" src=""https://user-images.githubusercontent.com/65191963/153235606-e34c7406-d339-4b20-8ba7-fb29b42c713d.PNG"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/516
https://github.com/google/deepvariant/issues/516:500,usability,help,help,500,"Adjacent deletions > 100% allele depth; I have a sample with a 4bp deletion at pos 775 and 1bp del at 779. Looking at the raw reads they seem to always be separate, and I confirmed there are no 5bp deletions called in this region in the alignment (via analyzing cigarStrings). DeepVariant is calling the 4bp del at 40% AD and the 1bp del at 77% AD. How could these logically add up to > 100%? Wouldn't that imply instances of 5bp deletion, but that would be reported separately, no? If someone could help me interoperate this that would be great. <img width=""113"" alt=""ComplimentaryIndelsP01-24"" src=""https://user-images.githubusercontent.com/65191963/153235606-e34c7406-d339-4b20-8ba7-fb29b42c713d.PNG"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/516
https://github.com/google/deepvariant/issues/516:609,usability,user,user-images,609,"Adjacent deletions > 100% allele depth; I have a sample with a 4bp deletion at pos 775 and 1bp del at 779. Looking at the raw reads they seem to always be separate, and I confirmed there are no 5bp deletions called in this region in the alignment (via analyzing cigarStrings). DeepVariant is calling the 4bp del at 40% AD and the 1bp del at 77% AD. How could these logically add up to > 100%? Wouldn't that imply instances of 5bp deletion, but that would be reported separately, no? If someone could help me interoperate this that would be great. <img width=""113"" alt=""ComplimentaryIndelsP01-24"" src=""https://user-images.githubusercontent.com/65191963/153235606-e34c7406-d339-4b20-8ba7-fb29b42c713d.PNG"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/516
https://github.com/google/deepvariant/issues/517:189,availability,error,error,189,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:474,availability,error,error,474,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:663,availability,Operat,Operating,663,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1647,availability,error,error,1647,"**Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Trans",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:2057,availability,Error,Error,2057,"le/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants. I0217 17:00:24.205814 47945364948800 postprocess_variants.py:1204] Writing variants to VCF. I0217 17:00:24.205858 47945364948800 postprocess_variants.py:774] Writing output to VCF file: ""...""/_deepvariant.vcf.gz. I0217 17:00:24.230250 47945364948800 genomics_writer.py:175] Writing ""...""/_deepvariant.vcf.gz with NativeVcfWriter. I0217 17:00:24.234843 47945364948800 p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:705,deployability,version,version,705,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:723,deployability,Instal,Installation,723,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:4122,deployability,modul,module,4122,"5637 47945364948800 postprocess_variants.py:783] 100001 variants written. W0217 17:00:50.128011 47945364948800 postprocess_variants.py:403] Alt allele indices found from call_variants_outputs for variant reference_bases: ""GTTTT"". alternate_bases: ""G"". alternate_bases: ""GT"". alternate_bases: ""GTT"". calls {. info {. key: ""AD"". value {. values {. int_value: 18. }. values {. int_value: 33. }. values {. int_value: 10. }. values {. int_value: 6. }. }. }. info {. key: ""DP"". value {. values {. int_value: 79. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.4177215189873418. }. values {. number_value: 0.12658227848101267. }. values {. number_value: 0.0759493670886076. }. }. }. genotype: -1. genotype: -1. call_set_name: ""sample"". }. end: 160351258. reference_name: ""1"". start: 160351253. is [[0], [1], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:5438,energy efficiency,predict,predictions,5438,"ant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? . No, the quick start and also chr22 from the same sample ran through. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:705,integrability,version,version,705,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:2645,integrability,Transform,Transforming,2645,"r. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants. I0217 17:00:24.205814 47945364948800 postprocess_variants.py:1204] Writing variants to VCF. I0217 17:00:24.205858 47945364948800 postprocess_variants.py:774] Writing output to VCF file: ""...""/_deepvariant.vcf.gz. I0217 17:00:24.230250 47945364948800 genomics_writer.py:175] Writing ""...""/_deepvariant.vcf.gz with NativeVcfWriter. I0217 17:00:24.234843 47945364948800 postprocess_variants.py:783] 1 variants written. I0217 17:00:38.475637 47945364948800 postprocess_variants.py:783] 100001 variants written. W0217 17:00:50.128011 47945364948800 postprocess_variants.py:403] Alt allele indices found from call_variants_outputs for variant reference_bases: ""GTTTT"". alternate_bases: ""G"". alternate_bases: ""GT"". alternate_bases: ""GTT"". calls {. info {. key: ""AD"". value {. values {. int_value: 18. }. values {. int_value: 33. }. values {. int_value: 10. }. values {. int_value: 6. }. }. }. info {. key: ""DP"". value {. values {. int_value: 79. }. }. }. info {. key",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:2645,interoperability,Transform,Transforming,2645,"r. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants. I0217 17:00:24.205814 47945364948800 postprocess_variants.py:1204] Writing variants to VCF. I0217 17:00:24.205858 47945364948800 postprocess_variants.py:774] Writing output to VCF file: ""...""/_deepvariant.vcf.gz. I0217 17:00:24.230250 47945364948800 genomics_writer.py:175] Writing ""...""/_deepvariant.vcf.gz with NativeVcfWriter. I0217 17:00:24.234843 47945364948800 postprocess_variants.py:783] 1 variants written. I0217 17:00:38.475637 47945364948800 postprocess_variants.py:783] 100001 variants written. W0217 17:00:50.128011 47945364948800 postprocess_variants.py:403] Alt allele indices found from call_variants_outputs for variant reference_bases: ""GTTTT"". alternate_bases: ""G"". alternate_bases: ""GT"". alternate_bases: ""GTT"". calls {. info {. key: ""AD"". value {. values {. int_value: 18. }. values {. int_value: 33. }. values {. int_value: 10. }. values {. int_value: 6. }. }. }. info {. key: ""DP"". value {. values {. int_value: 79. }. }. }. info {. key",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:4218,interoperability,platform,platform,4218,"7945364948800 postprocess_variants.py:403] Alt allele indices found from call_variants_outputs for variant reference_bases: ""GTTTT"". alternate_bases: ""G"". alternate_bases: ""GT"". alternate_bases: ""GTT"". calls {. info {. key: ""AD"". value {. values {. int_value: 18. }. values {. int_value: 33. }. values {. int_value: 10. }. values {. int_value: 6. }. }. }. info {. key: ""DP"". value {. values {. int_value: 79. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.4177215189873418. }. values {. number_value: 0.12658227848101267. }. values {. number_value: 0.0759493670886076. }. }. }. genotype: -1. genotype: -1. call_set_name: ""sample"". }. end: 160351258. reference_name: ""1"". start: 160351253. is [[0], [1], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:705,modifiability,version,version,705,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:4122,modifiability,modul,module,4122,"5637 47945364948800 postprocess_variants.py:783] 100001 variants written. W0217 17:00:50.128011 47945364948800 postprocess_variants.py:403] Alt allele indices found from call_variants_outputs for variant reference_bases: ""GTTTT"". alternate_bases: ""G"". alternate_bases: ""GT"". alternate_bases: ""GTT"". calls {. info {. key: ""AD"". value {. values {. int_value: 18. }. values {. int_value: 33. }. values {. int_value: 10. }. values {. int_value: 6. }. }. }. info {. key: ""DP"". value {. values {. int_value: 79. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.4177215189873418. }. values {. number_value: 0.12658227848101267. }. values {. number_value: 0.0759493670886076. }. }. }. genotype: -1. genotype: -1. call_set_name: ""sample"". }. end: 160351258. reference_name: ""1"". start: 160351253. is [[0], [1], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:4191,modifiability,pac,packages,4191,"en. W0217 17:00:50.128011 47945364948800 postprocess_variants.py:403] Alt allele indices found from call_variants_outputs for variant reference_bases: ""GTTTT"". alternate_bases: ""G"". alternate_bases: ""GT"". alternate_bases: ""GTT"". calls {. info {. key: ""AD"". value {. values {. int_value: 18. }. values {. int_value: 33. }. values {. int_value: 10. }. values {. int_value: 6. }. }. }. info {. key: ""DP"". value {. values {. int_value: 79. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.4177215189873418. }. values {. number_value: 0.12658227848101267. }. values {. number_value: 0.0759493670886076. }. }. }. genotype: -1. genotype: -1. call_set_name: ""sample"". }. end: 160351258. reference_name: ""1"". start: 160351253. is [[0], [1], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:189,performance,error,error,189,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:474,performance,error,error,474,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1647,performance,error,error,1647,"**Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Trans",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:2057,performance,Error,Error,2057,"le/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants. I0217 17:00:24.205814 47945364948800 postprocess_variants.py:1204] Writing variants to VCF. I0217 17:00:24.205858 47945364948800 postprocess_variants.py:774] Writing output to VCF file: ""...""/_deepvariant.vcf.gz. I0217 17:00:24.230250 47945364948800 genomics_writer.py:175] Writing ""...""/_deepvariant.vcf.gz with NativeVcfWriter. I0217 17:00:24.234843 47945364948800 p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:5751,reliability,Doe,Does,5751,"ant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? . No, the quick start and also chr22 from the same sample ran through. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:49,safety,sanit,sanity,49,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:189,safety,error,error,189,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:252,safety,test,test,252,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:474,safety,error,error,474,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1085,safety,input,input,1085,"AQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1226,safety,input,input,1226,"iants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 202",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1313,safety,input,input,1313," any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/cal",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1378,safety,input,input,1378,"ere](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1465,safety,input,input,1465,"milar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I021",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1647,safety,error,error,1647,"**Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Trans",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1733,safety,input,input,1733,"method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants. I0217 17:00:24.205814 47945364948800 postpro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1886,safety,input,input,1886,"pecial that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants. I0217 17:00:24.205814 47945364948800 postprocess_variants.py:1204] Writing variants to VCF. I0217 17:00:24.205858 47945364948800 postprocess_variants.py:774] Writing output to VCF file: ""...""/_deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1980,safety,input,input,1980,": . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants. I0217 17:00:24.205814 47945364948800 postprocess_variants.py:1204] Writing variants to VCF. I0217 17:00:24.205858 47945364948800 postprocess_variants.py:774] Writing output to VCF file: ""...""/_deepvariant.vcf.gz. I0217 17:00:24.230250 47945364948800 genomics_writer.py:175] Writing ""...""/_de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:2057,safety,Error,Error,2057,"le/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants. I0217 17:00:24.205814 47945364948800 postprocess_variants.py:1204] Writing variants to VCF. I0217 17:00:24.205858 47945364948800 postprocess_variants.py:774] Writing output to VCF file: ""...""/_deepvariant.vcf.gz. I0217 17:00:24.230250 47945364948800 genomics_writer.py:175] Writing ""...""/_deepvariant.vcf.gz with NativeVcfWriter. I0217 17:00:24.234843 47945364948800 p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:4122,safety,modul,module,4122,"5637 47945364948800 postprocess_variants.py:783] 100001 variants written. W0217 17:00:50.128011 47945364948800 postprocess_variants.py:403] Alt allele indices found from call_variants_outputs for variant reference_bases: ""GTTTT"". alternate_bases: ""G"". alternate_bases: ""GT"". alternate_bases: ""GTT"". calls {. info {. key: ""AD"". value {. values {. int_value: 18. }. values {. int_value: 33. }. values {. int_value: 10. }. values {. int_value: 6. }. }. }. info {. key: ""DP"". value {. values {. int_value: 79. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.4177215189873418. }. values {. number_value: 0.12658227848101267. }. values {. number_value: 0.0759493670886076. }. }. }. genotype: -1. genotype: -1. call_set_name: ""sample"". }. end: 160351258. reference_name: ""1"". start: 160351253. is [[0], [1], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:5438,safety,predict,predictions,5438,"ant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? . No, the quick start and also chr22 from the same sample ran through. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:5664,safety,sanit,sanity,5664,"ant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? . No, the quick start and also chr22 from the same sample ran through. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:5730,safety,sanit,sanity,5730,"ant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? . No, the quick start and also chr22 from the same sample ran through. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:5772,safety,test,test,5772,"ant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? . No, the quick start and also chr22 from the same sample ran through. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:5808,safety,test,test,5808,"ant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? . No, the quick start and also chr22 from the same sample ran through. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:49,security,sanit,sanity,49,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:994,security,Modif,Modified,994,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:5664,security,sanit,sanity,5664,"ant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? . No, the quick start and also chr22 from the same sample ran through. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:5730,security,sanit,sanity,5730,"ant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? . No, the quick start and also chr22 from the same sample ran through. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:252,testability,test,test,252,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:849,testability,instrument,instrument,849,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:2063,testability,trace,trace,2063," -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants. I0217 17:00:24.205814 47945364948800 postprocess_variants.py:1204] Writing variants to VCF. I0217 17:00:24.205858 47945364948800 postprocess_variants.py:774] Writing output to VCF file: ""...""/_deepvariant.vcf.gz. I0217 17:00:24.230250 47945364948800 genomics_writer.py:175] Writing ""...""/_deepvariant.vcf.gz with NativeVcfWriter. I0217 17:00:24.234843 47945364948800 postpro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:3966,testability,Trace,Traceback,3966,"ng ""...""/_deepvariant.vcf.gz with NativeVcfWriter. I0217 17:00:24.234843 47945364948800 postprocess_variants.py:783] 1 variants written. I0217 17:00:38.475637 47945364948800 postprocess_variants.py:783] 100001 variants written. W0217 17:00:50.128011 47945364948800 postprocess_variants.py:403] Alt allele indices found from call_variants_outputs for variant reference_bases: ""GTTTT"". alternate_bases: ""G"". alternate_bases: ""GT"". alternate_bases: ""GTT"". calls {. info {. key: ""AD"". value {. values {. int_value: 18. }. values {. int_value: 33. }. values {. int_value: 10. }. values {. int_value: 6. }. }. }. info {. key: ""DP"". value {. values {. int_value: 79. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.4177215189873418. }. values {. number_value: 0.12658227848101267. }. values {. number_value: 0.0759493670886076. }. }. }. genotype: -1. genotype: -1. call_set_name: ""sample"". }. end: 160351258. reference_name: ""1"". start: 160351253. is [[0], [1], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:5772,testability,test,test,5772,"ant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? . No, the quick start and also chr22 from the same sample ran through. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:5808,testability,test,test,5808,"ant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? . No, the quick start and also chr22 from the same sample ran through. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:6054,testability,context,context,6054,"ant/deepvariant/postprocess_variants.py"", line 1249, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf. for variant in variant_iterable:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants. canonical_variant, predictions = merge_predictions(. File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? . No, the quick start and also chr22 from the same sample ran through. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:189,usability,error,error,189,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:474,usability,error,error,474,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:976,usability,Command,Command,976,"ValueError: `call_variants_outputs` did not pass sanity check.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1085,usability,input,input,1085,"AQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1226,usability,input,input,1226,"iants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 202",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1313,usability,input,input,1313," any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/cal",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1378,usability,input,input,1378,"ere](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1465,usability,input,input,1465,"milar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I021",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1647,usability,error,error,1647,"**Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Trans",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1733,usability,input,input,1733,"method (Docker, built from source, etc.): Singularity image built from docker image. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants. I0217 17:00:24.205814 47945364948800 postpro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1886,usability,input,input,1886,"pecial that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**. - Command: . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants. I0217 17:00:24.205814 47945364948800 postprocess_variants.py:1204] Writing variants to VCF. I0217 17:00:24.205858 47945364948800 postprocess_variants.py:774] Writing output to VCF file: ""...""/_deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:1980,usability,input,input,1980,": . ```. # Modified script. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants. I0217 17:00:24.205814 47945364948800 postprocess_variants.py:1204] Writing variants to VCF. I0217 17:00:24.205858 47945364948800 postprocess_variants.py:774] Writing output to VCF file: ""...""/_deepvariant.vcf.gz. I0217 17:00:24.230250 47945364948800 genomics_writer.py:175] Writing ""...""/_de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/517:2057,usability,Error,Error,2057,"le/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=hs37d5_PhiX.fa \. --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \. --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \. --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \. --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \. --num_shards=15. ```. I have also tried postprocessing with `group_variants`, which also produces a similar error. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. -B ${INPUT_PATH}:/input \. compute_envs/deepvariant_latest.sif \. /opt/deepvariant/bin/postprocess_variants \. --group_variants=false \. --ref=hs37d5_PhiX.fa \. --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \. --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz. ```. - Error trace: (if applicable). ```. I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_. 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz. 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285. I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes. I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants. I0217 17:00:24.205814 47945364948800 postprocess_variants.py:1204] Writing variants to VCF. I0217 17:00:24.205858 47945364948800 postprocess_variants.py:774] Writing output to VCF file: ""...""/_deepvariant.vcf.gz. I0217 17:00:24.230250 47945364948800 genomics_writer.py:175] Writing ""...""/_deepvariant.vcf.gz with NativeVcfWriter. I0217 17:00:24.234843 47945364948800 p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517
https://github.com/google/deepvariant/issues/518:1490,availability,echo,echo,1490,":. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_VCF=${MOTHER_ID}.vcf.gz. SIBLING_VCF=${SIBLING_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz. FATHER_GVCF=${FATHER_ID}.gvcf.gz. MOTHER_GVCF=${MOTHER_ID}.gvcf.gz. SIBLING_GVCF=${SIBLING_ID}.gvcf.gz. # Now use the booleans to choose whether or not you run each deepvariant over these samples. # We always run over proband. ## Proband . singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:7743,availability,echo,echo,7743,"5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband. MOTHER_ID=${Case_ID}_mother. FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_VCF=${MOTHER_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz. FATHER_GVCF=${FATHER_ID}.gvcf.gz. MOTHER_GVCF=${MOTHER_ID}.gvcf.gz. # Run singularity. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA_DIR}"":""/genomedir"" \. 	-B ""${OUTPUT_DIR}"":""/output"" \. 	docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/deeptrio/run_deeptrio \. 	--model_type=$SEQ_TYPE \. 	--ref=""/genomedir/$FASTA_FILE"" \. 	--reads_child=""/bamdir/$PROBAND_BAM"" \. 	--reads_parent1=""/bamdir/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:11915,availability,down,downstream,11915,": ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:.. ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters. Now, this is a **BIG** problem, because downstream tools like Exomiser will treat the output here for variant prioritization in rare disease cases. So take a look at this. Exomiser HTML for DeepVariant, where variant is represented as 1/1. This gene is the top ranked hit for this simulated rare disease case. Good:. ![image](https://user-images.githubusercontent.com/16579982/154753891-458869c2-741b-4728-a74f-f2d58ca7f816.png). Exomiser HTML output for DeepTrio, where variant is represented as 0/1. Notice that now the gene is not even in the top 10 candidates, as Exomiser has interpreted this variant incorrectly (likely assuming it was a PAR region on X). The variant score dropped to 0.00 because the assumption on the GT string was violated:. ![image](https://user-images.githubusercontent.com/16579982/154754163-4833835e-905e-450e-b081-a447340d034c.png). I can send the HTML outputs, VCFs, or even CRAMs if you are interested by email or other file transfer. Anyways, going to try with 1.3 while I wait for an answer here. Cheers,. Phil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:687,deployability,pipelin,pipeline,687,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:713,deployability,Pipelin,Pipeline,713,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:749,deployability,modul,module,749,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:1055,deployability,version,version,1055,"chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6329,deployability,scale,scaled,6329,"true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGUL",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6912,deployability,pipelin,pipeline,6912,": alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6923,deployability,Pipelin,Pipeline,6923,"default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6958,deployability,modul,module,6958,"i_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_pr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:7264,deployability,version,version,7264,": ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband. MOTHER_ID=${Case_ID}_mother. FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_VCF=${MOTHER_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:11204,deployability,scale,scaled,11204,"true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:.. ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters. Now, this is a **BIG** problem, because downstream tools like Exomiser will treat the output here for variant prioritization in rare disease cases. So take a look at this. Exomiser HTML for DeepVariant, where variant is represented as 1/1. This gene is the top ranked hit for this simulated rare disease case. Good:. ![image](https:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:731,energy efficiency,Load,Load,731,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:756,energy efficiency,load,load,756,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:797,energy efficiency,Load,Load,797,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:947,energy efficiency,profil,profile,947,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6329,energy efficiency,scale,scaled,6329,"true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGUL",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6940,energy efficiency,Load,Load,6940,"ero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6965,energy efficiency,load,load,6965,"od: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:7006,energy efficiency,Load,Load,7006,"ig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband. MOTHER_ID=${Case_ID}_mother. FATHER_ID=$",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:7156,energy efficiency,profil,profile,7156,": missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband. MOTHER_ID=${Case_ID}_mother. FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:11204,energy efficiency,scale,scaled,11204,"true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:.. ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters. Now, this is a **BIG** problem, because downstream tools like Exomiser will treat the output here for variant prioritization in rare disease cases. So take a look at this. Exomiser HTML for DeepVariant, where variant is represented as 1/1. This gene is the top ranked hit for this simulated rare disease case. Good:. ![image](https:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:687,integrability,pipelin,pipeline,687,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:713,integrability,Pipelin,Pipeline,713,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:1055,integrability,version,version,1055,"chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:1235,integrability,sub,submission,1235,"is may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_VCF=${MOTHER_ID}.vcf.gz. SIBLING_VCF=${SIBLING_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz. FATHER_GVCF=${FATHER_ID}.gvcf.gz. MOTHER_GVCF=${MOTHER_ID}.gvcf.gz. SIBLING_GVCF=${SIB",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:1335,integrability,Pub,Public,1335,"a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_VCF=${MOTHER_ID}.vcf.gz. SIBLING_VCF=${SIBLING_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz. FATHER_GVCF=${FATHER_ID}.gvcf.gz. MOTHER_GVCF=${MOTHER_ID}.gvcf.gz. SIBLING_GVCF=${SIBLING_ID}.gvcf.gz. # Now use the booleans to choose whether or not you run each deepvariant over th",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:4757,integrability,FILTER,FILTER,4757,"/$MOTHER_VCF"" \. 	 --output_gvcf=""/output/$MOTHER_GVCF"" \. 	 --num_shards=$NSLOTS . fi. # Father. if [ ""$FATHER_PRESENT"" = true ] ; then. 	. 	singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. 		-B ""${BAM_DIR}"":""/bamdir"" \. 		-B ""${FASTA_DIR}"":""/genomedir"" \. 		-B ""${OUTPUT_DIR}"":""/output"" \. 		docker://google/deepvariant:""${BIN_VERSION}"" \. 	 /opt/deepvariant/bin/run_deepvariant \. 	 --model_type=WGS \. 	 --ref=""/genomedir/$FASTA_FILE"" \. 	 --reads=""/bamdir/$FATHER_BAM"" \. 	 --output_vcf=""/output/$FATHER_VCF"" \. 	 --output_gvcf=""/output/$FATHER_GVCF"" \. 	 --num_shards=$NSLOTS . fi. #GLNexus. rm -rf $WORKING_DIR/${FAMILY_ID}_GLNexus.db/. /mnt/common/Precision/GLNexus/glnexus_cli -c DeepVariant${SEQ_TYPE} \. 	-d $WORKING_DIR/${FAMILY_ID}_GLNexus.db/. --threads $NSLOTS \. $WORKING_DIR/*gvcf.gz \. > ${FAMILY_ID}.glnexus.merged.bcf. bcftools view ${FAMILY_ID}.glnexus.merged.bcf | bgzip -c > ${FAMILY_ID}.merged.vcf.gz. ```. ## Variant:. ### VCF header:. ```. ##fileformat=VCFv4.2. ##FILTER=<ID=PASS,Description=""All filters passed"">. ##GLnexusVersion=v1.3.1-0-g0e1c9c9. ##GLnexusConfigName=DeepVariantWGS. ##GLnexusConfigCRC32C=706912162. ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:4790,integrability,filter,filters,4790,"/output/$MOTHER_GVCF"" \. 	 --num_shards=$NSLOTS . fi. # Father. if [ ""$FATHER_PRESENT"" = true ] ; then. 	. 	singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. 		-B ""${BAM_DIR}"":""/bamdir"" \. 		-B ""${FASTA_DIR}"":""/genomedir"" \. 		-B ""${OUTPUT_DIR}"":""/output"" \. 		docker://google/deepvariant:""${BIN_VERSION}"" \. 	 /opt/deepvariant/bin/run_deepvariant \. 	 --model_type=WGS \. 	 --ref=""/genomedir/$FASTA_FILE"" \. 	 --reads=""/bamdir/$FATHER_BAM"" \. 	 --output_vcf=""/output/$FATHER_VCF"" \. 	 --output_gvcf=""/output/$FATHER_GVCF"" \. 	 --num_shards=$NSLOTS . fi. #GLNexus. rm -rf $WORKING_DIR/${FAMILY_ID}_GLNexus.db/. /mnt/common/Precision/GLNexus/glnexus_cli -c DeepVariant${SEQ_TYPE} \. 	-d $WORKING_DIR/${FAMILY_ID}_GLNexus.db/. --threads $NSLOTS \. $WORKING_DIR/*gvcf.gz \. > ${FAMILY_ID}.glnexus.merged.bcf. bcftools view ${FAMILY_ID}.glnexus.merged.bcf | bgzip -c > ${FAMILY_ID}.merged.vcf.gz. ```. ## Variant:. ### VCF header:. ```. ##fileformat=VCFv4.2. ##FILTER=<ID=PASS,Description=""All filters passed"">. ##GLnexusVersion=v1.3.1-0-g0e1c9c9. ##GLnexusConfigName=DeepVariantWGS. ##GLnexusConfigCRC32C=706912162. ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:5615,integrability,filter,filtered,5615," ${FAMILY_ID}.glnexus.merged.bcf | bgzip -c > ${FAMILY_ID}.merged.vcf.gz. ```. ## Variant:. ### VCF header:. ```. ##fileformat=VCFv4.2. ##FILTER=<ID=PASS,Description=""All filters passed"">. ##GLnexusVersion=v1.3.1-0-g0e1c9c9. ##GLnexusConfigName=DeepVariantWGS. ##GLnexusConfigCRC32C=706912162. ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Va",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6663,integrability,FILTER,FILTER,6663,"pe: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## S",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6912,integrability,pipelin,pipeline,6912,": alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6923,integrability,Pipelin,Pipeline,6923,"default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:7264,integrability,version,version,7264,": ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband. MOTHER_ID=${Case_ID}_mother. FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_VCF=${MOTHER_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:7487,integrability,sub,submission,7487,"wVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband. MOTHER_ID=${Case_ID}_mother. FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_VCF=${MOTHER_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz. FATHER_GVCF=${FATHER_ID}.gvcf.gz. MOTHER_GVCF=${MOTHER_ID}.gvcf.gz. # Run singularity. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA_DIR}"":""/genomedir"" \. 	-B ""${OUTPUT_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:7587,integrability,Pub,Public,7587,"15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband. MOTHER_ID=${Case_ID}_mother. FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_VCF=${MOTHER_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz. FATHER_GVCF=${FATHER_ID}.gvcf.gz. MOTHER_GVCF=${MOTHER_ID}.gvcf.gz. # Run singularity. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA_DIR}"":""/genomedir"" \. 	-B ""${OUTPUT_DIR}"":""/output"" \. 	docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. 	/opt/deepvariant/bin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:9632,integrability,FILTER,FILTER,9632,"TYPE \. 	--ref=""/genomedir/$FASTA_FILE"" \. 	--reads_child=""/bamdir/$PROBAND_BAM"" \. 	--reads_parent1=""/bamdir/$FATHER_BAM"" \. 	--reads_parent2=""/bamdir/$MOTHER_BAM"" \. 	--output_vcf_child=""/output/$PROBAND_VCF"" \. 	--output_vcf_parent1=""/output/$FATHER_VCF"" \. 	--output_vcf_parent2=""/output/$MOTHER_VCF"" \. 	--sample_name_child=""${PROBAND_ID}"" \. 	--sample_name_parent1=""${FATHER_ID}"" \. 	--sample_name_parent2=""${MOTHER_ID}"" \. 	--num_shards=$NSLOTS \. 	--intermediate_results_dir=""/output/intermediate_results_dir"" \. 	--output_gvcf_child=""/output/$PROBAND_GVCF"" \. 	--output_gvcf_parent1=""/output/$FATHER_GVCF"" \. 	--output_gvcf_parent2=""/output/$MOTHER_GVCF"" . #GLNexus. /mnt/common/Precision/GLNexus/glnexus_cli -c DeepVariant${SEQ_TYPE} \. $PROBAND_GVCF \. $FATHER_GVCF \. $MOTHER_GVCF \. --threads $NSLOTS \. > ${FAMILY_ID}.glnexus.merged.bcf. bcftools view ${FAMILY_ID}.glnexus.merged.bcf | bgzip -c > ${FAMILY_ID}.merged.vcf.gz. ```. ## Variant. ### Header. ```. ##fileformat=VCFv4.2. ##FILTER=<ID=PASS,Description=""All filters passed"">. ##GLnexusVersion=v1.3.1-0-g0e1c9c9. ##GLnexusConfigName=DeepVariantWGS. ##GLnexusConfigCRC32C=706912162. ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:9665,integrability,filter,filters,9665,"FILE"" \. 	--reads_child=""/bamdir/$PROBAND_BAM"" \. 	--reads_parent1=""/bamdir/$FATHER_BAM"" \. 	--reads_parent2=""/bamdir/$MOTHER_BAM"" \. 	--output_vcf_child=""/output/$PROBAND_VCF"" \. 	--output_vcf_parent1=""/output/$FATHER_VCF"" \. 	--output_vcf_parent2=""/output/$MOTHER_VCF"" \. 	--sample_name_child=""${PROBAND_ID}"" \. 	--sample_name_parent1=""${FATHER_ID}"" \. 	--sample_name_parent2=""${MOTHER_ID}"" \. 	--num_shards=$NSLOTS \. 	--intermediate_results_dir=""/output/intermediate_results_dir"" \. 	--output_gvcf_child=""/output/$PROBAND_GVCF"" \. 	--output_gvcf_parent1=""/output/$FATHER_GVCF"" \. 	--output_gvcf_parent2=""/output/$MOTHER_GVCF"" . #GLNexus. /mnt/common/Precision/GLNexus/glnexus_cli -c DeepVariant${SEQ_TYPE} \. $PROBAND_GVCF \. $FATHER_GVCF \. $MOTHER_GVCF \. --threads $NSLOTS \. > ${FAMILY_ID}.glnexus.merged.bcf. bcftools view ${FAMILY_ID}.glnexus.merged.bcf | bgzip -c > ${FAMILY_ID}.merged.vcf.gz. ```. ## Variant. ### Header. ```. ##fileformat=VCFv4.2. ##FILTER=<ID=PASS,Description=""All filters passed"">. ##GLnexusVersion=v1.3.1-0-g0e1c9c9. ##GLnexusConfigName=DeepVariantWGS. ##GLnexusConfigCRC32C=706912162. ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:10490,integrability,filter,filtered,10490,"s view ${FAMILY_ID}.glnexus.merged.bcf | bgzip -c > ${FAMILY_ID}.merged.vcf.gz. ```. ## Variant. ### Header. ```. ##fileformat=VCFv4.2. ##FILTER=<ID=PASS,Description=""All filters passed"">. ##GLnexusVersion=v1.3.1-0-g0e1c9c9. ##GLnexusConfigName=DeepVariantWGS. ##GLnexusConfigCRC32C=706912162. ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Va",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:11538,integrability,FILTER,FILTER,11538,"pe: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:.. ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters. Now, this is a **BIG** problem, because downstream tools like Exomiser will treat the output here for variant prioritization in rare disease cases. So take a look at this. Exomiser HTML for DeepVariant, where variant is represented as 1/1. This gene is the top ranked hit for this simulated rare disease case. Good:. ![image](https://user-images.githubusercontent.com/16579982/154753891-458869c2-741b-4728-a74f-f2d58ca7f816.png). Exomiser HTML output for DeepTrio, where variant is represented as 0/1. Notice that now the gene is not even in the top 10 candidates, as Exomiser has interpreted this variant incorrectly (likely assuming it was a PAR region on X). The ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:5501,interoperability,FORMAT,FORMAT,5501,"ILY_ID}_GLNexus.db/. --threads $NSLOTS \. $WORKING_DIR/*gvcf.gz \. > ${FAMILY_ID}.glnexus.merged.bcf. bcftools view ${FAMILY_ID}.glnexus.merged.bcf | bgzip -c > ${FAMILY_ID}.merged.vcf.gz. ```. ## Variant:. ### VCF header:. ```. ##fileformat=VCFv4.2. ##FILTER=<ID=PASS,Description=""All filters passed"">. ##GLnexusVersion=v1.3.1-0-g0e1c9c9. ##GLnexusConfigName=DeepVariantWGS. ##GLnexusConfigCRC32C=706912162. ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:5781,interoperability,FORMAT,FORMAT,5781,"=""All filters passed"">. ##GLnexusVersion=v1.3.1-0-g0e1c9c9. ##GLnexusConfigName=DeepVariantWGS. ##GLnexusConfigCRC32C=706912162. ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6050,interoperability,FORMAT,FORMAT,6050,": 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6273,interoperability,FORMAT,FORMAT,6273,"ef_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6675,interoperability,FORMAT,FORMAT,6675," count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:10376,interoperability,FORMAT,FORMAT,10376,"\. $PROBAND_GVCF \. $FATHER_GVCF \. $MOTHER_GVCF \. --threads $NSLOTS \. > ${FAMILY_ID}.glnexus.merged.bcf. bcftools view ${FAMILY_ID}.glnexus.merged.bcf | bgzip -c > ${FAMILY_ID}.merged.vcf.gz. ```. ## Variant. ### Header. ```. ##fileformat=VCFv4.2. ##FILTER=<ID=PASS,Description=""All filters passed"">. ##GLnexusVersion=v1.3.1-0-g0e1c9c9. ##GLnexusConfigName=DeepVariantWGS. ##GLnexusConfigCRC32C=706912162. ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:10656,interoperability,FORMAT,FORMAT,10656,"=""All filters passed"">. ##GLnexusVersion=v1.3.1-0-g0e1c9c9. ##GLnexusConfigName=DeepVariantWGS. ##GLnexusConfigCRC32C=706912162. ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:10925,interoperability,FORMAT,FORMAT,10925,": 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:.. ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters. Now, this is a **BIG** problem, because downstream to",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:11148,interoperability,FORMAT,FORMAT,11148,"ef_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:.. ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters. Now, this is a **BIG** problem, because downstream tools like Exomiser will treat the output here for variant prioritization in rare disease cases. So take a look at this. Exomiser HTML for DeepVariant, where variant is represented as 1/1. This gene is the top ranked hit for ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:11550,interoperability,FORMAT,FORMAT,11550," count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:.. ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters. Now, this is a **BIG** problem, because downstream tools like Exomiser will treat the output here for variant prioritization in rare disease cases. So take a look at this. Exomiser HTML for DeepVariant, where variant is represented as 1/1. This gene is the top ranked hit for this simulated rare disease case. Good:. ![image](https://user-images.githubusercontent.com/16579982/154753891-458869c2-741b-4728-a74f-f2d58ca7f816.png). Exomiser HTML output for DeepTrio, where variant is represented as 0/1. Notice that now the gene is not even in the top 10 candidates, as Exomiser has interpreted this variant incorrectly (likely assuming it was a PAR region on X). The variant scor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:749,modifiability,modul,module,749,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:1055,modifiability,version,version,1055,"chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6329,modifiability,scal,scaled,6329,"true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGUL",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6958,modifiability,modul,module,6958,"i_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_pr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:7264,modifiability,version,version,7264,": ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband. MOTHER_ID=${Case_ID}_mother. FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_VCF=${MOTHER_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:11204,modifiability,scal,scaled,11204,"true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:.. ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters. Now, this is a **BIG** problem, because downstream tools like Exomiser will treat the output here for variant prioritization in rare disease cases. So take a look at this. Exomiser HTML for DeepVariant, where variant is represented as 1/1. This gene is the top ranked hit for this simulated rare disease case. Good:. ![image](https:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:731,performance,Load,Load,731,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:756,performance,load,load,756,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:797,performance,Load,Load,797,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:947,performance,profil,profile,947,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6329,performance,scale,scaled,6329,"true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGUL",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6940,performance,Load,Load,6940,"ero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6965,performance,load,load,6965,"od: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:7006,performance,Load,Load,7006,"ig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband. MOTHER_ID=${Case_ID}_mother. FATHER_ID=$",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:7156,performance,profil,profile,7156,": missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband. MOTHER_ID=${Case_ID}_mother. FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:11204,performance,scale,scaled,11204,"true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:.. ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters. Now, this is a **BIG** problem, because downstream tools like Exomiser will treat the output here for variant prioritization in rare disease cases. So take a look at this. Exomiser HTML for DeepVariant, where variant is represented as 1/1. This gene is the top ranked hit for this simulated rare disease case. Good:. ![image](https:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:749,safety,modul,module,749,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:1390,safety,test,test,1390,"e VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_VCF=${MOTHER_ID}.vcf.gz. SIBLING_VCF=${SIBLING_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz. FATHER_GVCF=${FATHER_ID}.gvcf.gz. MOTHER_GVCF=${MOTHER_ID}.gvcf.gz. SIBLING_GVCF=${SIBLING_ID}.gvcf.gz. # Now use the booleans to choose whether or not you run each deepvariant over these samples. # We always run over proband. ## Proband ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:6958,safety,modul,module,6958,"i_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_pr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:7642,safety,test,test,7642,"OS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband. MOTHER_ID=${Case_ID}_mother. FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_VCF=${MOTHER_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz. FATHER_GVCF=${FATHER_ID}.gvcf.gz. MOTHER_GVCF=${MOTHER_ID}.gvcf.gz. # Run singularity. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA_DIR}"":""/genomedir"" \. 	-B ""${OUTPUT_DIR}"":""/output"" \. 	docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/deeptrio/run_deeptrio \. 	--model_type=$SEQ_TYPE \. 	",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:340,testability,simul,simulated,340,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:1390,testability,test,test,1390,"e VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_VCF=${MOTHER_ID}.vcf.gz. SIBLING_VCF=${SIBLING_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz. FATHER_GVCF=${FATHER_ID}.gvcf.gz. MOTHER_GVCF=${MOTHER_ID}.gvcf.gz. SIBLING_GVCF=${SIBLING_ID}.gvcf.gz. # Now use the booleans to choose whether or not you run each deepvariant over these samples. # We always run over proband. ## Proband ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:7642,testability,test,test,7642,"OS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband. X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:.. ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:. Pipeline. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. export SINGULARITY_CACHEDIR=$PWD. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. Case_ID=Case1. FAMILY_ID=$Case_ID. PROBAND_ID=${Case_ID}_proband. MOTHER_ID=${Case_ID}_mother. FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz. FATHER_VCF=${FATHER_ID}.vcf.gz. MOTHER_VCF=${MOTHER_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz. FATHER_GVCF=${FATHER_ID}.gvcf.gz. MOTHER_GVCF=${MOTHER_ID}.gvcf.gz. # Run singularity. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. 	-B ""${BAM_DIR}"":""/bamdir"" \. 	-B ""${FASTA_DIR}"":""/genomedir"" \. 	-B ""${OUTPUT_DIR}"":""/output"" \. 	docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/deeptrio/run_deeptrio \. 	--model_type=$SEQ_TYPE \. 	",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:12156,testability,simul,simulated,12156," ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:.. ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters. Now, this is a **BIG** problem, because downstream tools like Exomiser will treat the output here for variant prioritization in rare disease cases. So take a look at this. Exomiser HTML for DeepVariant, where variant is represented as 1/1. This gene is the top ranked hit for this simulated rare disease case. Good:. ![image](https://user-images.githubusercontent.com/16579982/154753891-458869c2-741b-4728-a74f-f2d58ca7f816.png). Exomiser HTML output for DeepTrio, where variant is represented as 0/1. Notice that now the gene is not even in the top 10 candidates, as Exomiser has interpreted this variant incorrectly (likely assuming it was a PAR region on X). The variant score dropped to 0.00 because the assumption on the GT string was violated:. ![image](https://user-images.githubusercontent.com/16579982/154754163-4833835e-905e-450e-b081-a447340d034c.png). I can send the HTML outputs, VCFs, or even CRAMs if you are interested by email or other file transfer. Anyways, going to try with 1.3 while I wait for an answer here. Cheers,. Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:125,usability,tool,tool,125,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:512,usability,user,user-images,512,"Issue with DeepTrio (1.1.0) representation of hemizygous X-chromosome variants in male; Hello,. Noticed this issue with your tool DeepTrio regarding the representation of hemizygous variants in the non-pseudoautosomal (PAR) X-chromosome. This may be fixed now in 1.3? If so ignore this, but if not this is what I noticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:. ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:. ```. # Load singularity. module load singularity. BIN_VERSION=""1.1.0"". # Load env for bcftools. ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/. source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh. conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads. NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered). cd $SLURM_SUBMIT_DIR. WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space. mkdir -p $WORKING_DIR. cd $WORKING_DIR. #### GRCh38 #### . echo ""GRCh38 genome"". GENOME=GRCh38. FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/. FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS. BAM_DIR=$WORKING_DIR. FAMILY_ID=Case1. PROBAND_ID=Case1_proband. MOTHER_ID=Case1_mother. FATHER_ID=Case1_father. SIBLING_ID=. PED=$FAMILY_ID.ped. MOTHER_PRESENT=true. FATHER_PRESENT=true. SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam. FATHER_BAM=${FATHER_ID}.sorted.bam. MOTHER_BAM=${MOTHER_ID}.sorted.bam. SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:5101,usability,prefer,preference,5101,"\. 	 /opt/deepvariant/bin/run_deepvariant \. 	 --model_type=WGS \. 	 --ref=""/genomedir/$FASTA_FILE"" \. 	 --reads=""/bamdir/$FATHER_BAM"" \. 	 --output_vcf=""/output/$FATHER_VCF"" \. 	 --output_gvcf=""/output/$FATHER_GVCF"" \. 	 --num_shards=$NSLOTS . fi. #GLNexus. rm -rf $WORKING_DIR/${FAMILY_ID}_GLNexus.db/. /mnt/common/Precision/GLNexus/glnexus_cli -c DeepVariant${SEQ_TYPE} \. 	-d $WORKING_DIR/${FAMILY_ID}_GLNexus.db/. --threads $NSLOTS \. $WORKING_DIR/*gvcf.gz \. > ${FAMILY_ID}.glnexus.merged.bcf. bcftools view ${FAMILY_ID}.glnexus.merged.bcf | bgzip -c > ${FAMILY_ID}.merged.vcf.gz. ```. ## Variant:. ### VCF header:. ```. ##fileformat=VCFv4.2. ##FILTER=<ID=PASS,Description=""All filters passed"">. ##GLnexusVersion=v1.3.1-0-g0e1c9c9. ##GLnexusConfigName=DeepVariantWGS. ##GLnexusConfigCRC32C=706912162. ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genoty",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:9976,usability,prefer,preference,9976,". 	--sample_name_parent1=""${FATHER_ID}"" \. 	--sample_name_parent2=""${MOTHER_ID}"" \. 	--num_shards=$NSLOTS \. 	--intermediate_results_dir=""/output/intermediate_results_dir"" \. 	--output_gvcf_child=""/output/$PROBAND_GVCF"" \. 	--output_gvcf_parent1=""/output/$FATHER_GVCF"" \. 	--output_gvcf_parent2=""/output/$MOTHER_GVCF"" . #GLNexus. /mnt/common/Precision/GLNexus/glnexus_cli -c DeepVariant${SEQ_TYPE} \. $PROBAND_GVCF \. $FATHER_GVCF \. $MOTHER_GVCF \. --threads $NSLOTS \. > ${FAMILY_ID}.glnexus.merged.bcf. bcftools view ${FAMILY_ID}.glnexus.merged.bcf | bgzip -c > ${FAMILY_ID}.merged.vcf.gz. ```. ## Variant. ### Header. ```. ##fileformat=VCFv4.2. ##FILTER=<ID=PASS,Description=""All filters passed"">. ##GLnexusVersion=v1.3.1-0-g0e1c9c9. ##GLnexusConfigName=DeepVariantWGS. ##GLnexusConfigCRC32C=706912162. ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genoty",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:11926,usability,tool,tools,11926," ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:.. ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters. Now, this is a **BIG** problem, because downstream tools like Exomiser will treat the output here for variant prioritization in rare disease cases. So take a look at this. Exomiser HTML for DeepVariant, where variant is represented as 1/1. This gene is the top ranked hit for this simulated rare disease case. Good:. ![image](https://user-images.githubusercontent.com/16579982/154753891-458869c2-741b-4728-a74f-f2d58ca7f816.png). Exomiser HTML output for DeepTrio, where variant is represented as 0/1. Notice that now the gene is not even in the top 10 candidates, as Exomiser has interpreted this variant incorrectly (likely assuming it was a PAR region on X). The variant score dropped to 0.00 because the assumption on the GT string was violated:. ![image](https://user-images.githubusercontent.com/16579982/154754163-4833835e-905e-450e-b081-a447340d034c.png). I can send the HTML outputs, VCFs, or even CRAMs if you are interested by email or other file transfer. Anyways, going to try with 1.3 while I wait for an answer here. Cheers,. Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:12209,usability,user,user-images,12209," ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:.. ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters. Now, this is a **BIG** problem, because downstream tools like Exomiser will treat the output here for variant prioritization in rare disease cases. So take a look at this. Exomiser HTML for DeepVariant, where variant is represented as 1/1. This gene is the top ranked hit for this simulated rare disease case. Good:. ![image](https://user-images.githubusercontent.com/16579982/154753891-458869c2-741b-4728-a74f-f2d58ca7f816.png). Exomiser HTML output for DeepTrio, where variant is represented as 0/1. Notice that now the gene is not even in the top 10 candidates, as Exomiser has interpreted this variant incorrectly (likely assuming it was a PAR region on X). The variant score dropped to 0.00 because the assumption on the GT string was violated:. ![image](https://user-images.githubusercontent.com/16579982/154754163-4833835e-905e-450e-b081-a447340d034c.png). I can send the HTML outputs, VCFs, or even CRAMs if you are interested by email or other file transfer. Anyways, going to try with 1.3 while I wait for an answer here. Cheers,. Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/518:12643,usability,user,user-images,12643," ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}. ##bcftools_viewVersion=1.10.2+htslib-1.10.2. ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022. ```. ### Variant line. ```. #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband. X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:.. ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters. Now, this is a **BIG** problem, because downstream tools like Exomiser will treat the output here for variant prioritization in rare disease cases. So take a look at this. Exomiser HTML for DeepVariant, where variant is represented as 1/1. This gene is the top ranked hit for this simulated rare disease case. Good:. ![image](https://user-images.githubusercontent.com/16579982/154753891-458869c2-741b-4728-a74f-f2d58ca7f816.png). Exomiser HTML output for DeepTrio, where variant is represented as 0/1. Notice that now the gene is not even in the top 10 candidates, as Exomiser has interpreted this variant incorrectly (likely assuming it was a PAR region on X). The variant score dropped to 0.00 because the assumption on the GT string was violated:. ![image](https://user-images.githubusercontent.com/16579982/154754163-4833835e-905e-450e-b081-a447340d034c.png). I can send the HTML outputs, VCFs, or even CRAMs if you are interested by email or other file transfer. Anyways, going to try with 1.3 while I wait for an answer here. Cheers,. Phil.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518
https://github.com/google/deepvariant/issues/519:265,energy efficiency,model,model,265,"Polyploid support?; Hello, I'm just reaching out to see what it would take to add polyploid support to DeepVariant. I see this has been addressed a few times, but want to bring it up again. It looks like utility functions are there, is the only thing remaining the model to be changed, or what else would need to be changed? Any pointers greatly appreciated. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/519
https://github.com/google/deepvariant/issues/519:152,performance,time,times,152,"Polyploid support?; Hello, I'm just reaching out to see what it would take to add polyploid support to DeepVariant. I see this has been addressed a few times, but want to bring it up again. It looks like utility functions are there, is the only thing remaining the model to be changed, or what else would need to be changed? Any pointers greatly appreciated. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/519
https://github.com/google/deepvariant/issues/519:265,security,model,model,265,"Polyploid support?; Hello, I'm just reaching out to see what it would take to add polyploid support to DeepVariant. I see this has been addressed a few times, but want to bring it up again. It looks like utility functions are there, is the only thing remaining the model to be changed, or what else would need to be changed? Any pointers greatly appreciated. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/519
https://github.com/google/deepvariant/issues/519:10,usability,support,support,10,"Polyploid support?; Hello, I'm just reaching out to see what it would take to add polyploid support to DeepVariant. I see this has been addressed a few times, but want to bring it up again. It looks like utility functions are there, is the only thing remaining the model to be changed, or what else would need to be changed? Any pointers greatly appreciated. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/519
https://github.com/google/deepvariant/issues/519:92,usability,support,support,92,"Polyploid support?; Hello, I'm just reaching out to see what it would take to add polyploid support to DeepVariant. I see this has been addressed a few times, but want to bring it up again. It looks like utility functions are there, is the only thing remaining the model to be changed, or what else would need to be changed? Any pointers greatly appreciated. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/519
https://github.com/google/deepvariant/issues/520:1113,availability,Operat,Operating,1113,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1475,availability,Error,Error,1475,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1146,deployability,version,version,1146,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1173,deployability,Instal,Installation,1173,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:132,integrability,event,eventho,132,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1146,integrability,version,version,1146,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1455,integrability,wrap,wrapper,1455,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1455,interoperability,wrapper,wrapper,1455,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1146,modifiability,version,version,1146,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1475,performance,Error,Error,1475,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:981,safety,test,test,981,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1048,safety,test,test,1048,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1475,safety,Error,Error,1475,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:981,testability,test,test,981,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1048,testability,test,test,1048,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1264,testability,instrument,instrument,1264,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1481,testability,trace,trace,1481,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1411,usability,Command,Command,1411,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1475,usability,Error,Error,1475,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/520:1529,usability,user,user-images,1529,"Suspect variant call (variants not collapsed); **Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55. `. Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588. `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0. BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**. - Operating system:. - DeepVariant version: 1.3.0, latest . - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**. - Command: Call variants with run_deepvariant wrapper script. . - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520
https://github.com/google/deepvariant/issues/521:504,availability,down,down,504,"NoCall with missing PL value; Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:. ```. CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990. ```. We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:. ```. CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV. ```. The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here? Thank you for your help,. Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521
https://github.com/google/deepvariant/issues/521:210,deployability,releas,release,210,"NoCall with missing PL value; Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:. ```. CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990. ```. We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:. ```. CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV. ```. The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here? Thank you for your help,. Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521
https://github.com/google/deepvariant/issues/521:256,deployability,pipelin,pipeline,256,"NoCall with missing PL value; Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:. ```. CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990. ```. We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:. ```. CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV. ```. The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here? Thank you for your help,. Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521
https://github.com/google/deepvariant/issues/521:137,energy efficiency,model,models,137,"NoCall with missing PL value; Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:. ```. CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990. ```. We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:. ```. CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV. ```. The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here? Thank you for your help,. Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521
https://github.com/google/deepvariant/issues/521:256,integrability,pipelin,pipeline,256,"NoCall with missing PL value; Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:. ```. CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990. ```. We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:. ```. CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV. ```. The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here? Thank you for your help,. Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521
https://github.com/google/deepvariant/issues/521:436,safety,compl,complains,436,"NoCall with missing PL value; Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:. ```. CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990. ```. We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:. ```. CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV. ```. The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here? Thank you for your help,. Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521
https://github.com/google/deepvariant/issues/521:137,security,model,models,137,"NoCall with missing PL value; Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:. ```. CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990. ```. We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:. ```. CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV. ```. The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here? Thank you for your help,. Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521
https://github.com/google/deepvariant/issues/521:436,security,compl,complains,436,"NoCall with missing PL value; Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:. ```. CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990. ```. We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:. ```. CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV. ```. The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here? Thank you for your help,. Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521
https://github.com/google/deepvariant/issues/521:125,usability,custom,custom,125,"NoCall with missing PL value; Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:. ```. CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990. ```. We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:. ```. CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV. ```. The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here? Thank you for your help,. Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521
https://github.com/google/deepvariant/issues/521:854,usability,indicat,indicates,854,"NoCall with missing PL value; Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:. ```. CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990. ```. We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:. ```. CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV. ```. The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here? Thank you for your help,. Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521
https://github.com/google/deepvariant/issues/521:974,usability,help,help,974,"NoCall with missing PL value; Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:. ```. CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990. ```. We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:. ```. CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV. ```. The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here? Thank you for your help,. Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521
https://github.com/google/deepvariant/issues/522:18,availability,error,error,18,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:244,availability,error,error,244,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:332,availability,Operat,Operating,332,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1894,availability,Error,Error,1894,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1952,availability,error,error,1952,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:2045,availability,ping,pinging,2045,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:2211,availability,error,error,2211,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:375,deployability,version,version,375,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:393,deployability,Instal,Installation,393,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:526,deployability,log,login,526,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:784,deployability,FAIL,FAIL,784,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:963,deployability,modul,module,963,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:977,deployability,modul,module,977,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1002,deployability,modul,module,1002,"larity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1983,deployability,fail,failed,1983,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:2053,deployability,contain,container,2053,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:953,energy efficiency,cpu,cpu,953,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:984,energy efficiency,load,load,984,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1009,energy efficiency,load,load,1009," fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:375,integrability,version,version,375,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:758,integrability,event,events,758,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1942,interoperability,registr,registry,1942,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:2063,interoperability,registr,registry,2063,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:2072,interoperability,registr,registry-,2072,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:2107,interoperability,registr,registry-,2107,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:375,modifiability,version,version,375,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:963,modifiability,modul,module,963,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:977,modifiability,modul,module,977,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1002,modifiability,modul,module,1002,"larity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1840,modifiability,interm,intermediateresults,1840,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:18,performance,error,error,18,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:244,performance,error,error,244,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:894,performance,time,time,894,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:919,performance,time,time,919,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:953,performance,cpu,cpu,953,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:984,performance,load,load,984,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1009,performance,load,load,1009," fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1014,performance,parallel,parallel,1014,"error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker:/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1246,performance,parallel,parallel,1246," our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried al",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1894,performance,Error,Error,1894,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1952,performance,error,error,1952,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1976,performance,cach,cache,1976,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:2170,performance,network,network,2170,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:2211,performance,error,error,2211,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:259,reliability,doe,does,259,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:784,reliability,FAIL,FAIL,784,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1983,reliability,fail,failed,1983,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:18,safety,error,error,18,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:244,safety,error,error,244,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:526,safety,log,login,526,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:963,safety,modul,module,963,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:977,safety,modul,module,977,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1002,safety,modul,module,1002,"larity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1894,safety,Error,Error,1894,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1952,safety,error,error,1952,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:2211,safety,error,error,2211,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:526,security,log,login,526,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1997,security,checksum,checksum,1997,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:2170,security,network,network,2170,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:526,testability,log,login,526,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:18,usability,error,error,18,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:150,usability,clear,clear,150,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:244,usability,error,error,244,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:811,usability,user,user,811,"Singularity fatal error ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1894,usability,Error,Error,1894,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:1952,usability,error,error,1952,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/issues/522:2211,usability,error,error,2211,"k: . **Setup**. - Operating system: Linux HPC. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: WES. **Steps to reproduce:**. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p c_compute_wgp. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-33. #SBATCH --time=02:00:00. #SBATCH --time=072:00:00. #SBATCH --mem-per-cpu=32GB. module purge. module load singularity. module load parallel. set -eu. cd /scratch/c.c21087028/. BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \. --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \. --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \. --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \. --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate"". ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,. Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522
https://github.com/google/deepvariant/pull/523:25,deployability,API,API,25,OpenVINO 2022.1 with new API; We are not taking pull requests at this time.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/523
https://github.com/google/deepvariant/pull/523:25,integrability,API,API,25,OpenVINO 2022.1 with new API; We are not taking pull requests at this time.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/523
https://github.com/google/deepvariant/pull/523:25,interoperability,API,API,25,OpenVINO 2022.1 with new API; We are not taking pull requests at this time.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/523
https://github.com/google/deepvariant/pull/523:70,performance,time,time,70,OpenVINO 2022.1 with new API; We are not taking pull requests at this time.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/523
https://github.com/google/deepvariant/issues/524:107,deployability,contain,container,107,"Deepvariant 1.3 write to TMPDIR even if intermediate file dir is set; Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following. ```. Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx. ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524
https://github.com/google/deepvariant/issues/524:206,deployability,manag,manage,206,"Deepvariant 1.3 write to TMPDIR even if intermediate file dir is set; Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following. ```. Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx. ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524
https://github.com/google/deepvariant/issues/524:605,deployability,automat,automatically,605,"Deepvariant 1.3 write to TMPDIR even if intermediate file dir is set; Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following. ```. Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx. ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524
https://github.com/google/deepvariant/issues/524:907,deployability,pipelin,pipelines,907,"Deepvariant 1.3 write to TMPDIR even if intermediate file dir is set; Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following. ```. Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx. ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524
https://github.com/google/deepvariant/issues/524:206,energy efficiency,manag,manage,206,"Deepvariant 1.3 write to TMPDIR even if intermediate file dir is set; Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following. ```. Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx. ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524
https://github.com/google/deepvariant/issues/524:907,integrability,pipelin,pipelines,907,"Deepvariant 1.3 write to TMPDIR even if intermediate file dir is set; Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following. ```. Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx. ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524
https://github.com/google/deepvariant/issues/524:40,modifiability,interm,intermediate,40,"Deepvariant 1.3 write to TMPDIR even if intermediate file dir is set; Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following. ```. Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx. ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524
https://github.com/google/deepvariant/issues/524:708,modifiability,interm,intermediate,708,"Deepvariant 1.3 write to TMPDIR even if intermediate file dir is set; Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following. ```. Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx. ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524
https://github.com/google/deepvariant/issues/524:206,safety,manag,manage,206,"Deepvariant 1.3 write to TMPDIR even if intermediate file dir is set; Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following. ```. Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx. ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524
https://github.com/google/deepvariant/issues/524:605,testability,automat,automatically,605,"Deepvariant 1.3 write to TMPDIR even if intermediate file dir is set; Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following. ```. Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx. ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524
https://github.com/google/deepvariant/issues/524:167,usability,command,command,167,"Deepvariant 1.3 write to TMPDIR even if intermediate file dir is set; Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following. ```. Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx. ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524
https://github.com/google/deepvariant/issues/524:372,usability,custom,custom,372,"Deepvariant 1.3 write to TMPDIR even if intermediate file dir is set; Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following. ```. Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx. ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524
https://github.com/google/deepvariant/issues/524:1351,usability,command,command,1351,"Deepvariant 1.3 write to TMPDIR even if intermediate file dir is set; Hello,. I'm using DeepVariant docker container v1.3 to call variants using the `run_deepvariant` command. What I've done in the past to manage temp files was to create a `temp_dir` in the working directory and then use `--intermediate_results_dir temp_dir` to make DeepVariant write temp files in this custom location. However, the same approach is not working anymore for me in the new HPC system since on computing node the default temp folder stored in `$TMPDIR` is set to a special space `\localscratch` that is not among the path automatically mounted by Docker or Singularity (like \tmp) apparently. I realized that, in addition to intermediate files written to `--intermediate_results_dir`, DeepVariant writes some additional temp files to the default temp dir location (`$TMPDIR`) and this created some issues when running it in pipelines (like Nextflow). . I've created a work around by manually setting `$TMPDIR` in the sh script so that it points to another folder in the work directory, and I can see there are a bunch of small files created in there (~30Mb total) like the following. ```. Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx. ```. I wonder which kind of files are written to `$TMPDIR` and if it's possible to redirect them by command line option without having to set `$TMPDIR`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524
https://github.com/google/deepvariant/issues/525:195,availability,Operat,Operating,195,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:445,availability,Error,Error,445,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:228,deployability,version,version,228,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:240,deployability,Instal,Installation,240,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:228,integrability,version,version,228,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:228,modifiability,version,version,228,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:445,performance,Error,Error,445,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:477,reliability,Doe,Does,477,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:445,safety,Error,Error,445,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:498,safety,test,test,498,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:534,safety,test,test,534,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:324,testability,instrument,instrument,324,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:451,testability,trace,trace,451,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:498,testability,test,test,498,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:534,testability,test,test,534,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:709,testability,context,context,709,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:128,usability,clear,clear,128,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:433,usability,Command,Command,433,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/525:445,usability,Error,Error,445,"Hello,; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/525
https://github.com/google/deepvariant/issues/526:542,deployability,resourc,resources,542,"Running DeepVariant on large sample size; Hello,. I have tried following along the guidelines presented under [Best practices for multi-sample variant calling](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration) but am still unclear as to how to run DeepVariant on a large sample size (~200). The example provided only uses a trio and the paper cited doesn't provide any code for their large cohort experiment. Do you have any resources for adjusting the parameters/running the samples in parallel so as to not run all ~200 samples sequentially? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526
https://github.com/google/deepvariant/issues/526:542,energy efficiency,resourc,resources,542,"Running DeepVariant on large sample size; Hello,. I have tried following along the guidelines presented under [Best practices for multi-sample variant calling](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration) but am still unclear as to how to run DeepVariant on a large sample size (~200). The example provided only uses a trio and the paper cited doesn't provide any code for their large cohort experiment. Do you have any resources for adjusting the parameters/running the samples in parallel so as to not run all ~200 samples sequentially? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526
https://github.com/google/deepvariant/issues/526:570,modifiability,paramet,parameters,570,"Running DeepVariant on large sample size; Hello,. I have tried following along the guidelines presented under [Best practices for multi-sample variant calling](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration) but am still unclear as to how to run DeepVariant on a large sample size (~200). The example provided only uses a trio and the paper cited doesn't provide any code for their large cohort experiment. Do you have any resources for adjusting the parameters/running the samples in parallel so as to not run all ~200 samples sequentially? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526
https://github.com/google/deepvariant/issues/526:542,performance,resourc,resources,542,"Running DeepVariant on large sample size; Hello,. I have tried following along the guidelines presented under [Best practices for multi-sample variant calling](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration) but am still unclear as to how to run DeepVariant on a large sample size (~200). The example provided only uses a trio and the paper cited doesn't provide any code for their large cohort experiment. Do you have any resources for adjusting the parameters/running the samples in parallel so as to not run all ~200 samples sequentially? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526
https://github.com/google/deepvariant/issues/526:604,performance,parallel,parallel,604,"Running DeepVariant on large sample size; Hello,. I have tried following along the guidelines presented under [Best practices for multi-sample variant calling](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration) but am still unclear as to how to run DeepVariant on a large sample size (~200). The example provided only uses a trio and the paper cited doesn't provide any code for their large cohort experiment. Do you have any resources for adjusting the parameters/running the samples in parallel so as to not run all ~200 samples sequentially? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526
https://github.com/google/deepvariant/issues/526:116,reliability,pra,practices,116,"Running DeepVariant on large sample size; Hello,. I have tried following along the guidelines presented under [Best practices for multi-sample variant calling](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration) but am still unclear as to how to run DeepVariant on a large sample size (~200). The example provided only uses a trio and the paper cited doesn't provide any code for their large cohort experiment. Do you have any resources for adjusting the parameters/running the samples in parallel so as to not run all ~200 samples sequentially? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526
https://github.com/google/deepvariant/issues/526:243,reliability,pra,practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration,243,"Running DeepVariant on large sample size; Hello,. I have tried following along the guidelines presented under [Best practices for multi-sample variant calling](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration) but am still unclear as to how to run DeepVariant on a large sample size (~200). The example provided only uses a trio and the paper cited doesn't provide any code for their large cohort experiment. Do you have any resources for adjusting the parameters/running the samples in parallel so as to not run all ~200 samples sequentially? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526
https://github.com/google/deepvariant/issues/526:466,reliability,doe,doesn,466,"Running DeepVariant on large sample size; Hello,. I have tried following along the guidelines presented under [Best practices for multi-sample variant calling](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration) but am still unclear as to how to run DeepVariant on a large sample size (~200). The example provided only uses a trio and the paper cited doesn't provide any code for their large cohort experiment. Do you have any resources for adjusting the parameters/running the samples in parallel so as to not run all ~200 samples sequentially? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526
https://github.com/google/deepvariant/issues/526:542,safety,resourc,resources,542,"Running DeepVariant on large sample size; Hello,. I have tried following along the guidelines presented under [Best practices for multi-sample variant calling](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration) but am still unclear as to how to run DeepVariant on a large sample size (~200). The example provided only uses a trio and the paper cited doesn't provide any code for their large cohort experiment. Do you have any resources for adjusting the parameters/running the samples in parallel so as to not run all ~200 samples sequentially? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526
https://github.com/google/deepvariant/issues/526:542,testability,resourc,resources,542,"Running DeepVariant on large sample size; Hello,. I have tried following along the guidelines presented under [Best practices for multi-sample variant calling](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration) but am still unclear as to how to run DeepVariant on a large sample size (~200). The example provided only uses a trio and the paper cited doesn't provide any code for their large cohort experiment. Do you have any resources for adjusting the parameters/running the samples in parallel so as to not run all ~200 samples sequentially? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526
https://github.com/google/deepvariant/issues/526:83,usability,guid,guidelines,83,"Running DeepVariant on large sample size; Hello,. I have tried following along the guidelines presented under [Best practices for multi-sample variant calling](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration) but am still unclear as to how to run DeepVariant on a large sample size (~200). The example provided only uses a trio and the paper cited doesn't provide any code for their large cohort experiment. Do you have any resources for adjusting the parameters/running the samples in parallel so as to not run all ~200 samples sequentially? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526
https://github.com/google/deepvariant/issues/528:0,modifiability,paramet,parameters,0,"parameters setting for somatic/germline variant calling; Dear team, may I know if there is any different parameters setting between somatic variant calling and germline variant calling? The reason why I posted this question is that I found one heterozygous variant called by deepvariant but seems homozygous supported in IGV. I'm wondering if my setting of deepvariant is too loose for this variant? (ps, I just run deepvariant by default pacbio data setting) . Here is the result in VCF. The VAF is quite high, and`1,20` AD means only 1 read supported the widetype read? . chr4 3079267 . G T 36.1 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:21:1,20:0.952381:33,0,1. Here is the screenshot of IGV. <img width=""1212"" alt=""image"" src=""https://user-images.githubusercontent.com/44595075/158147856-1ff7a5c8-d68f-4711-8e73-0c0806089783.png"">. Thanks!!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528
https://github.com/google/deepvariant/issues/528:105,modifiability,paramet,parameters,105,"parameters setting for somatic/germline variant calling; Dear team, may I know if there is any different parameters setting between somatic variant calling and germline variant calling? The reason why I posted this question is that I found one heterozygous variant called by deepvariant but seems homozygous supported in IGV. I'm wondering if my setting of deepvariant is too loose for this variant? (ps, I just run deepvariant by default pacbio data setting) . Here is the result in VCF. The VAF is quite high, and`1,20` AD means only 1 read supported the widetype read? . chr4 3079267 . G T 36.1 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:21:1,20:0.952381:33,0,1. Here is the screenshot of IGV. <img width=""1212"" alt=""image"" src=""https://user-images.githubusercontent.com/44595075/158147856-1ff7a5c8-d68f-4711-8e73-0c0806089783.png"">. Thanks!!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528
https://github.com/google/deepvariant/issues/528:439,modifiability,pac,pacbio,439,"parameters setting for somatic/germline variant calling; Dear team, may I know if there is any different parameters setting between somatic variant calling and germline variant calling? The reason why I posted this question is that I found one heterozygous variant called by deepvariant but seems homozygous supported in IGV. I'm wondering if my setting of deepvariant is too loose for this variant? (ps, I just run deepvariant by default pacbio data setting) . Here is the result in VCF. The VAF is quite high, and`1,20` AD means only 1 read supported the widetype read? . chr4 3079267 . G T 36.1 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:21:1,20:0.952381:33,0,1. Here is the screenshot of IGV. <img width=""1212"" alt=""image"" src=""https://user-images.githubusercontent.com/44595075/158147856-1ff7a5c8-d68f-4711-8e73-0c0806089783.png"">. Thanks!!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528
https://github.com/google/deepvariant/issues/528:62,security,team,team,62,"parameters setting for somatic/germline variant calling; Dear team, may I know if there is any different parameters setting between somatic variant calling and germline variant calling? The reason why I posted this question is that I found one heterozygous variant called by deepvariant but seems homozygous supported in IGV. I'm wondering if my setting of deepvariant is too loose for this variant? (ps, I just run deepvariant by default pacbio data setting) . Here is the result in VCF. The VAF is quite high, and`1,20` AD means only 1 read supported the widetype read? . chr4 3079267 . G T 36.1 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:21:1,20:0.952381:33,0,1. Here is the screenshot of IGV. <img width=""1212"" alt=""image"" src=""https://user-images.githubusercontent.com/44595075/158147856-1ff7a5c8-d68f-4711-8e73-0c0806089783.png"">. Thanks!!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528
https://github.com/google/deepvariant/issues/528:308,usability,support,supported,308,"parameters setting for somatic/germline variant calling; Dear team, may I know if there is any different parameters setting between somatic variant calling and germline variant calling? The reason why I posted this question is that I found one heterozygous variant called by deepvariant but seems homozygous supported in IGV. I'm wondering if my setting of deepvariant is too loose for this variant? (ps, I just run deepvariant by default pacbio data setting) . Here is the result in VCF. The VAF is quite high, and`1,20` AD means only 1 read supported the widetype read? . chr4 3079267 . G T 36.1 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:21:1,20:0.952381:33,0,1. Here is the screenshot of IGV. <img width=""1212"" alt=""image"" src=""https://user-images.githubusercontent.com/44595075/158147856-1ff7a5c8-d68f-4711-8e73-0c0806089783.png"">. Thanks!!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528
https://github.com/google/deepvariant/issues/528:543,usability,support,supported,543,"parameters setting for somatic/germline variant calling; Dear team, may I know if there is any different parameters setting between somatic variant calling and germline variant calling? The reason why I posted this question is that I found one heterozygous variant called by deepvariant but seems homozygous supported in IGV. I'm wondering if my setting of deepvariant is too loose for this variant? (ps, I just run deepvariant by default pacbio data setting) . Here is the result in VCF. The VAF is quite high, and`1,20` AD means only 1 read supported the widetype read? . chr4 3079267 . G T 36.1 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:21:1,20:0.952381:33,0,1. Here is the screenshot of IGV. <img width=""1212"" alt=""image"" src=""https://user-images.githubusercontent.com/44595075/158147856-1ff7a5c8-d68f-4711-8e73-0c0806089783.png"">. Thanks!!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528
https://github.com/google/deepvariant/issues/528:729,usability,user,user-images,729,"parameters setting for somatic/germline variant calling; Dear team, may I know if there is any different parameters setting between somatic variant calling and germline variant calling? The reason why I posted this question is that I found one heterozygous variant called by deepvariant but seems homozygous supported in IGV. I'm wondering if my setting of deepvariant is too loose for this variant? (ps, I just run deepvariant by default pacbio data setting) . Here is the result in VCF. The VAF is quite high, and`1,20` AD means only 1 read supported the widetype read? . chr4 3079267 . G T 36.1 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:21:1,20:0.952381:33,0,1. Here is the screenshot of IGV. <img width=""1212"" alt=""image"" src=""https://user-images.githubusercontent.com/44595075/158147856-1ff7a5c8-d68f-4711-8e73-0c0806089783.png"">. Thanks!!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528
https://github.com/google/deepvariant/issues/529:244,availability,Operat,Operating,244,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:294,deployability,version,version,294,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:316,deployability,Instal,Installation,316,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:1120,deployability,log,log,1120,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:294,integrability,version,version,294,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:887,interoperability,specif,specific,887,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:294,modifiability,version,version,294,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:1120,safety,log,log,1120,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:1120,security,log,log,1120,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:422,testability,instrument,instrument,422,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:1120,testability,log,log,1120,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:33,usability,support,supporting,33,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:177,usability,clear,clear,177,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:569,usability,support,supporting,569,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:761,usability,support,supporting,761,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:923,usability,support,supporting,923,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:1196,usability,support,supporting,1196,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/529:1358,usability,help,help,1358,"Inconsistent number of reference supporting reads.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. YES. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: ubuntu **16.04**. - DeepVariant version: **1.1.0**. - Installation method (Docker, built from source, etc.): **built from source**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than **AD** did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/529
https://github.com/google/deepvariant/issues/530:48,availability,error,errors,48,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:197,availability,error,error,197,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:915,availability,servic,service,915,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:607,deployability,contain,container,607,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:915,deployability,servic,service,915,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:2462,deployability,stack,stack,2462,"TTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:2750,deployability,modul,module,2750,"${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:915,integrability,servic,service,915,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:438,interoperability,share,shared,438,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:836,interoperability,share,share,836,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:939,interoperability,share,share,939,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:964,interoperability,share,share,964,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:2592,interoperability,share,share,2592,"get -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:3693,interoperability,share,share,3693,"""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**. The quick test works on my system as long as my data is not in the /mnt/ folder. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
