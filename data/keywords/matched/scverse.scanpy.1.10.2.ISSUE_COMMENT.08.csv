id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/558:448,modifiability,coupl,couple,448,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:505,modifiability,coupl,couple,505,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:831,modifiability,variab,variable,831,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1076,modifiability,pac,packages,1076,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1374,modifiability,pac,packages,1374,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1399,modifiability,pac,packages,1399,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:272,performance,error,error,272,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:757,performance,Disk,DiskQuotaPolicy,757,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:933,performance,Cach,Caches,933,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1301,performance,cach,cache,1301,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1327,performance,cach,caching,1327,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:985,reliability,Sla,Slack,985,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1037,reliability,availab,available,1037,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:272,safety,error,error,272,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:851,safety,log,log,851,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1037,safety,avail,available,1037,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:851,security,log,log,851,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:897,security,model,model,897,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1037,security,availab,available,1037,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:448,testability,coupl,couple,448,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:505,testability,coupl,couple,505,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:851,testability,log,log,851,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:38,usability,user,user,38,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:146,usability,user,user,146,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:272,usability,error,error,272,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:534,usability,clear,clear,534,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:615,usability,behavi,behavior,615,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:624,usability,intuit,intuitive,624,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:708,usability,Help,Help,708,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:725,usability,User,User,725,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:730,usability,Guid,Guide,730,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1108,usability,learn,learn,1108,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download? @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`. * `seaborn` – `~/seaborn-data`. * `NLTK` – `~/nltk_data`. * `keras` and `tensorflow` – `~/.keras/datasets`. * `conda` – `~/miniconda3/`. * `intake` – `~/.intake/cache/` (specifically for caching feature). * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:324,availability,down,download,324,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:387,availability,down,download,387,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1162,availability,down,downloaded,1162,"nsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to conf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1371,availability,avail,available,1371,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:940,deployability,instal,installing,940,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1550,deployability,contain,contains,1550,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1731,deployability,instal,installs,1731,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1777,deployability,instal,installs,1777,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2226,energy efficiency,core,core,2226,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:626,integrability,configur,configured,626,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:896,integrability,coupl,couple,896,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:953,integrability,coupl,couple,953,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2163,integrability,configur,configure,2163,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:747,interoperability,distribut,distribution,747,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1671,interoperability,share,share,1671,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:115,modifiability,responsibil,responsibility,115,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:162,modifiability,responsibil,responsibility,162,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:626,modifiability,configur,configured,626,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:896,modifiability,coupl,couple,896,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:953,modifiability,coupl,couple,953,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1410,modifiability,pac,packages,1410,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2163,modifiability,configur,configure,2163,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:429,performance,cach,cached,429,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:449,performance,cach,cache,449,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:510,performance,cach,cache,510,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:795,performance,disk,disk,795,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1287,performance,cach,cache,1287,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1316,performance,cach,cache,1316,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1817,performance,cach,cache,1817,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1371,reliability,availab,available,1371,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1371,safety,avail,available,1371,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:626,security,configur,configured,626,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1371,security,availab,available,1371,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2163,security,configur,configure,2163,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:896,testability,coupl,couple,896,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:953,testability,coupl,couple,953,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:108,usability,user,user,108,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:216,usability,user,user,216,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:484,usability,help,help,484,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:836,usability,clear,clear,836,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:982,usability,clear,clear,982,"> Some pip wheel files are there for example. And scipy is also some 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My perso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1063,usability,behavi,behavior,1063,"e 100 MB right? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications pu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1072,usability,intuit,intuitive,1072,"ight? > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1772,usability,user,user,1772,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1926,usability,help,helps,1926,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1936,usability,user,user,1936,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1995,usability,person,personal,1995,"actly my stance as well. > How about printing the absolute path of the data's destination on download? I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right? Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature! > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data. - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:562,deployability,API,API,562,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console. $ scanpy settings. Config file: ~/.config/scanpy/scanpy.toml. cachedir='~/.cache/scanpy' (default). ... $ scanpy settings cachedir '/my/path'. Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml. $ scanpy settings cachedir. /my/path. ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:39,integrability,interfac,interfaces,39,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console. $ scanpy settings. Config file: ~/.config/scanpy/scanpy.toml. cachedir='~/.cache/scanpy' (default). ... $ scanpy settings cachedir '/my/path'. Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml. $ scanpy settings cachedir. /my/path. ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:562,integrability,API,API,562,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console. $ scanpy settings. Config file: ~/.config/scanpy/scanpy.toml. cachedir='~/.cache/scanpy' (default). ... $ scanpy settings cachedir '/my/path'. Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml. $ scanpy settings cachedir. /my/path. ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:39,interoperability,interfac,interfaces,39,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console. $ scanpy settings. Config file: ~/.config/scanpy/scanpy.toml. cachedir='~/.cache/scanpy' (default). ... $ scanpy settings cachedir '/my/path'. Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml. $ scanpy settings cachedir. /my/path. ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:562,interoperability,API,API,562,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console. $ scanpy settings. Config file: ~/.config/scanpy/scanpy.toml. cachedir='~/.cache/scanpy' (default). ... $ scanpy settings cachedir '/my/path'. Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml. $ scanpy settings cachedir. /my/path. ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:39,modifiability,interfac,interfaces,39,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console. $ scanpy settings. Config file: ~/.config/scanpy/scanpy.toml. cachedir='~/.cache/scanpy' (default). ... $ scanpy settings cachedir '/my/path'. Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml. $ scanpy settings cachedir. /my/path. ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:342,performance,cach,cachedir,342,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console. $ scanpy settings. Config file: ~/.config/scanpy/scanpy.toml. cachedir='~/.cache/scanpy' (default). ... $ scanpy settings cachedir '/my/path'. Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml. $ scanpy settings cachedir. /my/path. ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:355,performance,cach,cache,355,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console. $ scanpy settings. Config file: ~/.config/scanpy/scanpy.toml. cachedir='~/.cache/scanpy' (default). ... $ scanpy settings cachedir '/my/path'. Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml. $ scanpy settings cachedir. /my/path. ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:402,performance,cach,cachedir,402,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console. $ scanpy settings. Config file: ~/.config/scanpy/scanpy.toml. cachedir='~/.cache/scanpy' (default). ... $ scanpy settings cachedir '/my/path'. Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml. $ scanpy settings cachedir. /my/path. ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:427,performance,cach,cachedir,427,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console. $ scanpy settings. Config file: ~/.config/scanpy/scanpy.toml. cachedir='~/.cache/scanpy' (default). ... $ scanpy settings cachedir '/my/path'. Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml. $ scanpy settings cachedir. /my/path. ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:501,performance,cach,cachedir,501,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console. $ scanpy settings. Config file: ~/.config/scanpy/scanpy.toml. cachedir='~/.cache/scanpy' (default). ... $ scanpy settings cachedir '/my/path'. Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml. $ scanpy settings cachedir. /my/path. ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:26,usability,command,command,26,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console. $ scanpy settings. Config file: ~/.config/scanpy/scanpy.toml. cachedir='~/.cache/scanpy' (default). ... $ scanpy settings cachedir '/my/path'. Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml. $ scanpy settings cachedir. /my/path. ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:605,usability,interact,interactively,605,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console. $ scanpy settings. Config file: ~/.config/scanpy/scanpy.toml. cachedir='~/.cache/scanpy' (default). ... $ scanpy settings cachedir '/my/path'. Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml. $ scanpy settings cachedir. /my/path. ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:104,availability,down,download,104,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:201,availability,down,download,201,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:781,availability,down,downloading,781,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:861,availability,error,error,861,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:502,deployability,modul,module,502,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:699,deployability,manag,manages,699,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:332,energy efficiency,load,loaded,332,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:699,energy efficiency,manag,manages,699,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:159,integrability,configur,configure,159,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:427,integrability,configur,configured,427,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:470,integrability,event,eventually,470,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:924,integrability,configur,configured,924,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:159,modifiability,configur,configure,159,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:427,modifiability,configur,configured,427,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:502,modifiability,modul,module,502,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:924,modifiability,configur,configured,924,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:173,performance,cach,cache,173,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:318,performance,time,time,318,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:332,performance,load,loaded,332,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:372,performance,cach,cache,372,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:861,performance,error,error,861,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1030,performance,cach,cachedir,1030,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1065,performance,cach,cachedir,1065,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:502,safety,modul,module,502,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:699,safety,manag,manages,699,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:861,safety,error,error,861,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:159,security,configur,configure,159,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:427,security,configur,configured,427,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:924,security,configur,configured,924,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:7,usability,user,user,7,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:12,usability,experien,experience,12,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:91,usability,learn,learn,91,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:523,usability,document,documentation,523,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:652,usability,user,user,652,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:861,usability,error,error,861,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:133,availability,down,download,133,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:625,availability,down,downloaded,625,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:957,availability,down,downloaded,957,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1032,availability,down,downloaded,1032,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:265,deployability,log,logging,265,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:413,deployability,contain,contains,413,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:446,energy efficiency,model,model,446,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1140,integrability,interfac,interfaces,1140,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1287,integrability,interfac,interface,1287,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1140,interoperability,interfac,interfaces,1140,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1287,interoperability,interfac,interface,1287,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1140,modifiability,interfac,interfaces,1140,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1287,modifiability,interfac,interface,1287,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:67,performance,cach,cache,67,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:175,performance,cach,cached,175,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:195,performance,cach,cache,195,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:331,performance,disk,disk,331,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:901,performance,disk,disk,901,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:983,performance,cach,cached,983,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1107,performance,disk,disk,1107,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:265,safety,log,logging,265,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:833,safety,test,test,833,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:265,security,log,logging,265,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:446,security,model,model,446,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:265,testability,log,logging,265,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:833,testability,test,test,833,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:234,usability,prefer,prefer,234,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:888,usability,clear,cleared,888,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1127,usability,command,command,1127,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1264,usability,support,support,1264,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1274,usability,command,command,1274,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1386,usability,tool,tool,1386,"> And scipy is also some 100 MB right? Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:59,availability,down,downloader,59,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:791,availability,state,state,791,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:168,deployability,configurat,configuration,168,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:277,deployability,configurat,configuration,277,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:548,deployability,manag,managers,548,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:692,deployability,configurat,configuration,692,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:864,deployability,manag,manager,864,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:920,deployability,log,logging,920,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:548,energy efficiency,manag,managers,548,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:864,energy efficiency,manag,manager,864,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:168,integrability,configur,configuration,168,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:277,integrability,configur,configuration,277,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:692,integrability,configur,configuration,692,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:791,integrability,state,state,791,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:168,modifiability,configur,configuration,168,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:277,modifiability,configur,configuration,277,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:692,modifiability,configur,configuration,692,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1078,modifiability,variab,variable,1078,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:147,performance,time,time,147,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:351,safety,compl,complicated,351,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:548,safety,manag,managers,548,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:565,safety,test,testing,565,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:864,safety,manag,manager,864,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:920,safety,log,logging,920,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:168,security,configur,configuration,168,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:277,security,configur,configuration,277,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:351,security,compl,complicated,351,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:692,security,configur,configuration,692,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:920,security,log,logging,920,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1056,security,session,session,1056,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:540,testability,context,context,540,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:565,testability,test,testing,565,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:662,testability,plan,planning,662,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:920,testability,log,logging,920,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:594,usability,document,documentation,594,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally. * What's the appropriate way to set logging level? It seems to keep changing and breaking things. * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:93,availability,down,down,93,"@flying-sheep that user experience seems pretty reasonable. I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for `expression_atlas` would have a reference to `dataset_dir`? Also on point 4, I've definitely had conda exit with helpful errors when I ran out of space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:360,availability,error,errors,360,"@flying-sheep that user experience seems pretty reasonable. I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for `expression_atlas` would have a reference to `dataset_dir`? Also on point 4, I've definitely had conda exit with helpful errors when I ran out of space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:360,performance,error,errors,360,"@flying-sheep that user experience seems pretty reasonable. I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for `expression_atlas` would have a reference to `dataset_dir`? Also on point 4, I've definitely had conda exit with helpful errors when I ran out of space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:360,safety,error,errors,360,"@flying-sheep that user experience seems pretty reasonable. I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for `expression_atlas` would have a reference to `dataset_dir`? Also on point 4, I've definitely had conda exit with helpful errors when I ran out of space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:200,security,access,access,200,"@flying-sheep that user experience seems pretty reasonable. I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for `expression_atlas` would have a reference to `dataset_dir`? Also on point 4, I've definitely had conda exit with helpful errors when I ran out of space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:19,usability,user,user,19,"@flying-sheep that user experience seems pretty reasonable. I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for `expression_atlas` would have a reference to `dataset_dir`? Also on point 4, I've definitely had conda exit with helpful errors when I ran out of space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:24,usability,experien,experience,24,"@flying-sheep that user experience seems pretty reasonable. I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for `expression_atlas` would have a reference to `dataset_dir`? Also on point 4, I've definitely had conda exit with helpful errors when I ran out of space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:352,usability,help,helpful,352,"@flying-sheep that user experience seems pretty reasonable. I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for `expression_atlas` would have a reference to `dataset_dir`? Also on point 4, I've definitely had conda exit with helpful errors when I ran out of space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:360,usability,error,errors,360,"@flying-sheep that user experience seems pretty reasonable. I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for `expression_atlas` would have a reference to `dataset_dir`? Also on point 4, I've definitely had conda exit with helpful errors when I ran out of space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1069,availability,down,downloaded,1069," terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scrip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1144,availability,down,downloaded,1144,"s up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1401,availability,down,download,1401,"k rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1583,availability,backup,backupped,1583," if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant set",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2508,availability,down,down,2508,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:336,deployability,log,logging,336,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:523,deployability,log,log,523,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:600,deployability,log,log,600,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:885,deployability,manag,manager,885,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1252,deployability,automat,automatically,1252,"nyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementati",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2205,deployability,configurat,configuration,2205,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2416,deployability,configurat,configuration,2416,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:622,energy efficiency,model,model,622,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:885,energy efficiency,manag,manager,885,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:485,integrability,discover,discover,485,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1456,integrability,configur,configured,1456," time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1752,integrability,interfac,interface,1752,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2205,integrability,configur,configuration,2205,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2416,integrability,configur,configuration,2416,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:485,interoperability,discover,discover,485,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:942,interoperability,share,share,942,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1752,interoperability,interfac,interface,1752,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:877,modifiability,pac,package-manager,877,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1456,modifiability,configur,configured,1456," time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1752,modifiability,interfac,interface,1752,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2205,modifiability,configur,configuration,2205,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2416,modifiability,configur,configuration,2416,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:29,performance,cach,cache,29,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:116,performance,cach,cached,116,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:160,performance,cach,cache,160,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:245,performance,time,time,245,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:253,performance,time,time,253,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:402,performance,disk,disk,402,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:462,performance,time,time,462,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:750,performance,cach,cache,750,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:767,performance,cach,cache,767,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:923,performance,cach,cache,923,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1013,performance,disk,disk,1013,"ally under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1095,performance,cach,cached,1095,"t is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1219,performance,disk,disk,1219,"ean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1422,performance,cach,cache,1422,"g. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configura",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1548,performance,disk,disk,1548,"ight? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2110,performance,time,time,2110,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1237,reliability,doe,doesn,1237,"from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realiz",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1583,reliability,backup,backupped,1583," if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant set",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:336,safety,log,logging,336,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:523,safety,log,log,523,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:600,safety,log,log,600,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:885,safety,manag,manager,885,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1583,safety,backup,backupped,1583," if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant set",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1989,safety,except,except,1989,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2279,safety,compl,complicated,2279,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:336,security,log,logging,336,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:523,security,log,log,523,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:600,security,log,log,600,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:622,security,model,model,622,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1456,security,configur,configured,1456," time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2205,security,configur,configuration,2205,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2279,security,compl,complicated,2279,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2416,security,configur,configuration,2416,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2615,security,access,access,2615,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:336,testability,log,logging,336,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:523,testability,log,log,523,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:600,testability,log,log,600,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1252,testability,automat,automatically,1252,"nyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementati",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2386,testability,plan,planning,2386,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:305,usability,prefer,prefer,305,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:473,usability,help,help,473,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:485,usability,discov,discover,485,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1000,usability,clear,cleared,1000,"ipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1352,usability,help,help,1352,"'d put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it? > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1729,usability,support,support,1729,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1739,usability,command,command,1739,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:1851,usability,tool,tool,1851,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:2013,usability,command,commands,2013,"nstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right? The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir? sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:381,integrability,topic,topics,381,"These and similar discussions tend to get very long and complicated! 🙂 . So, excuse if I'm not addressing everything; it's probably even not on purpose. I wrote my response here (https://github.com/theislab/scanpy/pull/573) so that it directly includes the use case of the expression atlas. I'm closing this issue. We can follow up on PR-related things in the PR. And on all other topics in issues dedicated to these topics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:417,integrability,topic,topics,417,"These and similar discussions tend to get very long and complicated! 🙂 . So, excuse if I'm not addressing everything; it's probably even not on purpose. I wrote my response here (https://github.com/theislab/scanpy/pull/573) so that it directly includes the use case of the expression atlas. I'm closing this issue. We can follow up on PR-related things in the PR. And on all other topics in issues dedicated to these topics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:56,safety,compl,complicated,56,"These and similar discussions tend to get very long and complicated! 🙂 . So, excuse if I'm not addressing everything; it's probably even not on purpose. I wrote my response here (https://github.com/theislab/scanpy/pull/573) so that it directly includes the use case of the expression atlas. I'm closing this issue. We can follow up on PR-related things in the PR. And on all other topics in issues dedicated to these topics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:56,security,compl,complicated,56,"These and similar discussions tend to get very long and complicated! 🙂 . So, excuse if I'm not addressing everything; it's probably even not on purpose. I wrote my response here (https://github.com/theislab/scanpy/pull/573) so that it directly includes the use case of the expression atlas. I'm closing this issue. We can follow up on PR-related things in the PR. And on all other topics in issues dedicated to these topics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:17,deployability,continu,continue,17,"So either we now continue the discussion in that completely unrelated PR or just accept whatever @ivirshup favors at the moment without discussion. This is the issue for exactly that discussion, and before we agree on something, #573 can’t change how things work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:49,safety,compl,completely,49,"So either we now continue the discussion in that completely unrelated PR or just accept whatever @ivirshup favors at the moment without discussion. This is the issue for exactly that discussion, and before we agree on something, #573 can’t change how things work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:49,security,compl,completely,49,"So either we now continue the discussion in that completely unrelated PR or just accept whatever @ivirshup favors at the moment without discussion. This is the issue for exactly that discussion, and before we agree on something, #573 can’t change how things work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:212,energy efficiency,current,current,212,"I totally agree that there are unresolved discussions here. I also think this issue has multiple conversations in it now, and it's probably a good idea to split it up. I think it's totally fine to not change the current behavior for now. I'd just note the initial PR didn't change where datasets are placed, and that change was in response to review. My intent was not to disregard this conversation, I had just assumed you and Alex had talked in a separate forum. Sorry about any confusion caused!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:97,interoperability,convers,conversations,97,"I totally agree that there are unresolved discussions here. I also think this issue has multiple conversations in it now, and it's probably a good idea to split it up. I think it's totally fine to not change the current behavior for now. I'd just note the initial PR didn't change where datasets are placed, and that change was in response to review. My intent was not to disregard this conversation, I had just assumed you and Alex had talked in a separate forum. Sorry about any confusion caused!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:387,interoperability,convers,conversation,387,"I totally agree that there are unresolved discussions here. I also think this issue has multiple conversations in it now, and it's probably a good idea to split it up. I think it's totally fine to not change the current behavior for now. I'd just note the initial PR didn't change where datasets are placed, and that change was in response to review. My intent was not to disregard this conversation, I had just assumed you and Alex had talked in a separate forum. Sorry about any confusion caused!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:343,safety,review,review,343,"I totally agree that there are unresolved discussions here. I also think this issue has multiple conversations in it now, and it's probably a good idea to split it up. I think it's totally fine to not change the current behavior for now. I'd just note the initial PR didn't change where datasets are placed, and that change was in response to review. My intent was not to disregard this conversation, I had just assumed you and Alex had talked in a separate forum. Sorry about any confusion caused!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:343,testability,review,review,343,"I totally agree that there are unresolved discussions here. I also think this issue has multiple conversations in it now, and it's probably a good idea to split it up. I think it's totally fine to not change the current behavior for now. I'd just note the initial PR didn't change where datasets are placed, and that change was in response to review. My intent was not to disregard this conversation, I had just assumed you and Alex had talked in a separate forum. Sorry about any confusion caused!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:220,usability,behavi,behavior,220,"I totally agree that there are unresolved discussions here. I also think this issue has multiple conversations in it now, and it's probably a good idea to split it up. I think it's totally fine to not change the current behavior for now. I'd just note the initial PR didn't change where datasets are placed, and that change was in response to review. My intent was not to disregard this conversation, I had just assumed you and Alex had talked in a separate forum. Sorry about any confusion caused!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/560:66,deployability,version,version,66,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:109,deployability,releas,released,109,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:118,deployability,version,version,118,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:142,deployability,instal,install,142,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:166,deployability,version,version,166,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:197,deployability,instal,install,197,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:326,deployability,instal,installation,326,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:356,deployability,version,version,356,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:391,deployability,version,version,391,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:489,deployability,releas,released,489,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:499,deployability,version,version,499,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:555,deployability,instal,installed,555,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:608,deployability,version,version,608,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:66,integrability,version,version,66,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:118,integrability,version,version,118,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:166,integrability,version,version,166,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:356,integrability,version,version,356,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:391,integrability,version,version,391,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:499,integrability,version,version,499,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:608,integrability,version,version,608,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:66,modifiability,version,version,66,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:118,modifiability,version,version,118,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:166,modifiability,version,version,166,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:356,modifiability,version,version,356,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:391,modifiability,version,version,391,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:499,modifiability,version,version,499,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:608,modifiability,version,version,608,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:464,usability,document,documentation,464,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:624,usability,document,documentation,624,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:652,usability,menu,menu,652,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:695,usability,user,user-images,695,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:87,usability,document,documentation,87,"@falexwolf @flying-sheep Do you guys think it makes sense to switch to ""stable"" in the documentation by default similarly to TensorFlow, PyTorch and Pandas which use stable by default?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:92,deployability,releas,releases,92,"Thank you and sorry about the confusion, I remembered this was an option present in earlier releases, but I was wrong!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:43,safety,reme,remembered,43,"Thank you and sorry about the confusion, I remembered this was an option present in earlier releases, but I was wrong!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:219,availability,avail,available,219,"@gokceneraslan We had it switched to stable by default for some time already. I'm fine doing this again; I switched it back because I found some typos, etc., some missing explanation that I wanted to become immediately available...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:64,performance,time,time,64,"@gokceneraslan We had it switched to stable by default for some time already. I'm fine doing this again; I switched it back because I found some typos, etc., some missing explanation that I wanted to become immediately available...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:219,reliability,availab,available,219,"@gokceneraslan We had it switched to stable by default for some time already. I'm fine doing this again; I switched it back because I found some typos, etc., some missing explanation that I wanted to become immediately available...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:219,safety,avail,available,219,"@gokceneraslan We had it switched to stable by default for some time already. I'm fine doing this again; I switched it back because I found some typos, etc., some missing explanation that I wanted to become immediately available...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:219,security,availab,available,219,"@gokceneraslan We had it switched to stable by default for some time already. I'm fine doing this again; I switched it back because I found some typos, etc., some missing explanation that I wanted to become immediately available...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:14,usability,user,user,14,"After another user had problems, I switched it over.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/561:573,availability,error,error,573,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:325,deployability,instal,install,325,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:439,deployability,modul,module,439,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:607,deployability,version,versions,607,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:607,integrability,version,versions,607,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:439,modifiability,modul,module,439,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:607,modifiability,version,versions,607,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:573,performance,error,error,573,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:275,safety,test,testenv,275,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:312,safety,test,testenv,312,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:439,safety,modul,module,439,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:573,safety,error,error,573,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:275,testability,test,testenv,275,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:312,testability,test,testenv,312,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:573,usability,error,error,573,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:626,usability,support,supported,626,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```. conda create -yn testenv python=3.5.2. conda activate testenv. pip install scanpy. python -c ""import scanpy"" . ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:115,deployability,releas,releases,115,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:179,deployability,updat,update,179,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:236,deployability,version,version,236,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:47,integrability,sub,submits,47,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:236,integrability,version,version,236,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:236,modifiability,version,version,236,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:179,safety,updat,update,179,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:179,security,updat,update,179,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:213,security,secur,security,213,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:25,testability,simpl,simple,25,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:25,usability,simpl,simple,25,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:89,usability,support,support,89,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:421,availability,error,error,421,"Thank you so much for the feedback! I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using . conda create -n scanpy python=3.6 scanpy. conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:98,deployability,updat,update,98,"Thank you so much for the feedback! I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using . conda create -n scanpy python=3.6 scanpy. conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:421,performance,error,error,421,"Thank you so much for the feedback! I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using . conda create -n scanpy python=3.6 scanpy. conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:98,safety,updat,update,98,"Thank you so much for the feedback! I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using . conda create -n scanpy python=3.6 scanpy. conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:421,safety,error,error,421,"Thank you so much for the feedback! I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using . conda create -n scanpy python=3.6 scanpy. conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:98,security,updat,update,98,"Thank you so much for the feedback! I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using . conda create -n scanpy python=3.6 scanpy. conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:492,security,ssh,ssh,492,"Thank you so much for the feedback! I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using . conda create -n scanpy python=3.6 scanpy. conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:557,security,token,token,557,"Thank you so much for the feedback! I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using . conda create -n scanpy python=3.6 scanpy. conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:26,usability,feedback,feedback,26,"Thank you so much for the feedback! I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using . conda create -n scanpy python=3.6 scanpy. conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:421,usability,error,error,421,"Thank you so much for the feedback! I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using . conda create -n scanpy python=3.6 scanpy. conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:41,security,token,token,41,"Hmm, that's strange. I use the generated token to connect to remote notebook servers pretty frequently. Any ideas why it isn't working for you? Here's the workflow I follow. First, on the machine you'd like to work on, in the conda environment you want to use:. ```. jupyter notebook --no-browser --port=8889. ```. This will start the notebook server and report the security token. On my machine I create an ssh tunnel to the server with the following command (replacing `<remote_user>` and `<remote_host>` with your info):. ```. ssh -N -L 8889:localhost:8889 <remote_user>@<remote_host>. ```. Now I point my browser to `localhost:8889` and will be connected to the remote server, where it'll ask for the token. After pasting that in I'm connected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:366,security,secur,security,366,"Hmm, that's strange. I use the generated token to connect to remote notebook servers pretty frequently. Any ideas why it isn't working for you? Here's the workflow I follow. First, on the machine you'd like to work on, in the conda environment you want to use:. ```. jupyter notebook --no-browser --port=8889. ```. This will start the notebook server and report the security token. On my machine I create an ssh tunnel to the server with the following command (replacing `<remote_user>` and `<remote_host>` with your info):. ```. ssh -N -L 8889:localhost:8889 <remote_user>@<remote_host>. ```. Now I point my browser to `localhost:8889` and will be connected to the remote server, where it'll ask for the token. After pasting that in I'm connected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:375,security,token,token,375,"Hmm, that's strange. I use the generated token to connect to remote notebook servers pretty frequently. Any ideas why it isn't working for you? Here's the workflow I follow. First, on the machine you'd like to work on, in the conda environment you want to use:. ```. jupyter notebook --no-browser --port=8889. ```. This will start the notebook server and report the security token. On my machine I create an ssh tunnel to the server with the following command (replacing `<remote_user>` and `<remote_host>` with your info):. ```. ssh -N -L 8889:localhost:8889 <remote_user>@<remote_host>. ```. Now I point my browser to `localhost:8889` and will be connected to the remote server, where it'll ask for the token. After pasting that in I'm connected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:408,security,ssh,ssh,408,"Hmm, that's strange. I use the generated token to connect to remote notebook servers pretty frequently. Any ideas why it isn't working for you? Here's the workflow I follow. First, on the machine you'd like to work on, in the conda environment you want to use:. ```. jupyter notebook --no-browser --port=8889. ```. This will start the notebook server and report the security token. On my machine I create an ssh tunnel to the server with the following command (replacing `<remote_user>` and `<remote_host>` with your info):. ```. ssh -N -L 8889:localhost:8889 <remote_user>@<remote_host>. ```. Now I point my browser to `localhost:8889` and will be connected to the remote server, where it'll ask for the token. After pasting that in I'm connected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:530,security,ssh,ssh,530,"Hmm, that's strange. I use the generated token to connect to remote notebook servers pretty frequently. Any ideas why it isn't working for you? Here's the workflow I follow. First, on the machine you'd like to work on, in the conda environment you want to use:. ```. jupyter notebook --no-browser --port=8889. ```. This will start the notebook server and report the security token. On my machine I create an ssh tunnel to the server with the following command (replacing `<remote_user>` and `<remote_host>` with your info):. ```. ssh -N -L 8889:localhost:8889 <remote_user>@<remote_host>. ```. Now I point my browser to `localhost:8889` and will be connected to the remote server, where it'll ask for the token. After pasting that in I'm connected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:705,security,token,token,705,"Hmm, that's strange. I use the generated token to connect to remote notebook servers pretty frequently. Any ideas why it isn't working for you? Here's the workflow I follow. First, on the machine you'd like to work on, in the conda environment you want to use:. ```. jupyter notebook --no-browser --port=8889. ```. This will start the notebook server and report the security token. On my machine I create an ssh tunnel to the server with the following command (replacing `<remote_user>` and `<remote_host>` with your info):. ```. ssh -N -L 8889:localhost:8889 <remote_user>@<remote_host>. ```. Now I point my browser to `localhost:8889` and will be connected to the remote server, where it'll ask for the token. After pasting that in I'm connected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:155,usability,workflow,workflow,155,"Hmm, that's strange. I use the generated token to connect to remote notebook servers pretty frequently. Any ideas why it isn't working for you? Here's the workflow I follow. First, on the machine you'd like to work on, in the conda environment you want to use:. ```. jupyter notebook --no-browser --port=8889. ```. This will start the notebook server and report the security token. On my machine I create an ssh tunnel to the server with the following command (replacing `<remote_user>` and `<remote_host>` with your info):. ```. ssh -N -L 8889:localhost:8889 <remote_user>@<remote_host>. ```. Now I point my browser to `localhost:8889` and will be connected to the remote server, where it'll ask for the token. After pasting that in I'm connected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:452,usability,command,command,452,"Hmm, that's strange. I use the generated token to connect to remote notebook servers pretty frequently. Any ideas why it isn't working for you? Here's the workflow I follow. First, on the machine you'd like to work on, in the conda environment you want to use:. ```. jupyter notebook --no-browser --port=8889. ```. This will start the notebook server and report the security token. On my machine I create an ssh tunnel to the server with the following command (replacing `<remote_user>` and `<remote_host>` with your info):. ```. ssh -N -L 8889:localhost:8889 <remote_user>@<remote_host>. ```. Now I point my browser to `localhost:8889` and will be connected to the remote server, where it'll ask for the token. After pasting that in I'm connected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:64,deployability,updat,update,64,"> I'll definitely talk to the admin, but I am not sure he would update. An admin that doesn’t take security risks seriously isn’t doing their job properly. ---. > Jupyter Notebook requires JavaScript. that probably means that Jupyter notebook tries to run lynx or www or sone other text-only browser. `jupyter notebook --no-browser` is correct and the tokens aren’t machine-specific. [I set up stuff differently](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup), but @ivirshup’s setup should work perfectly as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:482,deployability,automat,automatic-password-setup,482,"> I'll definitely talk to the admin, but I am not sure he would update. An admin that doesn’t take security risks seriously isn’t doing their job properly. ---. > Jupyter Notebook requires JavaScript. that probably means that Jupyter notebook tries to run lynx or www or sone other text-only browser. `jupyter notebook --no-browser` is correct and the tokens aren’t machine-specific. [I set up stuff differently](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup), but @ivirshup’s setup should work perfectly as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:374,interoperability,specif,specific,374,"> I'll definitely talk to the admin, but I am not sure he would update. An admin that doesn’t take security risks seriously isn’t doing their job properly. ---. > Jupyter Notebook requires JavaScript. that probably means that Jupyter notebook tries to run lynx or www or sone other text-only browser. `jupyter notebook --no-browser` is correct and the tokens aren’t machine-specific. [I set up stuff differently](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup), but @ivirshup’s setup should work perfectly as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:86,reliability,doe,doesn,86,"> I'll definitely talk to the admin, but I am not sure he would update. An admin that doesn’t take security risks seriously isn’t doing their job properly. ---. > Jupyter Notebook requires JavaScript. that probably means that Jupyter notebook tries to run lynx or www or sone other text-only browser. `jupyter notebook --no-browser` is correct and the tokens aren’t machine-specific. [I set up stuff differently](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup), but @ivirshup’s setup should work perfectly as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:64,safety,updat,update,64,"> I'll definitely talk to the admin, but I am not sure he would update. An admin that doesn’t take security risks seriously isn’t doing their job properly. ---. > Jupyter Notebook requires JavaScript. that probably means that Jupyter notebook tries to run lynx or www or sone other text-only browser. `jupyter notebook --no-browser` is correct and the tokens aren’t machine-specific. [I set up stuff differently](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup), but @ivirshup’s setup should work perfectly as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:108,safety,risk,risks,108,"> I'll definitely talk to the admin, but I am not sure he would update. An admin that doesn’t take security risks seriously isn’t doing their job properly. ---. > Jupyter Notebook requires JavaScript. that probably means that Jupyter notebook tries to run lynx or www or sone other text-only browser. `jupyter notebook --no-browser` is correct and the tokens aren’t machine-specific. [I set up stuff differently](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup), but @ivirshup’s setup should work perfectly as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:64,security,updat,update,64,"> I'll definitely talk to the admin, but I am not sure he would update. An admin that doesn’t take security risks seriously isn’t doing their job properly. ---. > Jupyter Notebook requires JavaScript. that probably means that Jupyter notebook tries to run lynx or www or sone other text-only browser. `jupyter notebook --no-browser` is correct and the tokens aren’t machine-specific. [I set up stuff differently](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup), but @ivirshup’s setup should work perfectly as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:99,security,secur,security,99,"> I'll definitely talk to the admin, but I am not sure he would update. An admin that doesn’t take security risks seriously isn’t doing their job properly. ---. > Jupyter Notebook requires JavaScript. that probably means that Jupyter notebook tries to run lynx or www or sone other text-only browser. `jupyter notebook --no-browser` is correct and the tokens aren’t machine-specific. [I set up stuff differently](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup), but @ivirshup’s setup should work perfectly as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:108,security,risk,risks,108,"> I'll definitely talk to the admin, but I am not sure he would update. An admin that doesn’t take security risks seriously isn’t doing their job properly. ---. > Jupyter Notebook requires JavaScript. that probably means that Jupyter notebook tries to run lynx or www or sone other text-only browser. `jupyter notebook --no-browser` is correct and the tokens aren’t machine-specific. [I set up stuff differently](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup), but @ivirshup’s setup should work perfectly as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:352,security,token,tokens,352,"> I'll definitely talk to the admin, but I am not sure he would update. An admin that doesn’t take security risks seriously isn’t doing their job properly. ---. > Jupyter Notebook requires JavaScript. that probably means that Jupyter notebook tries to run lynx or www or sone other text-only browser. `jupyter notebook --no-browser` is correct and the tokens aren’t machine-specific. [I set up stuff differently](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup), but @ivirshup’s setup should work perfectly as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:492,security,password,password-setup,492,"> I'll definitely talk to the admin, but I am not sure he would update. An admin that doesn’t take security risks seriously isn’t doing their job properly. ---. > Jupyter Notebook requires JavaScript. that probably means that Jupyter notebook tries to run lynx or www or sone other text-only browser. `jupyter notebook --no-browser` is correct and the tokens aren’t machine-specific. [I set up stuff differently](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup), but @ivirshup’s setup should work perfectly as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:482,testability,automat,automatic-password-setup,482,"> I'll definitely talk to the admin, but I am not sure he would update. An admin that doesn’t take security risks seriously isn’t doing their job properly. ---. > Jupyter Notebook requires JavaScript. that probably means that Jupyter notebook tries to run lynx or www or sone other text-only browser. `jupyter notebook --no-browser` is correct and the tokens aren’t machine-specific. [I set up stuff differently](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup), but @ivirshup’s setup should work perfectly as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:46,deployability,instal,installed,46,"Just an extra idea... In case you have docker installed, you could use a dockerized scanpy. The guys running SCENIC have a dockerized version of it in their workflow. Maybe you could ask them nicely for their docker image?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:134,deployability,version,version,134,"Just an extra idea... In case you have docker installed, you could use a dockerized scanpy. The guys running SCENIC have a dockerized version of it in their workflow. Maybe you could ask them nicely for their docker image?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:134,integrability,version,version,134,"Just an extra idea... In case you have docker installed, you could use a dockerized scanpy. The guys running SCENIC have a dockerized version of it in their workflow. Maybe you could ask them nicely for their docker image?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:134,modifiability,version,version,134,"Just an extra idea... In case you have docker installed, you could use a dockerized scanpy. The guys running SCENIC have a dockerized version of it in their workflow. Maybe you could ask them nicely for their docker image?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:157,usability,workflow,workflow,157,"Just an extra idea... In case you have docker installed, you could use a dockerized scanpy. The guys running SCENIC have a dockerized version of it in their workflow. Maybe you could ask them nicely for their docker image?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:90,deployability,updat,updated,90,"There’s also https://github.com/FASTGenomics/base_image_alpine_scanpy, but it hasn’t been updated in a while. Should be no big problem to update it though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:138,deployability,updat,update,138,"There’s also https://github.com/FASTGenomics/base_image_alpine_scanpy, but it hasn’t been updated in a while. Should be no big problem to update it though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:90,safety,updat,updated,90,"There’s also https://github.com/FASTGenomics/base_image_alpine_scanpy, but it hasn’t been updated in a while. Should be no big problem to update it though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:138,safety,updat,update,138,"There’s also https://github.com/FASTGenomics/base_image_alpine_scanpy, but it hasn’t been updated in a while. Should be no big problem to update it though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:90,security,updat,updated,90,"There’s also https://github.com/FASTGenomics/base_image_alpine_scanpy, but it hasn’t been updated in a while. Should be no big problem to update it though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:138,security,updat,update,138,"There’s also https://github.com/FASTGenomics/base_image_alpine_scanpy, but it hasn’t been updated in a while. Should be no big problem to update it though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:62,deployability,manag,managed,62,Thank you so much for all the suggestions and help! I finally managed to get it working with the conda environment.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:62,energy efficiency,manag,managed,62,Thank you so much for all the suggestions and help! I finally managed to get it working with the conda environment.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:62,safety,manag,managed,62,Thank you so much for all the suggestions and help! I finally managed to get it working with the conda environment.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:46,usability,help,help,46,Thank you so much for all the suggestions and help! I finally managed to get it working with the conda environment.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/562:52,integrability,sub,submit,52,"I thing that is not so difficult to achieve this. I submit a PR soon. Fidel Ramírez . > On 26 Mar 2019, at 22:02, Alex Wolf <notifications@github.com> wrote:. > . > @fidelram, as discussed today, could we adopt pl.rank_genes_groups_dotplot so that it reads this information from .uns['rank_genes_groups']? > . > Maybe just a simple switch? Or having arguments color and size be a choice from a selection {pvals, pvals_adj, log2FC, expression, frac-genes-expressed}. > . > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:325,testability,simpl,simple,325,"I thing that is not so difficult to achieve this. I submit a PR soon. Fidel Ramírez . > On 26 Mar 2019, at 22:02, Alex Wolf <notifications@github.com> wrote:. > . > @fidelram, as discussed today, could we adopt pl.rank_genes_groups_dotplot so that it reads this information from .uns['rank_genes_groups']? > . > Maybe just a simple switch? Or having arguments color and size be a choice from a selection {pvals, pvals_adj, log2FC, expression, frac-genes-expressed}. > . > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:325,usability,simpl,simple,325,"I thing that is not so difficult to achieve this. I submit a PR soon. Fidel Ramírez . > On 26 Mar 2019, at 22:02, Alex Wolf <notifications@github.com> wrote:. > . > @fidelram, as discussed today, could we adopt pl.rank_genes_groups_dotplot so that it reads this information from .uns['rank_genes_groups']? > . > Maybe just a simple switch? Or having arguments color and size be a choice from a selection {pvals, pvals_adj, log2FC, expression, frac-genes-expressed}. > . > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:468,availability,cluster,cluster-specific,468,"> @fidelram, as discussed today, could we adopt `pl.rank_genes_groups_dotplot` so that it reads this information from `.uns['rank_genes_groups']`? > . > Maybe just a simple switch? Or having arguments `color` and `size` be a choice from a selection {`pvals`, `pvals_adj`, `log2FC`, `expression`, `frac-genes-expressed`}. I would also love that actually 😄 . `rank_genes_groups` results (LFC, p-val etc) and things like `mean-expression`, `frac-genes-expressed` are all cluster-specific features, which reminds me of an `obs`-like structure with clusters in rows. Right now, mean expression and fractions are calculated in a private function (`_prepare_dataframe`) in `plotting/_anndata.py` but we can move this to `utils.py` or so, call it in sc.tl.rank_genes_groups() and store the resulting data frame in ad.uns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:544,availability,cluster,clusters,544,"> @fidelram, as discussed today, could we adopt `pl.rank_genes_groups_dotplot` so that it reads this information from `.uns['rank_genes_groups']`? > . > Maybe just a simple switch? Or having arguments `color` and `size` be a choice from a selection {`pvals`, `pvals_adj`, `log2FC`, `expression`, `frac-genes-expressed`}. I would also love that actually 😄 . `rank_genes_groups` results (LFC, p-val etc) and things like `mean-expression`, `frac-genes-expressed` are all cluster-specific features, which reminds me of an `obs`-like structure with clusters in rows. Right now, mean expression and fractions are calculated in a private function (`_prepare_dataframe`) in `plotting/_anndata.py` but we can move this to `utils.py` or so, call it in sc.tl.rank_genes_groups() and store the resulting data frame in ad.uns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:468,deployability,cluster,cluster-specific,468,"> @fidelram, as discussed today, could we adopt `pl.rank_genes_groups_dotplot` so that it reads this information from `.uns['rank_genes_groups']`? > . > Maybe just a simple switch? Or having arguments `color` and `size` be a choice from a selection {`pvals`, `pvals_adj`, `log2FC`, `expression`, `frac-genes-expressed`}. I would also love that actually 😄 . `rank_genes_groups` results (LFC, p-val etc) and things like `mean-expression`, `frac-genes-expressed` are all cluster-specific features, which reminds me of an `obs`-like structure with clusters in rows. Right now, mean expression and fractions are calculated in a private function (`_prepare_dataframe`) in `plotting/_anndata.py` but we can move this to `utils.py` or so, call it in sc.tl.rank_genes_groups() and store the resulting data frame in ad.uns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:544,deployability,cluster,clusters,544,"> @fidelram, as discussed today, could we adopt `pl.rank_genes_groups_dotplot` so that it reads this information from `.uns['rank_genes_groups']`? > . > Maybe just a simple switch? Or having arguments `color` and `size` be a choice from a selection {`pvals`, `pvals_adj`, `log2FC`, `expression`, `frac-genes-expressed`}. I would also love that actually 😄 . `rank_genes_groups` results (LFC, p-val etc) and things like `mean-expression`, `frac-genes-expressed` are all cluster-specific features, which reminds me of an `obs`-like structure with clusters in rows. Right now, mean expression and fractions are calculated in a private function (`_prepare_dataframe`) in `plotting/_anndata.py` but we can move this to `utils.py` or so, call it in sc.tl.rank_genes_groups() and store the resulting data frame in ad.uns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:476,interoperability,specif,specific,476,"> @fidelram, as discussed today, could we adopt `pl.rank_genes_groups_dotplot` so that it reads this information from `.uns['rank_genes_groups']`? > . > Maybe just a simple switch? Or having arguments `color` and `size` be a choice from a selection {`pvals`, `pvals_adj`, `log2FC`, `expression`, `frac-genes-expressed`}. I would also love that actually 😄 . `rank_genes_groups` results (LFC, p-val etc) and things like `mean-expression`, `frac-genes-expressed` are all cluster-specific features, which reminds me of an `obs`-like structure with clusters in rows. Right now, mean expression and fractions are calculated in a private function (`_prepare_dataframe`) in `plotting/_anndata.py` but we can move this to `utils.py` or so, call it in sc.tl.rank_genes_groups() and store the resulting data frame in ad.uns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:166,testability,simpl,simple,166,"> @fidelram, as discussed today, could we adopt `pl.rank_genes_groups_dotplot` so that it reads this information from `.uns['rank_genes_groups']`? > . > Maybe just a simple switch? Or having arguments `color` and `size` be a choice from a selection {`pvals`, `pvals_adj`, `log2FC`, `expression`, `frac-genes-expressed`}. I would also love that actually 😄 . `rank_genes_groups` results (LFC, p-val etc) and things like `mean-expression`, `frac-genes-expressed` are all cluster-specific features, which reminds me of an `obs`-like structure with clusters in rows. Right now, mean expression and fractions are calculated in a private function (`_prepare_dataframe`) in `plotting/_anndata.py` but we can move this to `utils.py` or so, call it in sc.tl.rank_genes_groups() and store the resulting data frame in ad.uns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:166,usability,simpl,simple,166,"> @fidelram, as discussed today, could we adopt `pl.rank_genes_groups_dotplot` so that it reads this information from `.uns['rank_genes_groups']`? > . > Maybe just a simple switch? Or having arguments `color` and `size` be a choice from a selection {`pvals`, `pvals_adj`, `log2FC`, `expression`, `frac-genes-expressed`}. I would also love that actually 😄 . `rank_genes_groups` results (LFC, p-val etc) and things like `mean-expression`, `frac-genes-expressed` are all cluster-specific features, which reminds me of an `obs`-like structure with clusters in rows. Right now, mean expression and fractions are calculated in a private function (`_prepare_dataframe`) in `plotting/_anndata.py` but we can move this to `utils.py` or so, call it in sc.tl.rank_genes_groups() and store the resulting data frame in ad.uns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:54,availability,cluster,clusters,54,"@gokceneraslan the idea of a `obs`-like structure for clusters or in general for categorical data is interesting. Currently, in `.uns` we are using numpy structured arrays for ranked genes which are not so easy to navigate. A `obs`-like object using `pandas` could facilitate data manipulation and visualization. @falexwolf should we give it a try or do you see problem with this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:54,deployability,cluster,clusters,54,"@gokceneraslan the idea of a `obs`-like structure for clusters or in general for categorical data is interesting. Currently, in `.uns` we are using numpy structured arrays for ranked genes which are not so easy to navigate. A `obs`-like object using `pandas` could facilitate data manipulation and visualization. @falexwolf should we give it a try or do you see problem with this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:114,energy efficiency,Current,Currently,114,"@gokceneraslan the idea of a `obs`-like structure for clusters or in general for categorical data is interesting. Currently, in `.uns` we are using numpy structured arrays for ranked genes which are not so easy to navigate. A `obs`-like object using `pandas` could facilitate data manipulation and visualization. @falexwolf should we give it a try or do you see problem with this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:214,usability,navigat,navigate,214,"@gokceneraslan the idea of a `obs`-like structure for clusters or in general for categorical data is interesting. Currently, in `.uns` we are using numpy structured arrays for ranked genes which are not so easy to navigate. A `obs`-like object using `pandas` could facilitate data manipulation and visualization. @falexwolf should we give it a try or do you see problem with this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:298,usability,visual,visualization,298,"@gokceneraslan the idea of a `obs`-like structure for clusters or in general for categorical data is interesting. Currently, in `.uns` we are using numpy structured arrays for ranked genes which are not so easy to navigate. A `obs`-like object using `pandas` could facilitate data manipulation and visualization. @falexwolf should we give it a try or do you see problem with this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:61,usability,support,support,61,If this is stored in `.uns` we would have to wait for pandas support for `.uns`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:242,safety,compl,complicated,242,"Why is it that .obs, .var, and .uns don't have data frames in them? `np.recarray` don't seem like a very popular data structure elsewhere. Also, I'd like to suggest that storing all differential expression within the anndata object might get complicated, and deserve it's own class. It'd be nice if it could be easy to tell what cells and genes were compared, what exactly was being tested, and which direction is ""up"". That said, the results should definitely be easily accessible as a data frame.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:383,safety,test,tested,383,"Why is it that .obs, .var, and .uns don't have data frames in them? `np.recarray` don't seem like a very popular data structure elsewhere. Also, I'd like to suggest that storing all differential expression within the anndata object might get complicated, and deserve it's own class. It'd be nice if it could be easy to tell what cells and genes were compared, what exactly was being tested, and which direction is ""up"". That said, the results should definitely be easily accessible as a data frame.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:242,security,compl,complicated,242,"Why is it that .obs, .var, and .uns don't have data frames in them? `np.recarray` don't seem like a very popular data structure elsewhere. Also, I'd like to suggest that storing all differential expression within the anndata object might get complicated, and deserve it's own class. It'd be nice if it could be easy to tell what cells and genes were compared, what exactly was being tested, and which direction is ""up"". That said, the results should definitely be easily accessible as a data frame.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:471,security,access,accessible,471,"Why is it that .obs, .var, and .uns don't have data frames in them? `np.recarray` don't seem like a very popular data structure elsewhere. Also, I'd like to suggest that storing all differential expression within the anndata object might get complicated, and deserve it's own class. It'd be nice if it could be easy to tell what cells and genes were compared, what exactly was being tested, and which direction is ""up"". That said, the results should definitely be easily accessible as a data frame.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:383,testability,test,tested,383,"Why is it that .obs, .var, and .uns don't have data frames in them? `np.recarray` don't seem like a very popular data structure elsewhere. Also, I'd like to suggest that storing all differential expression within the anndata object might get complicated, and deserve it's own class. It'd be nice if it could be easy to tell what cells and genes were compared, what exactly was being tested, and which direction is ""up"". That said, the results should definitely be easily accessible as a data frame.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:195,deployability,API,API,195,"Huh? `obs` and `var` have been `DataFrame`s for a long time now. and `uns` is a dict. The reason why they haven’t originally been that is that `DataFrame`s are super complicated and have a giant API. We thought having something simpler would be easier to work with, but in the end convenience won.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:195,integrability,API,API,195,"Huh? `obs` and `var` have been `DataFrame`s for a long time now. and `uns` is a dict. The reason why they haven’t originally been that is that `DataFrame`s are super complicated and have a giant API. We thought having something simpler would be easier to work with, but in the end convenience won.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:195,interoperability,API,API,195,"Huh? `obs` and `var` have been `DataFrame`s for a long time now. and `uns` is a dict. The reason why they haven’t originally been that is that `DataFrame`s are super complicated and have a giant API. We thought having something simpler would be easier to work with, but in the end convenience won.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:55,performance,time,time,55,"Huh? `obs` and `var` have been `DataFrame`s for a long time now. and `uns` is a dict. The reason why they haven’t originally been that is that `DataFrame`s are super complicated and have a giant API. We thought having something simpler would be easier to work with, but in the end convenience won.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:166,safety,compl,complicated,166,"Huh? `obs` and `var` have been `DataFrame`s for a long time now. and `uns` is a dict. The reason why they haven’t originally been that is that `DataFrame`s are super complicated and have a giant API. We thought having something simpler would be easier to work with, but in the end convenience won.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:166,security,compl,complicated,166,"Huh? `obs` and `var` have been `DataFrame`s for a long time now. and `uns` is a dict. The reason why they haven’t originally been that is that `DataFrame`s are super complicated and have a giant API. We thought having something simpler would be easier to work with, but in the end convenience won.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:228,testability,simpl,simpler,228,"Huh? `obs` and `var` have been `DataFrame`s for a long time now. and `uns` is a dict. The reason why they haven’t originally been that is that `DataFrame`s are super complicated and have a giant API. We thought having something simpler would be easier to work with, but in the end convenience won.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:228,usability,simpl,simpler,228,"Huh? `obs` and `var` have been `DataFrame`s for a long time now. and `uns` is a dict. The reason why they haven’t originally been that is that `DataFrame`s are super complicated and have a giant API. We thought having something simpler would be easier to work with, but in the end convenience won.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:90,availability,cluster,clusterfuck,90,I think enabling that `.obsm['X_pca']` by subclassing pandas DataFrames would have been a clusterfuck.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:90,deployability,cluster,clusterfuck,90,I think enabling that `.obsm['X_pca']` by subclassing pandas DataFrames would have been a clusterfuck.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:42,integrability,sub,subclassing,42,I think enabling that `.obsm['X_pca']` by subclassing pandas DataFrames would have been a clusterfuck.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:50,availability,cluster,clusterfuck,50,"> subclassing pandas DataFrames would have been a clusterfuck. Definitely would be. I'd thought the alternative would be to subclass something like a `dict`, but have the setters check that the values have the right size. Also taking subsets across all the values when subsetting an AnnData, but that's just adding a loop.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:50,deployability,cluster,clusterfuck,50,"> subclassing pandas DataFrames would have been a clusterfuck. Definitely would be. I'd thought the alternative would be to subclass something like a `dict`, but have the setters check that the values have the right size. Also taking subsets across all the values when subsetting an AnnData, but that's just adding a loop.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2,integrability,sub,subclassing,2,"> subclassing pandas DataFrames would have been a clusterfuck. Definitely would be. I'd thought the alternative would be to subclass something like a `dict`, but have the setters check that the values have the right size. Also taking subsets across all the values when subsetting an AnnData, but that's just adding a loop.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:124,integrability,sub,subclass,124,"> subclassing pandas DataFrames would have been a clusterfuck. Definitely would be. I'd thought the alternative would be to subclass something like a `dict`, but have the setters check that the values have the right size. Also taking subsets across all the values when subsetting an AnnData, but that's just adding a loop.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:234,integrability,sub,subsets,234,"> subclassing pandas DataFrames would have been a clusterfuck. Definitely would be. I'd thought the alternative would be to subclass something like a `dict`, but have the setters check that the values have the right size. Also taking subsets across all the values when subsetting an AnnData, but that's just adding a loop.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:269,integrability,sub,subsetting,269,"> subclassing pandas DataFrames would have been a clusterfuck. Definitely would be. I'd thought the alternative would be to subclass something like a `dict`, but have the setters check that the values have the right size. Also taking subsets across all the values when subsetting an AnnData, but that's just adding a loop.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:48,integrability,sub,subclass,48,"Sure, but I think it’s about the same effort to subclass a mapping while adding array features compared to the other way round.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:12,integrability,topic,topic,12,"Back on the topic of getting dot plots a bit more flexible, I've been working on an approach that could work. You can check it out in [this binder environment](https://mybinder.org/v2/gh/ivirshup/scanpy-interactive/master?filepath=notebooks%2Fflexible_de.ipynb), but it's based on two main ideas:. 1. It'd be nice if there were an easy way to get aggregated values for groups, so I've added a crude `groupby` to `AnnData`. 2. Our differential expression results are like a 3d array, with axes `[""genes"", ""group"", ""values""]` where values are things like p-values and mean expression. Here's a quick example of the output:. ![image](https://user-images.githubusercontent.com/8238804/56495993-0dc4fc00-653b-11e9-8831-a830b2ead841.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:140,interoperability,bind,binder,140,"Back on the topic of getting dot plots a bit more flexible, I've been working on an approach that could work. You can check it out in [this binder environment](https://mybinder.org/v2/gh/ivirshup/scanpy-interactive/master?filepath=notebooks%2Fflexible_de.ipynb), but it's based on two main ideas:. 1. It'd be nice if there were an easy way to get aggregated values for groups, so I've added a crude `groupby` to `AnnData`. 2. Our differential expression results are like a 3d array, with axes `[""genes"", ""group"", ""values""]` where values are things like p-values and mean expression. Here's a quick example of the output:. ![image](https://user-images.githubusercontent.com/8238804/56495993-0dc4fc00-653b-11e9-8831-a830b2ead841.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:140,modifiability,bind,binder,140,"Back on the topic of getting dot plots a bit more flexible, I've been working on an approach that could work. You can check it out in [this binder environment](https://mybinder.org/v2/gh/ivirshup/scanpy-interactive/master?filepath=notebooks%2Fflexible_de.ipynb), but it's based on two main ideas:. 1. It'd be nice if there were an easy way to get aggregated values for groups, so I've added a crude `groupby` to `AnnData`. 2. Our differential expression results are like a 3d array, with axes `[""genes"", ""group"", ""values""]` where values are things like p-values and mean expression. Here's a quick example of the output:. ![image](https://user-images.githubusercontent.com/8238804/56495993-0dc4fc00-653b-11e9-8831-a830b2ead841.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:203,usability,interact,interactive,203,"Back on the topic of getting dot plots a bit more flexible, I've been working on an approach that could work. You can check it out in [this binder environment](https://mybinder.org/v2/gh/ivirshup/scanpy-interactive/master?filepath=notebooks%2Fflexible_de.ipynb), but it's based on two main ideas:. 1. It'd be nice if there were an easy way to get aggregated values for groups, so I've added a crude `groupby` to `AnnData`. 2. Our differential expression results are like a 3d array, with axes `[""genes"", ""group"", ""values""]` where values are things like p-values and mean expression. Here's a quick example of the output:. ![image](https://user-images.githubusercontent.com/8238804/56495993-0dc4fc00-653b-11e9-8831-a830b2ead841.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:639,usability,user,user-images,639,"Back on the topic of getting dot plots a bit more flexible, I've been working on an approach that could work. You can check it out in [this binder environment](https://mybinder.org/v2/gh/ivirshup/scanpy-interactive/master?filepath=notebooks%2Fflexible_de.ipynb), but it's based on two main ideas:. 1. It'd be nice if there were an easy way to get aggregated values for groups, so I've added a crude `groupby` to `AnnData`. 2. Our differential expression results are like a 3d array, with axes `[""genes"", ""group"", ""values""]` where values are things like p-values and mean expression. Here's a quick example of the output:. ![image](https://user-images.githubusercontent.com/8238804/56495993-0dc4fc00-653b-11e9-8831-a830b2ead841.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:310,deployability,log,log,310,"This looks really cool... but then I haven't used dot plots much before, so not sure what this is replacing... I just wonder if you can put different thresholds on the `pvals_adj` colour bar, such that every p-value `> 0.05` is grey or something like that... Or even better would be to put the colour bar on a log scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:314,deployability,scale,scale,314,"This looks really cool... but then I haven't used dot plots much before, so not sure what this is replacing... I just wonder if you can put different thresholds on the `pvals_adj` colour bar, such that every p-value `> 0.05` is grey or something like that... Or even better would be to put the colour bar on a log scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:18,energy efficiency,cool,cool,18,"This looks really cool... but then I haven't used dot plots much before, so not sure what this is replacing... I just wonder if you can put different thresholds on the `pvals_adj` colour bar, such that every p-value `> 0.05` is grey or something like that... Or even better would be to put the colour bar on a log scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:314,energy efficiency,scale,scale,314,"This looks really cool... but then I haven't used dot plots much before, so not sure what this is replacing... I just wonder if you can put different thresholds on the `pvals_adj` colour bar, such that every p-value `> 0.05` is grey or something like that... Or even better would be to put the colour bar on a log scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:314,modifiability,scal,scale,314,"This looks really cool... but then I haven't used dot plots much before, so not sure what this is replacing... I just wonder if you can put different thresholds on the `pvals_adj` colour bar, such that every p-value `> 0.05` is grey or something like that... Or even better would be to put the colour bar on a log scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:314,performance,scale,scale,314,"This looks really cool... but then I haven't used dot plots much before, so not sure what this is replacing... I just wonder if you can put different thresholds on the `pvals_adj` colour bar, such that every p-value `> 0.05` is grey or something like that... Or even better would be to put the colour bar on a log scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:310,safety,log,log,310,"This looks really cool... but then I haven't used dot plots much before, so not sure what this is replacing... I just wonder if you can put different thresholds on the `pvals_adj` colour bar, such that every p-value `> 0.05` is grey or something like that... Or even better would be to put the colour bar on a log scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:310,security,log,log,310,"This looks really cool... but then I haven't used dot plots much before, so not sure what this is replacing... I just wonder if you can put different thresholds on the `pvals_adj` colour bar, such that every p-value `> 0.05` is grey or something like that... Or even better would be to put the colour bar on a log scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:310,testability,log,log,310,"This looks really cool... but then I haven't used dot plots much before, so not sure what this is replacing... I just wonder if you can put different thresholds on the `pvals_adj` colour bar, such that every p-value `> 0.05` is grey or something like that... Or even better would be to put the colour bar on a log scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:26,availability,cluster,clusters,26,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:26,deployability,cluster,clusters,26,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:676,deployability,log,logic,676,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:839,deployability,manag,manageable,839,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:304,energy efficiency,power,powerful,304,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:839,energy efficiency,manag,manageable,839,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:44,safety,Compl,Completely,44,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:272,safety,test,test,272,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:676,safety,log,logic,676,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:714,safety,compl,complicated,714,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:839,safety,manag,manageable,839,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1951,safety,compl,completely,1951,"usters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means of aggregating return values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:44,security,Compl,Completely,44,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:524,security,access,accessible,524,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:676,security,log,logic,676,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:714,security,compl,complicated,714,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1027,security,access,accessing,1027,"usters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means of aggregating return values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1951,security,compl,completely,1951,"usters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means of aggregating return values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:157,testability,simpl,simply,157,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:272,testability,test,test,272,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:676,testability,log,logic,676,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:157,usability,simpl,simply,157,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:439,usability,intuit,intuitive,439,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:749,usability,visual,visualizes,749,"> obs-like structure with clusters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1827,usability,support,supported,1827,"usters in rows. Completely agreed! 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... . 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means of aggregating return values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:633,deployability,api,api,633,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:697,deployability,modul,module,697,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:862,deployability,version,version,862,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1030,energy efficiency,model,model,1030,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:633,integrability,api,api,633,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:862,integrability,version,version,862,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:257,interoperability,bind,binder,257,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:633,interoperability,api,api,633,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1044,interoperability,semant,semantics,1044,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:257,modifiability,bind,binder,257,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:697,modifiability,modul,module,697,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:862,modifiability,version,version,862,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:697,safety,modul,module,697,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:205,security,access,access,205,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1030,security,model,model,1030,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:83,usability,intuit,intuitive,83,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:372,usability,close,close,372,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:243,deployability,build,build,243,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:439,deployability,API,API,439,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:524,deployability,modul,module,524,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1111,deployability,API,APIs,1111," discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't conve",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2025,deployability,continu,continue,2025,"pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2257,deployability,API,API,2257,"ays, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2573,deployability,API,API,2573," the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2723,deployability,log,logic,2723,"eanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everythi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2945,deployability,pipelin,pipeline,2945,"bout how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:3962,deployability,API,API,3962,"ace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4120,deployability,API,API,4120,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4715,deployability,API,API,4715,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4741,deployability,API,API,4741,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4759,deployability,continu,continue,4759,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4918,deployability,log,logreg,4918,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4966,deployability,depend,dependency,4966,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:679,energy efficiency,core,core,679,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:752,energy efficiency,model,model,752,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:3600,energy efficiency,current,current,3600,"ighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the fu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4778,energy efficiency,core,core,4778,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:439,integrability,API,API,439,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1111,integrability,API,APIs,1111," discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't conve",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2257,integrability,API,API,2257,"ays, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2573,integrability,API,API,2573," the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2945,integrability,pipelin,pipeline,2945,"bout how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:3962,integrability,API,API,3962,"ace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4120,integrability,API,API,4120,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4715,integrability,API,API,4715,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4741,integrability,API,API,4741,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4966,integrability,depend,dependency,4966,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:57,interoperability,format,format,57,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:439,interoperability,API,API,439,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1111,interoperability,API,APIs,1111," discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't conve",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1244,interoperability,format,format,1244,"d a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2257,interoperability,API,API,2257,"ays, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2573,interoperability,API,API,2573," the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:3962,interoperability,API,API,3962,"ace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4016,interoperability,conflict,conflicting,4016,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4120,interoperability,API,API,4120,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4320,interoperability,specif,specific,4320,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4715,interoperability,API,API,4715,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4741,interoperability,API,API,4741,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:524,modifiability,modul,module,524,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1648,modifiability,scal,scalable,1648,"e sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one sho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4815,modifiability,scal,scalable,4815,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4849,modifiability,scal,scalable,4849,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4966,modifiability,depend,dependency,4966,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1190,performance,disk,disk,1190,"ay that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A gener",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1480,performance,disk,disk,1480,"y agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1648,performance,scalab,scalable,1648,"e sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one sho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1979,performance,disk,disk,1979,"nt to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but lat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2067,performance,disk,disk,2067," There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2080,performance,memor,memory,2080,"ade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4815,performance,scalab,scalable,4815,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4849,performance,scalab,scalable,4849,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2759,reliability,doe,doesn,2759,"cause Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the futur",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:98,safety,reme,remember,98,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:473,safety,Compl,Completely,473,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:524,safety,modul,module,524,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2299,safety,compl,completely,2299,", which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2320,safety,compl,complexity,2320,"ved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2723,safety,log,logic,2723,"eanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everythi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4530,safety,test,tests,4530,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4892,safety,test,test,4892,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4918,safety,log,logreg,4918,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4966,safety,depend,dependency,4966,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4998,safety,compl,complex,4998,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:473,security,Compl,Completely,473,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:752,security,model,model,752,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2299,security,compl,completely,2299,", which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2320,security,compl,complexity,2320,"ved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2723,security,log,logic,2723,"eanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everythi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4918,security,log,logreg,4918,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4998,security,compl,complex,4998,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1282,testability,simpl,simply,1282,"olution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2723,testability,log,logic,2723,"eanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everythi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2774,testability,simpl,simple,2774,"ly writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:3854,testability,simpl,simply,3854,"nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4415,testability,simpl,simply,4415,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4530,testability,test,tests,4530,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4892,testability,test,test,4892,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4918,testability,log,logreg,4918,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4966,testability,depend,dependency,4966,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:728,usability,learn,learn,728,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:803,usability,user,user,803,"Sounds great! Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1177,usability,efficien,efficient,1177,": If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dict",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1282,usability,simpl,simply,1282,"olution? > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1518,usability,command,command,1518,"` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there? **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2080,usability,memor,memory,2080,"ade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2354,usability,user,user,2354,"ught that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then h",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2383,usability,visual,visualizations,2383,"mpression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:2774,usability,simpl,simple,2774,"ly writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:3636,usability,efficien,efficient,3636,"e`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I do",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:3854,usability,simpl,simply,3854,"nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:3892,usability,user,users,3892,"dle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t tes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4095,usability,clear,clear,4095,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4415,usability,simpl,simply,4415,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4724,usability,minim,minimal,4724,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:4912,usability,learn,learn,4912,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:448,deployability,modul,modules,448,"I really like the `sc.extract` idea (or `sc.cast`/`sc.object`). It would be pretty cool if that went both ways though. For example, if I want to generate differential expression results with `diffxpy`, store everything in my `AnnData` object, and visualize later with `sc.pl.rank_genes_groups_violin()`, this could be done by some kind of `sc.read.diffxpy()` function. Maybe you could just extend `sc.read` and `sc.write` and make them into larger modules? Or rename both to `sc.io`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:83,energy efficiency,cool,cool,83,"I really like the `sc.extract` idea (or `sc.cast`/`sc.object`). It would be pretty cool if that went both ways though. For example, if I want to generate differential expression results with `diffxpy`, store everything in my `AnnData` object, and visualize later with `sc.pl.rank_genes_groups_violin()`, this could be done by some kind of `sc.read.diffxpy()` function. Maybe you could just extend `sc.read` and `sc.write` and make them into larger modules? Or rename both to `sc.io`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:390,modifiability,exten,extend,390,"I really like the `sc.extract` idea (or `sc.cast`/`sc.object`). It would be pretty cool if that went both ways though. For example, if I want to generate differential expression results with `diffxpy`, store everything in my `AnnData` object, and visualize later with `sc.pl.rank_genes_groups_violin()`, this could be done by some kind of `sc.read.diffxpy()` function. Maybe you could just extend `sc.read` and `sc.write` and make them into larger modules? Or rename both to `sc.io`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:448,modifiability,modul,modules,448,"I really like the `sc.extract` idea (or `sc.cast`/`sc.object`). It would be pretty cool if that went both ways though. For example, if I want to generate differential expression results with `diffxpy`, store everything in my `AnnData` object, and visualize later with `sc.pl.rank_genes_groups_violin()`, this could be done by some kind of `sc.read.diffxpy()` function. Maybe you could just extend `sc.read` and `sc.write` and make them into larger modules? Or rename both to `sc.io`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:448,safety,modul,modules,448,"I really like the `sc.extract` idea (or `sc.cast`/`sc.object`). It would be pretty cool if that went both ways though. For example, if I want to generate differential expression results with `diffxpy`, store everything in my `AnnData` object, and visualize later with `sc.pl.rank_genes_groups_violin()`, this could be done by some kind of `sc.read.diffxpy()` function. Maybe you could just extend `sc.read` and `sc.write` and make them into larger modules? Or rename both to `sc.io`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:247,usability,visual,visualize,247,"I really like the `sc.extract` idea (or `sc.cast`/`sc.object`). It would be pretty cool if that went both ways though. For example, if I want to generate differential expression results with `diffxpy`, store everything in my `AnnData` object, and visualize later with `sc.pl.rank_genes_groups_violin()`, this could be done by some kind of `sc.read.diffxpy()` function. Maybe you could just extend `sc.read` and `sc.write` and make them into larger modules? Or rename both to `sc.io`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:45,deployability,modul,module,45,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:259,deployability,build,build,259,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:566,deployability,continu,continue,566,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:630,energy efficiency,current,current,630,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:1070,integrability,sub,subset,1070,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:715,interoperability,standard,standard,715,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:852,interoperability,specif,specific,852,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:45,modifiability,modul,module,45,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:861,modifiability,pac,package,861,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:608,performance,disk,disk,608,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:978,performance,disk,disk,978,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:45,safety,modul,module,45,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:819,safety,compl,complicated,819,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:35,security,access,accessors,35,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:819,security,compl,complicated,819,"I like the idea for the ""nice data accessors module""! Maybe `sc.get`? `sc.get.obs_values(adata, ...)`, `sc.get.neighbors(adata, ...)`. We'd definitely have to be sure we're returning a nice object. | If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution? I think this would involve throwing away recarrays, unless someone wants to write a converter (not me). I'm also not so sure how mature/ stable `diffxpy` is, but Theis lab people might have a better sense of that? | That being said: it's likely that we'll continue to choose representations for on-disk. I like that the current representations are pretty easy to read in other languages as they're mostly standard hdf5 types. I think there are definitely cases where it make sense to break cross-compat, like complicated datastructures for a specific package (an index, for example). | If one uses xarray or dataframes, one has to think about how this gets written to disk. My impression is `xarray` were designed to be similar to `netCDF` files, which [are a subset of hdf5](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#How-can-I-convert-netCDF-4-files-into-HDF5-files). `pandas`, on the other hand, has a pretty opaque `hdf5` representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:295,energy efficiency,power,powerful,295,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:504,integrability,sub,subset,504,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:175,modifiability,reu,reused,175,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:820,performance,disk,disk,820,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:874,performance,perform,performance,874,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:598,reliability,doe,does,598,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:137,testability,simpl,simple,137,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:356,testability,context,context,356,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:369,testability,simpl,simple,369,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:137,usability,simpl,simple,137,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:369,usability,simpl,simple,369,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:874,usability,perform,performance,874,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:67,interoperability,format,formats,67,xarray doesn't do sparse :(. They're also holding off for csd/ csf formats in pydata/sparse I believe.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:7,reliability,doe,doesn,7,xarray doesn't do sparse :(. They're also holding off for csd/ csf formats in pydata/sparse I believe.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:264,deployability,API,API,264,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:354,deployability,contain,container,354,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:477,deployability,API,API,477,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:510,deployability,API,API,510,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:527,deployability,API,API,527,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:199,integrability,Sub,Subclass,199,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:264,integrability,API,API,264,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:322,integrability,sub,subclass,322,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:477,integrability,API,API,477,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:510,integrability,API,API,510,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:527,integrability,API,API,527,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:264,interoperability,API,API,264,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:477,interoperability,API,API,477,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:501,interoperability,specif,specific,501,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:510,interoperability,API,API,510,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:527,interoperability,API,API,527,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:406,modifiability,extens,extensibility,406,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:234,security,access,accessor,234,"OK, we have those alternatives:. Alternative | Pro | Con. ---|---|---. Keep everything as it is | People will have the best unterstanding of its structure and not treat it as a black box | Unwieldy. Subclass AnnData in scanpy and add accessor methods/attrs | Nice API | <ul><li>Everyone would start using Scanpy’s AnnData subclass instead of the generic container that I think is a great design choice for extensibility<li>Hides AnnData structure</ul>. `sc.get` | <ul><li>Nice API<li>Separates Scanpy-specific API from AnnData API</ul> | Hides AnnData structure. I think `sc.get` is the best option here!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/563:91,availability,sla,slashes,91,"> it is trying to use linux formatting on a Windows machine. that’s not the issue, forward slashes and relative paths work perfectly on windows. The issue is that this code uses string manipulation to work with paths, which is error-prone. https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/readwrite.py#L440-L441. @falexwolf using `pathlib` for path manipulation as much as possible protects us from mistakes like this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:227,availability,error,error-prone,227,"> it is trying to use linux formatting on a Windows machine. that’s not the issue, forward slashes and relative paths work perfectly on windows. The issue is that this code uses string manipulation to work with paths, which is error-prone. https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/readwrite.py#L440-L441. @falexwolf using `pathlib` for path manipulation as much as possible protects us from mistakes like this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:28,interoperability,format,formatting,28,"> it is trying to use linux formatting on a Windows machine. that’s not the issue, forward slashes and relative paths work perfectly on windows. The issue is that this code uses string manipulation to work with paths, which is error-prone. https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/readwrite.py#L440-L441. @falexwolf using `pathlib` for path manipulation as much as possible protects us from mistakes like this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:227,performance,error,error-prone,227,"> it is trying to use linux formatting on a Windows machine. that’s not the issue, forward slashes and relative paths work perfectly on windows. The issue is that this code uses string manipulation to work with paths, which is error-prone. https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/readwrite.py#L440-L441. @falexwolf using `pathlib` for path manipulation as much as possible protects us from mistakes like this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:91,reliability,sla,slashes,91,"> it is trying to use linux formatting on a Windows machine. that’s not the issue, forward slashes and relative paths work perfectly on windows. The issue is that this code uses string manipulation to work with paths, which is error-prone. https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/readwrite.py#L440-L441. @falexwolf using `pathlib` for path manipulation as much as possible protects us from mistakes like this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:227,safety,error,error-prone,227,"> it is trying to use linux formatting on a Windows machine. that’s not the issue, forward slashes and relative paths work perfectly on windows. The issue is that this code uses string manipulation to work with paths, which is error-prone. https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/readwrite.py#L440-L441. @falexwolf using `pathlib` for path manipulation as much as possible protects us from mistakes like this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:227,usability,error,error-prone,227,"> it is trying to use linux formatting on a Windows machine. that’s not the issue, forward slashes and relative paths work perfectly on windows. The issue is that this code uses string manipulation to work with paths, which is error-prone. https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/readwrite.py#L440-L441. @falexwolf using `pathlib` for path manipulation as much as possible protects us from mistakes like this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/565:74,availability,avail,available,74,"This feature is still in the development version of scanpy, therefore not available in the released scanpy version yet. See https://github.com/theislab/scanpy/issues/560 for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:41,deployability,version,version,41,"This feature is still in the development version of scanpy, therefore not available in the released scanpy version yet. See https://github.com/theislab/scanpy/issues/560 for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:91,deployability,releas,released,91,"This feature is still in the development version of scanpy, therefore not available in the released scanpy version yet. See https://github.com/theislab/scanpy/issues/560 for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:107,deployability,version,version,107,"This feature is still in the development version of scanpy, therefore not available in the released scanpy version yet. See https://github.com/theislab/scanpy/issues/560 for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:41,integrability,version,version,41,"This feature is still in the development version of scanpy, therefore not available in the released scanpy version yet. See https://github.com/theislab/scanpy/issues/560 for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:107,integrability,version,version,107,"This feature is still in the development version of scanpy, therefore not available in the released scanpy version yet. See https://github.com/theislab/scanpy/issues/560 for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:41,modifiability,version,version,41,"This feature is still in the development version of scanpy, therefore not available in the released scanpy version yet. See https://github.com/theislab/scanpy/issues/560 for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:107,modifiability,version,version,107,"This feature is still in the development version of scanpy, therefore not available in the released scanpy version yet. See https://github.com/theislab/scanpy/issues/560 for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:74,reliability,availab,available,74,"This feature is still in the development version of scanpy, therefore not available in the released scanpy version yet. See https://github.com/theislab/scanpy/issues/560 for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:74,safety,avail,available,74,"This feature is still in the development version of scanpy, therefore not available in the released scanpy version yet. See https://github.com/theislab/scanpy/issues/560 for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:74,security,availab,available,74,"This feature is still in the development version of scanpy, therefore not available in the released scanpy version yet. See https://github.com/theislab/scanpy/issues/560 for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/566:73,availability,error,error,73,"```py. >>> from math import sqrt . >>> sqrt(-1). ValueError: math domain error. ```. I assume it’s the square root throwing this. Assuming that it only happens when you pass a negative argument, the term inside can only become negative if `ns[imask] < 0` or `ns[imask] > n_cells`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:73,performance,error,error,73,"```py. >>> from math import sqrt . >>> sqrt(-1). ValueError: math domain error. ```. I assume it’s the square root throwing this. Assuming that it only happens when you pass a negative argument, the term inside can only become negative if `ns[imask] < 0` or `ns[imask] > n_cells`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:73,safety,error,error,73,"```py. >>> from math import sqrt . >>> sqrt(-1). ValueError: math domain error. ```. I assume it’s the square root throwing this. Assuming that it only happens when you pass a negative argument, the term inside can only become negative if `ns[imask] < 0` or `ns[imask] > n_cells`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:73,usability,error,error,73,"```py. >>> from math import sqrt . >>> sqrt(-1). ValueError: math domain error. ```. I assume it’s the square root throwing this. Assuming that it only happens when you pass a negative argument, the term inside can only become negative if `ns[imask] < 0` or `ns[imask] > n_cells`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:160,availability,error,error,160,"Hey,. So I don't understand how I can get around this issue with the wilcoxon test. I'm following the scanpy tutorial and getting this 'ValueError: math domain error'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:160,performance,error,error,160,"Hey,. So I don't understand how I can get around this issue with the wilcoxon test. I'm following the scanpy tutorial and getting this 'ValueError: math domain error'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:78,safety,test,test,78,"Hey,. So I don't understand how I can get around this issue with the wilcoxon test. I'm following the scanpy tutorial and getting this 'ValueError: math domain error'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:160,safety,error,error,160,"Hey,. So I don't understand how I can get around this issue with the wilcoxon test. I'm following the scanpy tutorial and getting this 'ValueError: math domain error'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:17,testability,understand,understand,17,"Hey,. So I don't understand how I can get around this issue with the wilcoxon test. I'm following the scanpy tutorial and getting this 'ValueError: math domain error'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:78,testability,test,test,78,"Hey,. So I don't understand how I can get around this issue with the wilcoxon test. I'm following the scanpy tutorial and getting this 'ValueError: math domain error'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:160,usability,error,error,160,"Hey,. So I don't understand how I can get around this issue with the wilcoxon test. I'm following the scanpy tutorial and getting this 'ValueError: math domain error'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:35,availability,error,error,35,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:831,availability,error,error,831,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:208,deployability,modul,module,208,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:208,modifiability,modul,module,208,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:360,modifiability,pac,packages,360,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:528,modifiability,layer,layer,528,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:35,performance,error,error,35,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:831,performance,error,error,831,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:35,safety,error,error,35,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:180,safety,input,input-,180,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:208,safety,modul,module,208,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:831,safety,error,error,831,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:134,testability,Trace,Traceback,134,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:35,usability,error,error,35,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:180,usability,input,input-,180,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:376,usability,tool,tools,376,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:831,usability,error,error,831,"Hi, I am also still receiving this error! > ---------------------------------------------------------------------------. > ValueError Traceback (most recent call last). > <ipython-input-141-e471c5e20fbd> in <module>. > ----> 1 sc.tl.rank_genes_groups(adata, 'louvain_05',n_genes=100,method=""wilcoxon"",use_raw=False). > . > e:\programs\python\python38\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, layer, **kwds). > 398 mean_rest, var_rest = _get_mean_var(X[mask_rest]). > 399 . > --> 400 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. > 401 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). > 402 scores[np.isnan(scores)] = 0. > . > ValueError: math domain error. How can I deal with it? I though it was a bug that is fixed now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/567:171,availability,error,error,171,"Not sure we can do a lot with just a traceback. Could you create a [minimal example](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) to reproduce this error, and describe what you were trying to do?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:171,performance,error,error,171,"Not sure we can do a lot with just a traceback. Could you create a [minimal example](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) to reproduce this error, and describe what you were trying to do?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:171,safety,error,error,171,"Not sure we can do a lot with just a traceback. Could you create a [minimal example](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) to reproduce this error, and describe what you were trying to do?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:37,testability,trace,traceback,37,"Not sure we can do a lot with just a traceback. Could you create a [minimal example](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) to reproduce this error, and describe what you were trying to do?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:68,usability,minim,minimal,68,"Not sure we can do a lot with just a traceback. Could you create a [minimal example](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) to reproduce this error, and describe what you were trying to do?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:132,usability,minim,minimal-bug-reports,132,"Not sure we can do a lot with just a traceback. Could you create a [minimal example](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) to reproduce this error, and describe what you were trying to do?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:171,usability,error,error,171,"Not sure we can do a lot with just a traceback. Could you create a [minimal example](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) to reproduce this error, and describe what you were trying to do?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:293,availability,error,errors,293,"Hi. Please see below for the minimal script used. Thanks! ```python. import numpy as np. import pandas as pd. import scanpy as sc. import scvelo as scv. import matplotlib.pyplot as plt. from pathlib import Path. scv.settings.set_figure_params('scvelo'). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.settings.autosave = True. sc.settings.autoshow = False. sc.set_figure_params(scanpy=True, dpi=80, dpi_save=160, frameon=False, vector_friendly=False, format='pdf'). adata = sc.read_loom(""./Velocyto_comb.loom""). sc.pp.filter_cells(adata, min_genes=1000). sc.pp.filter_genes(adata, min_cells=10). print('\nDoing initial filtering...\nKeeping', len(adata.obs_names), 'cells and', len(adata.var_names), 'genes.\n'). mito_genes = adata.var_names.str.startswith('MT-'). # Calculate the percent of genes derived from mito vs genome. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. 	adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],. 			 jitter=0.4, multi_panel=True, save = '_preFiltering_plot.pdf', show = False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:1099,deployability,observ,observations-annotation,1099,"Hi. Please see below for the minimal script used. Thanks! ```python. import numpy as np. import pandas as pd. import scanpy as sc. import scvelo as scv. import matplotlib.pyplot as plt. from pathlib import Path. scv.settings.set_figure_params('scvelo'). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.settings.autosave = True. sc.settings.autoshow = False. sc.set_figure_params(scanpy=True, dpi=80, dpi_save=160, frameon=False, vector_friendly=False, format='pdf'). adata = sc.read_loom(""./Velocyto_comb.loom""). sc.pp.filter_cells(adata, min_genes=1000). sc.pp.filter_genes(adata, min_cells=10). print('\nDoing initial filtering...\nKeeping', len(adata.obs_names), 'cells and', len(adata.var_names), 'genes.\n'). mito_genes = adata.var_names.str.startswith('MT-'). # Calculate the percent of genes derived from mito vs genome. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. 	adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],. 			 jitter=0.4, multi_panel=True, save = '_preFiltering_plot.pdf', show = False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:661,integrability,filter,filtering,661,"Hi. Please see below for the minimal script used. Thanks! ```python. import numpy as np. import pandas as pd. import scanpy as sc. import scvelo as scv. import matplotlib.pyplot as plt. from pathlib import Path. scv.settings.set_figure_params('scvelo'). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.settings.autosave = True. sc.settings.autoshow = False. sc.set_figure_params(scanpy=True, dpi=80, dpi_save=160, frameon=False, vector_friendly=False, format='pdf'). adata = sc.read_loom(""./Velocyto_comb.loom""). sc.pp.filter_cells(adata, min_genes=1000). sc.pp.filter_genes(adata, min_cells=10). print('\nDoing initial filtering...\nKeeping', len(adata.obs_names), 'cells and', len(adata.var_names), 'genes.\n'). mito_genes = adata.var_names.str.startswith('MT-'). # Calculate the percent of genes derived from mito vs genome. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. 	adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],. 			 jitter=0.4, multi_panel=True, save = '_preFiltering_plot.pdf', show = False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:918,integrability,transform,transform,918,"Hi. Please see below for the minimal script used. Thanks! ```python. import numpy as np. import pandas as pd. import scanpy as sc. import scvelo as scv. import matplotlib.pyplot as plt. from pathlib import Path. scv.settings.set_figure_params('scvelo'). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.settings.autosave = True. sc.settings.autoshow = False. sc.set_figure_params(scanpy=True, dpi=80, dpi_save=160, frameon=False, vector_friendly=False, format='pdf'). adata = sc.read_loom(""./Velocyto_comb.loom""). sc.pp.filter_cells(adata, min_genes=1000). sc.pp.filter_genes(adata, min_cells=10). print('\nDoing initial filtering...\nKeeping', len(adata.obs_names), 'cells and', len(adata.var_names), 'genes.\n'). mito_genes = adata.var_names.str.startswith('MT-'). # Calculate the percent of genes derived from mito vs genome. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. 	adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],. 			 jitter=0.4, multi_panel=True, save = '_preFiltering_plot.pdf', show = False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:493,interoperability,format,format,493,"Hi. Please see below for the minimal script used. Thanks! ```python. import numpy as np. import pandas as pd. import scanpy as sc. import scvelo as scv. import matplotlib.pyplot as plt. from pathlib import Path. scv.settings.set_figure_params('scvelo'). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.settings.autosave = True. sc.settings.autoshow = False. sc.set_figure_params(scanpy=True, dpi=80, dpi_save=160, frameon=False, vector_friendly=False, format='pdf'). adata = sc.read_loom(""./Velocyto_comb.loom""). sc.pp.filter_cells(adata, min_genes=1000). sc.pp.filter_genes(adata, min_cells=10). print('\nDoing initial filtering...\nKeeping', len(adata.obs_names), 'cells and', len(adata.var_names), 'genes.\n'). mito_genes = adata.var_names.str.startswith('MT-'). # Calculate the percent of genes derived from mito vs genome. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. 	adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],. 			 jitter=0.4, multi_panel=True, save = '_preFiltering_plot.pdf', show = False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:918,interoperability,transform,transform,918,"Hi. Please see below for the minimal script used. Thanks! ```python. import numpy as np. import pandas as pd. import scanpy as sc. import scvelo as scv. import matplotlib.pyplot as plt. from pathlib import Path. scv.settings.set_figure_params('scvelo'). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.settings.autosave = True. sc.settings.autoshow = False. sc.set_figure_params(scanpy=True, dpi=80, dpi_save=160, frameon=False, vector_friendly=False, format='pdf'). adata = sc.read_loom(""./Velocyto_comb.loom""). sc.pp.filter_cells(adata, min_genes=1000). sc.pp.filter_genes(adata, min_cells=10). print('\nDoing initial filtering...\nKeeping', len(adata.obs_names), 'cells and', len(adata.var_names), 'genes.\n'). mito_genes = adata.var_names.str.startswith('MT-'). # Calculate the percent of genes derived from mito vs genome. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. 	adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],. 			 jitter=0.4, multi_panel=True, save = '_preFiltering_plot.pdf', show = False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:293,performance,error,errors,293,"Hi. Please see below for the minimal script used. Thanks! ```python. import numpy as np. import pandas as pd. import scanpy as sc. import scvelo as scv. import matplotlib.pyplot as plt. from pathlib import Path. scv.settings.set_figure_params('scvelo'). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.settings.autosave = True. sc.settings.autoshow = False. sc.set_figure_params(scanpy=True, dpi=80, dpi_save=160, frameon=False, vector_friendly=False, format='pdf'). adata = sc.read_loom(""./Velocyto_comb.loom""). sc.pp.filter_cells(adata, min_genes=1000). sc.pp.filter_genes(adata, min_cells=10). print('\nDoing initial filtering...\nKeeping', len(adata.obs_names), 'cells and', len(adata.var_names), 'genes.\n'). mito_genes = adata.var_names.str.startswith('MT-'). # Calculate the percent of genes derived from mito vs genome. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. 	adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],. 			 jitter=0.4, multi_panel=True, save = '_preFiltering_plot.pdf', show = False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:293,safety,error,errors,293,"Hi. Please see below for the minimal script used. Thanks! ```python. import numpy as np. import pandas as pd. import scanpy as sc. import scvelo as scv. import matplotlib.pyplot as plt. from pathlib import Path. scv.settings.set_figure_params('scvelo'). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.settings.autosave = True. sc.settings.autoshow = False. sc.set_figure_params(scanpy=True, dpi=80, dpi_save=160, frameon=False, vector_friendly=False, format='pdf'). adata = sc.read_loom(""./Velocyto_comb.loom""). sc.pp.filter_cells(adata, min_genes=1000). sc.pp.filter_genes(adata, min_cells=10). print('\nDoing initial filtering...\nKeeping', len(adata.obs_names), 'cells and', len(adata.var_names), 'genes.\n'). mito_genes = adata.var_names.str.startswith('MT-'). # Calculate the percent of genes derived from mito vs genome. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. 	adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],. 			 jitter=0.4, multi_panel=True, save = '_preFiltering_plot.pdf', show = False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:1099,testability,observ,observations-annotation,1099,"Hi. Please see below for the minimal script used. Thanks! ```python. import numpy as np. import pandas as pd. import scanpy as sc. import scvelo as scv. import matplotlib.pyplot as plt. from pathlib import Path. scv.settings.set_figure_params('scvelo'). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.settings.autosave = True. sc.settings.autoshow = False. sc.set_figure_params(scanpy=True, dpi=80, dpi_save=160, frameon=False, vector_friendly=False, format='pdf'). adata = sc.read_loom(""./Velocyto_comb.loom""). sc.pp.filter_cells(adata, min_genes=1000). sc.pp.filter_genes(adata, min_cells=10). print('\nDoing initial filtering...\nKeeping', len(adata.obs_names), 'cells and', len(adata.var_names), 'genes.\n'). mito_genes = adata.var_names.str.startswith('MT-'). # Calculate the percent of genes derived from mito vs genome. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. 	adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],. 			 jitter=0.4, multi_panel=True, save = '_preFiltering_plot.pdf', show = False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:29,usability,minim,minimal,29,"Hi. Please see below for the minimal script used. Thanks! ```python. import numpy as np. import pandas as pd. import scanpy as sc. import scvelo as scv. import matplotlib.pyplot as plt. from pathlib import Path. scv.settings.set_figure_params('scvelo'). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.settings.autosave = True. sc.settings.autoshow = False. sc.set_figure_params(scanpy=True, dpi=80, dpi_save=160, frameon=False, vector_friendly=False, format='pdf'). adata = sc.read_loom(""./Velocyto_comb.loom""). sc.pp.filter_cells(adata, min_genes=1000). sc.pp.filter_genes(adata, min_cells=10). print('\nDoing initial filtering...\nKeeping', len(adata.obs_names), 'cells and', len(adata.var_names), 'genes.\n'). mito_genes = adata.var_names.str.startswith('MT-'). # Calculate the percent of genes derived from mito vs genome. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. 	adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],. 			 jitter=0.4, multi_panel=True, save = '_preFiltering_plot.pdf', show = False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:293,usability,error,errors,293,"Hi. Please see below for the minimal script used. Thanks! ```python. import numpy as np. import pandas as pd. import scanpy as sc. import scvelo as scv. import matplotlib.pyplot as plt. from pathlib import Path. scv.settings.set_figure_params('scvelo'). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.settings.autosave = True. sc.settings.autoshow = False. sc.set_figure_params(scanpy=True, dpi=80, dpi_save=160, frameon=False, vector_friendly=False, format='pdf'). adata = sc.read_loom(""./Velocyto_comb.loom""). sc.pp.filter_cells(adata, min_genes=1000). sc.pp.filter_genes(adata, min_cells=10). print('\nDoing initial filtering...\nKeeping', len(adata.obs_names), 'cells and', len(adata.var_names), 'genes.\n'). mito_genes = adata.var_names.str.startswith('MT-'). # Calculate the percent of genes derived from mito vs genome. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. 	adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],. 			 jitter=0.4, multi_panel=True, save = '_preFiltering_plot.pdf', show = False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:329,usability,hint,hints,329,"Hi. Please see below for the minimal script used. Thanks! ```python. import numpy as np. import pandas as pd. import scanpy as sc. import scvelo as scv. import matplotlib.pyplot as plt. from pathlib import Path. scv.settings.set_figure_params('scvelo'). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.settings.autosave = True. sc.settings.autoshow = False. sc.set_figure_params(scanpy=True, dpi=80, dpi_save=160, frameon=False, vector_friendly=False, format='pdf'). adata = sc.read_loom(""./Velocyto_comb.loom""). sc.pp.filter_cells(adata, min_genes=1000). sc.pp.filter_genes(adata, min_cells=10). print('\nDoing initial filtering...\nKeeping', len(adata.obs_names), 'cells and', len(adata.var_names), 'genes.\n'). mito_genes = adata.var_names.str.startswith('MT-'). # Calculate the percent of genes derived from mito vs genome. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. 	adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],. 			 jitter=0.4, multi_panel=True, save = '_preFiltering_plot.pdf', show = False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:194,availability,error,error,194,"Would you mind reading through the link I sent and cutting this back? This doesn't fit the ""reproducible"" criteria (I don't have that data file), and I'm not sure which line actually causes the error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:194,performance,error,error,194,"Would you mind reading through the link I sent and cutting this back? This doesn't fit the ""reproducible"" criteria (I don't have that data file), and I'm not sure which line actually causes the error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:75,reliability,doe,doesn,75,"Would you mind reading through the link I sent and cutting this back? This doesn't fit the ""reproducible"" criteria (I don't have that data file), and I'm not sure which line actually causes the error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:194,safety,error,error,194,"Would you mind reading through the link I sent and cutting this back? This doesn't fit the ""reproducible"" criteria (I don't have that data file), and I'm not sure which line actually causes the error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:194,usability,error,error,194,"Would you mind reading through the link I sent and cutting this back? This doesn't fit the ""reproducible"" criteria (I don't have that data file), and I'm not sure which line actually causes the error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:218,availability,error,error,218,"Actually, no need to post more. A labmate managed to find this too. It looks like it has something to do with Matplotlib and the `TkAgg` backend (see: https://github.com/matplotlib/matplotlib/issues/13414). While this error doesn't occur in my normal environment, I can reproduce this error in a conda environment:. ```sh. conda create -yn conda_scanpy scanpy. conda activate conda_scanpy. python -c ""import matplotlib.pyplot as plt; plt.figure()"". ```. We were able to get plotting to work by switching to the `Agg` backend. You can do this by adding the following lines to the top of your script:. ```python. import matplotlib as mpl. mpl.use(""Agg""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:285,availability,error,error,285,"Actually, no need to post more. A labmate managed to find this too. It looks like it has something to do with Matplotlib and the `TkAgg` backend (see: https://github.com/matplotlib/matplotlib/issues/13414). While this error doesn't occur in my normal environment, I can reproduce this error in a conda environment:. ```sh. conda create -yn conda_scanpy scanpy. conda activate conda_scanpy. python -c ""import matplotlib.pyplot as plt; plt.figure()"". ```. We were able to get plotting to work by switching to the `Agg` backend. You can do this by adding the following lines to the top of your script:. ```python. import matplotlib as mpl. mpl.use(""Agg""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:42,deployability,manag,managed,42,"Actually, no need to post more. A labmate managed to find this too. It looks like it has something to do with Matplotlib and the `TkAgg` backend (see: https://github.com/matplotlib/matplotlib/issues/13414). While this error doesn't occur in my normal environment, I can reproduce this error in a conda environment:. ```sh. conda create -yn conda_scanpy scanpy. conda activate conda_scanpy. python -c ""import matplotlib.pyplot as plt; plt.figure()"". ```. We were able to get plotting to work by switching to the `Agg` backend. You can do this by adding the following lines to the top of your script:. ```python. import matplotlib as mpl. mpl.use(""Agg""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:42,energy efficiency,manag,managed,42,"Actually, no need to post more. A labmate managed to find this too. It looks like it has something to do with Matplotlib and the `TkAgg` backend (see: https://github.com/matplotlib/matplotlib/issues/13414). While this error doesn't occur in my normal environment, I can reproduce this error in a conda environment:. ```sh. conda create -yn conda_scanpy scanpy. conda activate conda_scanpy. python -c ""import matplotlib.pyplot as plt; plt.figure()"". ```. We were able to get plotting to work by switching to the `Agg` backend. You can do this by adding the following lines to the top of your script:. ```python. import matplotlib as mpl. mpl.use(""Agg""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:218,performance,error,error,218,"Actually, no need to post more. A labmate managed to find this too. It looks like it has something to do with Matplotlib and the `TkAgg` backend (see: https://github.com/matplotlib/matplotlib/issues/13414). While this error doesn't occur in my normal environment, I can reproduce this error in a conda environment:. ```sh. conda create -yn conda_scanpy scanpy. conda activate conda_scanpy. python -c ""import matplotlib.pyplot as plt; plt.figure()"". ```. We were able to get plotting to work by switching to the `Agg` backend. You can do this by adding the following lines to the top of your script:. ```python. import matplotlib as mpl. mpl.use(""Agg""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:285,performance,error,error,285,"Actually, no need to post more. A labmate managed to find this too. It looks like it has something to do with Matplotlib and the `TkAgg` backend (see: https://github.com/matplotlib/matplotlib/issues/13414). While this error doesn't occur in my normal environment, I can reproduce this error in a conda environment:. ```sh. conda create -yn conda_scanpy scanpy. conda activate conda_scanpy. python -c ""import matplotlib.pyplot as plt; plt.figure()"". ```. We were able to get plotting to work by switching to the `Agg` backend. You can do this by adding the following lines to the top of your script:. ```python. import matplotlib as mpl. mpl.use(""Agg""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:224,reliability,doe,doesn,224,"Actually, no need to post more. A labmate managed to find this too. It looks like it has something to do with Matplotlib and the `TkAgg` backend (see: https://github.com/matplotlib/matplotlib/issues/13414). While this error doesn't occur in my normal environment, I can reproduce this error in a conda environment:. ```sh. conda create -yn conda_scanpy scanpy. conda activate conda_scanpy. python -c ""import matplotlib.pyplot as plt; plt.figure()"". ```. We were able to get plotting to work by switching to the `Agg` backend. You can do this by adding the following lines to the top of your script:. ```python. import matplotlib as mpl. mpl.use(""Agg""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:42,safety,manag,managed,42,"Actually, no need to post more. A labmate managed to find this too. It looks like it has something to do with Matplotlib and the `TkAgg` backend (see: https://github.com/matplotlib/matplotlib/issues/13414). While this error doesn't occur in my normal environment, I can reproduce this error in a conda environment:. ```sh. conda create -yn conda_scanpy scanpy. conda activate conda_scanpy. python -c ""import matplotlib.pyplot as plt; plt.figure()"". ```. We were able to get plotting to work by switching to the `Agg` backend. You can do this by adding the following lines to the top of your script:. ```python. import matplotlib as mpl. mpl.use(""Agg""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:218,safety,error,error,218,"Actually, no need to post more. A labmate managed to find this too. It looks like it has something to do with Matplotlib and the `TkAgg` backend (see: https://github.com/matplotlib/matplotlib/issues/13414). While this error doesn't occur in my normal environment, I can reproduce this error in a conda environment:. ```sh. conda create -yn conda_scanpy scanpy. conda activate conda_scanpy. python -c ""import matplotlib.pyplot as plt; plt.figure()"". ```. We were able to get plotting to work by switching to the `Agg` backend. You can do this by adding the following lines to the top of your script:. ```python. import matplotlib as mpl. mpl.use(""Agg""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:285,safety,error,error,285,"Actually, no need to post more. A labmate managed to find this too. It looks like it has something to do with Matplotlib and the `TkAgg` backend (see: https://github.com/matplotlib/matplotlib/issues/13414). While this error doesn't occur in my normal environment, I can reproduce this error in a conda environment:. ```sh. conda create -yn conda_scanpy scanpy. conda activate conda_scanpy. python -c ""import matplotlib.pyplot as plt; plt.figure()"". ```. We were able to get plotting to work by switching to the `Agg` backend. You can do this by adding the following lines to the top of your script:. ```python. import matplotlib as mpl. mpl.use(""Agg""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:218,usability,error,error,218,"Actually, no need to post more. A labmate managed to find this too. It looks like it has something to do with Matplotlib and the `TkAgg` backend (see: https://github.com/matplotlib/matplotlib/issues/13414). While this error doesn't occur in my normal environment, I can reproduce this error in a conda environment:. ```sh. conda create -yn conda_scanpy scanpy. conda activate conda_scanpy. python -c ""import matplotlib.pyplot as plt; plt.figure()"". ```. We were able to get plotting to work by switching to the `Agg` backend. You can do this by adding the following lines to the top of your script:. ```python. import matplotlib as mpl. mpl.use(""Agg""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:285,usability,error,error,285,"Actually, no need to post more. A labmate managed to find this too. It looks like it has something to do with Matplotlib and the `TkAgg` backend (see: https://github.com/matplotlib/matplotlib/issues/13414). While this error doesn't occur in my normal environment, I can reproduce this error in a conda environment:. ```sh. conda create -yn conda_scanpy scanpy. conda activate conda_scanpy. python -c ""import matplotlib.pyplot as plt; plt.figure()"". ```. We were able to get plotting to work by switching to the `Agg` backend. You can do this by adding the following lines to the top of your script:. ```python. import matplotlib as mpl. mpl.use(""Agg""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:59,availability,error,error,59,@kt6k were you using a conda environment when you hit this error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:59,performance,error,error,59,@kt6k were you using a conda environment when you hit this error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:59,safety,error,error,59,@kt6k were you using a conda environment when you hit this error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:59,usability,error,error,59,@kt6k were you using a conda environment when you hit this error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:287,integrability,filter,filtered,287,"Hi ivisrup. This may already have been discussed elsewhere (or I could open a new issue)... It appears that I am loosing a bunch of genes/transcripts during the Velocyto analysis, and I am trying to figure out what is going on. After running Velocyto, there are approximately 5000 final filtered transcripts (presumably ones used for the vector calculation). However, several genes/transcripts of interest that should be highly expressed in most cells (seen using software like Seurat) are not listed in the final filtered genes. Thus, I am suspecting that many genes/transcripts are, for some reason, being lost during Velocyto calculation. . Does Velocyto filter out genes that are not differentially expressed (namely, genes/transcripts that are not useful to plot vector)? My cells are captured using Fluidigm. . Could you please (if possible) elaborate on this? Very much appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:514,integrability,filter,filtered,514,"Hi ivisrup. This may already have been discussed elsewhere (or I could open a new issue)... It appears that I am loosing a bunch of genes/transcripts during the Velocyto analysis, and I am trying to figure out what is going on. After running Velocyto, there are approximately 5000 final filtered transcripts (presumably ones used for the vector calculation). However, several genes/transcripts of interest that should be highly expressed in most cells (seen using software like Seurat) are not listed in the final filtered genes. Thus, I am suspecting that many genes/transcripts are, for some reason, being lost during Velocyto calculation. . Does Velocyto filter out genes that are not differentially expressed (namely, genes/transcripts that are not useful to plot vector)? My cells are captured using Fluidigm. . Could you please (if possible) elaborate on this? Very much appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:658,integrability,filter,filter,658,"Hi ivisrup. This may already have been discussed elsewhere (or I could open a new issue)... It appears that I am loosing a bunch of genes/transcripts during the Velocyto analysis, and I am trying to figure out what is going on. After running Velocyto, there are approximately 5000 final filtered transcripts (presumably ones used for the vector calculation). However, several genes/transcripts of interest that should be highly expressed in most cells (seen using software like Seurat) are not listed in the final filtered genes. Thus, I am suspecting that many genes/transcripts are, for some reason, being lost during Velocyto calculation. . Does Velocyto filter out genes that are not differentially expressed (namely, genes/transcripts that are not useful to plot vector)? My cells are captured using Fluidigm. . Could you please (if possible) elaborate on this? Very much appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:644,reliability,Doe,Does,644,"Hi ivisrup. This may already have been discussed elsewhere (or I could open a new issue)... It appears that I am loosing a bunch of genes/transcripts during the Velocyto analysis, and I am trying to figure out what is going on. After running Velocyto, there are approximately 5000 final filtered transcripts (presumably ones used for the vector calculation). However, several genes/transcripts of interest that should be highly expressed in most cells (seen using software like Seurat) are not listed in the final filtered genes. Thus, I am suspecting that many genes/transcripts are, for some reason, being lost during Velocyto calculation. . Does Velocyto filter out genes that are not differentially expressed (namely, genes/transcripts that are not useful to plot vector)? My cells are captured using Fluidigm. . Could you please (if possible) elaborate on this? Very much appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:299,deployability,depend,depending,299,"@kt6k, I think that's worth opening a separate issue for. I'm not too experienced with the velocyto tools, but I suspect the issue might be more appropriate for either [velocyto-team/velocyto.py](https://github.com/velocyto-team/velocyto.py) or [theislab/scvelo](https://github.com/theislab/scvelo) depending on when this filtering is occurring.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:299,integrability,depend,depending,299,"@kt6k, I think that's worth opening a separate issue for. I'm not too experienced with the velocyto tools, but I suspect the issue might be more appropriate for either [velocyto-team/velocyto.py](https://github.com/velocyto-team/velocyto.py) or [theislab/scvelo](https://github.com/theislab/scvelo) depending on when this filtering is occurring.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:322,integrability,filter,filtering,322,"@kt6k, I think that's worth opening a separate issue for. I'm not too experienced with the velocyto tools, but I suspect the issue might be more appropriate for either [velocyto-team/velocyto.py](https://github.com/velocyto-team/velocyto.py) or [theislab/scvelo](https://github.com/theislab/scvelo) depending on when this filtering is occurring.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:299,modifiability,depend,depending,299,"@kt6k, I think that's worth opening a separate issue for. I'm not too experienced with the velocyto tools, but I suspect the issue might be more appropriate for either [velocyto-team/velocyto.py](https://github.com/velocyto-team/velocyto.py) or [theislab/scvelo](https://github.com/theislab/scvelo) depending on when this filtering is occurring.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:299,safety,depend,depending,299,"@kt6k, I think that's worth opening a separate issue for. I'm not too experienced with the velocyto tools, but I suspect the issue might be more appropriate for either [velocyto-team/velocyto.py](https://github.com/velocyto-team/velocyto.py) or [theislab/scvelo](https://github.com/theislab/scvelo) depending on when this filtering is occurring.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:178,security,team,team,178,"@kt6k, I think that's worth opening a separate issue for. I'm not too experienced with the velocyto tools, but I suspect the issue might be more appropriate for either [velocyto-team/velocyto.py](https://github.com/velocyto-team/velocyto.py) or [theislab/scvelo](https://github.com/theislab/scvelo) depending on when this filtering is occurring.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:224,security,team,team,224,"@kt6k, I think that's worth opening a separate issue for. I'm not too experienced with the velocyto tools, but I suspect the issue might be more appropriate for either [velocyto-team/velocyto.py](https://github.com/velocyto-team/velocyto.py) or [theislab/scvelo](https://github.com/theislab/scvelo) depending on when this filtering is occurring.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:299,testability,depend,depending,299,"@kt6k, I think that's worth opening a separate issue for. I'm not too experienced with the velocyto tools, but I suspect the issue might be more appropriate for either [velocyto-team/velocyto.py](https://github.com/velocyto-team/velocyto.py) or [theislab/scvelo](https://github.com/theislab/scvelo) depending on when this filtering is occurring.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:70,usability,experien,experienced,70,"@kt6k, I think that's worth opening a separate issue for. I'm not too experienced with the velocyto tools, but I suspect the issue might be more appropriate for either [velocyto-team/velocyto.py](https://github.com/velocyto-team/velocyto.py) or [theislab/scvelo](https://github.com/theislab/scvelo) depending on when this filtering is occurring.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:100,usability,tool,tools,100,"@kt6k, I think that's worth opening a separate issue for. I'm not too experienced with the velocyto tools, but I suspect the issue might be more appropriate for either [velocyto-team/velocyto.py](https://github.com/velocyto-team/velocyto.py) or [theislab/scvelo](https://github.com/theislab/scvelo) depending on when this filtering is occurring.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/pull/568:130,reliability,doe,doesn,130,Looks good! Additionally we should also add some issue templates (maybe one for bugs and one for feature requests). (But that ofc doesn’t have to be in this PR),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/pull/568:100,reliability,pra,practice,100,"@flying-sheep, while I like the idea of issue templates in theory, I often find them patronizing in practice. Do you have examples of ones that you've liked?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/pull/568:255,security,auth,authored,255,"Those are good! Short and simple. On Sat, Mar 30, 2019 at 9:02 PM Philipp A. <notifications@github.com> wrote:. > The ones from vscode are pretty good:. > https://github.com/Microsoft/vscode/issues/new/choose. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/568#issuecomment-478229664>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AH221DYPmPqIrPkmsxrzuc2eWtN_QRLzks5vbzY9gaJpZM4cRcJj>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/pull/568:470,security,auth,auth,470,"Those are good! Short and simple. On Sat, Mar 30, 2019 at 9:02 PM Philipp A. <notifications@github.com> wrote:. > The ones from vscode are pretty good:. > https://github.com/Microsoft/vscode/issues/new/choose. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/568#issuecomment-478229664>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AH221DYPmPqIrPkmsxrzuc2eWtN_QRLzks5vbzY9gaJpZM4cRcJj>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/pull/568:26,testability,simpl,simple,26,"Those are good! Short and simple. On Sat, Mar 30, 2019 at 9:02 PM Philipp A. <notifications@github.com> wrote:. > The ones from vscode are pretty good:. > https://github.com/Microsoft/vscode/issues/new/choose. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/568#issuecomment-478229664>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AH221DYPmPqIrPkmsxrzuc2eWtN_QRLzks5vbzY9gaJpZM4cRcJj>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/pull/568:26,usability,simpl,simple,26,"Those are good! Short and simple. On Sat, Mar 30, 2019 at 9:02 PM Philipp A. <notifications@github.com> wrote:. > The ones from vscode are pretty good:. > https://github.com/Microsoft/vscode/issues/new/choose. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/568#issuecomment-478229664>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AH221DYPmPqIrPkmsxrzuc2eWtN_QRLzks5vbzY9gaJpZM4cRcJj>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/issues/569:255,availability,cluster,clusters,255,"Can you provide an example that we can test for this? On Fri, Mar 29, 2019 at 8:37 AM jiawen wang <notifications@github.com>. wrote:. > Dear,. > I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of. > top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30. > times slowers than seurat's Doheatmap(). Could you modify it to accelerate. > the process ? >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/569>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VPvaV-CHAOc88jcDpj8iSlwQgqUks5vbcK7gaJpZM4cRzgZ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:276,availability,slo,slowly,276,"Can you provide an example that we can test for this? On Fri, Mar 29, 2019 at 8:37 AM jiawen wang <notifications@github.com>. wrote:. > Dear,. > I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of. > top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30. > times slowers than seurat's Doheatmap(). Could you modify it to accelerate. > the process ? >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/569>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VPvaV-CHAOc88jcDpj8iSlwQgqUks5vbcK7gaJpZM4cRzgZ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:302,availability,slo,slowers,302,"Can you provide an example that we can test for this? On Fri, Mar 29, 2019 at 8:37 AM jiawen wang <notifications@github.com>. wrote:. > Dear,. > I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of. > top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30. > times slowers than seurat's Doheatmap(). Could you modify it to accelerate. > the process ? >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/569>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VPvaV-CHAOc88jcDpj8iSlwQgqUks5vbcK7gaJpZM4cRzgZ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:255,deployability,cluster,clusters,255,"Can you provide an example that we can test for this? On Fri, Mar 29, 2019 at 8:37 AM jiawen wang <notifications@github.com>. wrote:. > Dear,. > I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of. > top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30. > times slowers than seurat's Doheatmap(). Could you modify it to accelerate. > the process ? >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/569>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VPvaV-CHAOc88jcDpj8iSlwQgqUks5vbcK7gaJpZM4cRzgZ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:203,energy efficiency,heat,heatmap,203,"Can you provide an example that we can test for this? On Fri, Mar 29, 2019 at 8:37 AM jiawen wang <notifications@github.com>. wrote:. > Dear,. > I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of. > top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30. > times slowers than seurat's Doheatmap(). Could you modify it to accelerate. > the process ? >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/569>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VPvaV-CHAOc88jcDpj8iSlwQgqUks5vbcK7gaJpZM4cRzgZ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:437,integrability,sub,subscribed,437,"Can you provide an example that we can test for this? On Fri, Mar 29, 2019 at 8:37 AM jiawen wang <notifications@github.com>. wrote:. > Dear,. > I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of. > top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30. > times slowers than seurat's Doheatmap(). Could you modify it to accelerate. > the process ? >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/569>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VPvaV-CHAOc88jcDpj8iSlwQgqUks5vbcK7gaJpZM4cRzgZ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:296,performance,time,times,296,"Can you provide an example that we can test for this? On Fri, Mar 29, 2019 at 8:37 AM jiawen wang <notifications@github.com>. wrote:. > Dear,. > I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of. > top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30. > times slowers than seurat's Doheatmap(). Could you modify it to accelerate. > the process ? >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/569>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VPvaV-CHAOc88jcDpj8iSlwQgqUks5vbcK7gaJpZM4cRzgZ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:276,reliability,slo,slowly,276,"Can you provide an example that we can test for this? On Fri, Mar 29, 2019 at 8:37 AM jiawen wang <notifications@github.com>. wrote:. > Dear,. > I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of. > top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30. > times slowers than seurat's Doheatmap(). Could you modify it to accelerate. > the process ? >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/569>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VPvaV-CHAOc88jcDpj8iSlwQgqUks5vbcK7gaJpZM4cRzgZ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:302,reliability,slo,slowers,302,"Can you provide an example that we can test for this? On Fri, Mar 29, 2019 at 8:37 AM jiawen wang <notifications@github.com>. wrote:. > Dear,. > I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of. > top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30. > times slowers than seurat's Doheatmap(). Could you modify it to accelerate. > the process ? >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/569>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VPvaV-CHAOc88jcDpj8iSlwQgqUks5vbcK7gaJpZM4cRzgZ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:39,safety,test,test,39,"Can you provide an example that we can test for this? On Fri, Mar 29, 2019 at 8:37 AM jiawen wang <notifications@github.com>. wrote:. > Dear,. > I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of. > top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30. > times slowers than seurat's Doheatmap(). Could you modify it to accelerate. > the process ? >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/569>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VPvaV-CHAOc88jcDpj8iSlwQgqUks5vbcK7gaJpZM4cRzgZ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:347,security,modif,modify,347,"Can you provide an example that we can test for this? On Fri, Mar 29, 2019 at 8:37 AM jiawen wang <notifications@github.com>. wrote:. > Dear,. > I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of. > top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30. > times slowers than seurat's Doheatmap(). Could you modify it to accelerate. > the process ? >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/569>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VPvaV-CHAOc88jcDpj8iSlwQgqUks5vbcK7gaJpZM4cRzgZ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:634,security,auth,auth,634,"Can you provide an example that we can test for this? On Fri, Mar 29, 2019 at 8:37 AM jiawen wang <notifications@github.com>. wrote:. > Dear,. > I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of. > top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30. > times slowers than seurat's Doheatmap(). Could you modify it to accelerate. > the process ? >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/569>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VPvaV-CHAOc88jcDpj8iSlwQgqUks5vbcK7gaJpZM4cRzgZ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:39,testability,test,test,39,"Can you provide an example that we can test for this? On Fri, Mar 29, 2019 at 8:37 AM jiawen wang <notifications@github.com>. wrote:. > Dear,. > I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of. > top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30. > times slowers than seurat's Doheatmap(). Could you modify it to accelerate. > the process ? >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/569>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VPvaV-CHAOc88jcDpj8iSlwQgqUks5vbcK7gaJpZM4cRzgZ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/570:246,availability,sli,slightly,246,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:380,availability,avail,available,380,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:573,availability,avail,available,573,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:819,availability,sli,slightly,819,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:742,deployability,api,api-wrap,742,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:742,integrability,api,api-wrap,742,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:742,interoperability,api,api-wrap,742,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:246,reliability,sli,slightly,246,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:380,reliability,availab,available,380,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:573,reliability,availab,available,573,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:819,reliability,sli,slightly,819,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:380,safety,avail,available,380,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:573,safety,avail,available,573,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:75,security,sign,signature,75,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:380,security,availab,available,380,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:573,security,availab,available,573,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:785,security,sign,signature,785,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:411,usability,document,documentation,411,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:604,usability,document,documentation,604,"Sounds like a great idea. generally the order should be the same as in the signature, but I don’t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst. Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~louvain.find_partition`. ```. ```rst. Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the available options, consult the documentation for :func:`~leidenalg.find_partition`. ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`’s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:87,availability,cluster,clustering,87,maybe `sc.tl.modularity_clustering()` so that you keep the option open of adding other clustering functions in the future. Not exactly shorter though ;).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:87,deployability,cluster,clustering,87,maybe `sc.tl.modularity_clustering()` so that you keep the option open of adding other clustering functions in the future. Not exactly shorter though ;).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:71,energy efficiency,optim,optimization,71,"@gokceneraslan since they are largely the same thing (just a different optimization strategy), do we even need to keep both? Otherwise, I think I'd prefer them to be separate functions, so you don't get argument interactions. For example, the `partition_type` argument has to be a type from the same package as the method, otherwise there are segfaults.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:300,modifiability,pac,package,300,"@gokceneraslan since they are largely the same thing (just a different optimization strategy), do we even need to keep both? Otherwise, I think I'd prefer them to be separate functions, so you don't get argument interactions. For example, the `partition_type` argument has to be a type from the same package as the method, otherwise there are segfaults.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:71,performance,optimiz,optimization,71,"@gokceneraslan since they are largely the same thing (just a different optimization strategy), do we even need to keep both? Otherwise, I think I'd prefer them to be separate functions, so you don't get argument interactions. For example, the `partition_type` argument has to be a type from the same package as the method, otherwise there are segfaults.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:148,usability,prefer,prefer,148,"@gokceneraslan since they are largely the same thing (just a different optimization strategy), do we even need to keep both? Otherwise, I think I'd prefer them to be separate functions, so you don't get argument interactions. For example, the `partition_type` argument has to be a type from the same package as the method, otherwise there are segfaults.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:212,usability,interact,interactions,212,"@gokceneraslan since they are largely the same thing (just a different optimization strategy), do we even need to keep both? Otherwise, I think I'd prefer them to be separate functions, so you don't get argument interactions. For example, the `partition_type` argument has to be a type from the same package as the method, otherwise there are segfaults.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/571:66,deployability,automat,automatically,66,"@flying-sheep, can you reset that stable points to 1.4 so that it automatically points to 1.4.1, now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/issues/571:66,testability,automat,automatically,66,"@flying-sheep, can you reset that stable points to 1.4 so that it automatically points to 1.4.1, now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/pull/572:117,interoperability,conflict,conflicts,117,"Hey @falexwolf, if starting the percentile bins from 10 is not intentional (as mentioned in #624 ) I can resolve the conflicts and merge this along with the fix in #624 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/573:42,availability,failur,failure,42,Changing to WIP since I'd like to improve failure handling before this gets merged.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:42,deployability,fail,failure,42,Changing to WIP since I'd like to improve failure handling before this gets merged.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:42,performance,failur,failure,42,Changing to WIP since I'd like to improve failure handling before this gets merged.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:42,reliability,fail,failure,42,Changing to WIP since I'd like to improve failure handling before this gets merged.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:29,safety,review,review,29,That should do it. Ready for review,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:29,testability,review,review,29,That should do it. Ready for review,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:1514,availability,state,stated,1514,"Isaac,. this is great, thank you so much! Regarding the default for the dataset directory. I like this solution! Very small edits in addition to what I commented in the code:. * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`? * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272. * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`? Notes:. * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(…)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and Gökcen favored if I'm correctly summarizing the long thread? * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose. * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file... * `pyplot.rc_context` sounds awesome. * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file... .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:453,deployability,api,api,453,"Isaac,. this is great, thank you so much! Regarding the default for the dataset directory. I like this solution! Very small edits in addition to what I commented in the code:. * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`? * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272. * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`? Notes:. * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(…)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and Gökcen favored if I'm correctly summarizing the long thread? * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose. * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file... * `pyplot.rc_context` sounds awesome. * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file... .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:453,integrability,api,api,453,"Isaac,. this is great, thank you so much! Regarding the default for the dataset directory. I like this solution! Very small edits in addition to what I commented in the code:. * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`? * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272. * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`? Notes:. * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(…)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and Gökcen favored if I'm correctly summarizing the long thread? * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose. * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file... * `pyplot.rc_context` sounds awesome. * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file... .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:1514,integrability,state,stated,1514,"Isaac,. this is great, thank you so much! Regarding the default for the dataset directory. I like this solution! Very small edits in addition to what I commented in the code:. * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`? * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272. * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`? Notes:. * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(…)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and Gökcen favored if I'm correctly summarizing the long thread? * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose. * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file... * `pyplot.rc_context` sounds awesome. * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file... .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:453,interoperability,api,api,453,"Isaac,. this is great, thank you so much! Regarding the default for the dataset directory. I like this solution! Very small edits in addition to what I commented in the code:. * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`? * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272. * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`? Notes:. * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(…)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and Gökcen favored if I'm correctly summarizing the long thread? * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose. * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file... * `pyplot.rc_context` sounds awesome. * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file... .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:262,performance,time,time,262,"Isaac,. this is great, thank you so much! Regarding the default for the dataset directory. I like this solution! Very small edits in addition to what I commented in the code:. * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`? * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272. * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`? Notes:. * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(…)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and Gökcen favored if I'm correctly summarizing the long thread? * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose. * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file... * `pyplot.rc_context` sounds awesome. * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file... .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:620,performance,cach,cache,620,"Isaac,. this is great, thank you so much! Regarding the default for the dataset directory. I like this solution! Very small edits in addition to what I commented in the code:. * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`? * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272. * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`? Notes:. * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(…)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and Gökcen favored if I'm correctly summarizing the long thread? * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose. * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file... * `pyplot.rc_context` sounds awesome. * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file... .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:718,performance,cach,cache,718,"Isaac,. this is great, thank you so much! Regarding the default for the dataset directory. I like this solution! Very small edits in addition to what I commented in the code:. * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`? * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272. * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`? Notes:. * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(…)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and Gökcen favored if I'm correctly summarizing the long thread? * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose. * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file... * `pyplot.rc_context` sounds awesome. * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file... .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:775,performance,cach,cache,775,"Isaac,. this is great, thank you so much! Regarding the default for the dataset directory. I like this solution! Very small edits in addition to what I commented in the code:. * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`? * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272. * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`? Notes:. * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(…)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and Gökcen favored if I'm correctly summarizing the long thread? * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose. * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file... * `pyplot.rc_context` sounds awesome. * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file... .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:818,performance,cach,cache,818,"Isaac,. this is great, thank you so much! Regarding the default for the dataset directory. I like this solution! Very small edits in addition to what I commented in the code:. * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`? * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272. * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`? Notes:. * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(…)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and Gökcen favored if I'm correctly summarizing the long thread? * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose. * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file... * `pyplot.rc_context` sounds awesome. * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file... .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:1297,usability,document,documented,1297,"Isaac,. this is great, thank you so much! Regarding the default for the dataset directory. I like this solution! Very small edits in addition to what I commented in the code:. * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`? * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272. * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`? Notes:. * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(…)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and Gökcen favored if I'm correctly summarizing the long thread? * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose. * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file... * `pyplot.rc_context` sounds awesome. * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file... .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:499,deployability,api,api,499,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:532,deployability,api,api,532,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:826,deployability,configurat,configuration,826,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:788,energy efficiency,alloc,allocations,788,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:499,integrability,api,api,499,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:532,integrability,api,api,532,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:826,integrability,configur,configuration,826,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:102,interoperability,specif,specify,102,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:499,interoperability,api,api,499,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:532,interoperability,api,api,532,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:826,modifiability,configur,configuration,826,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:1070,modifiability,variab,variable,1070,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:310,performance,time,time,310,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:852,performance,time,time,852,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:663,reliability,Doe,Does,663,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:826,security,configur,configuration,826,"> Can we call this epi_sc_expression_atlas instead of expression_atlas? Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct? > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/? Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:154,deployability,fail,fail,154,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:268,deployability,fail,fail,268,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:331,deployability,fail,failed,331,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:390,deployability,fail,failing,390,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:471,deployability,build,build,471,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:43,energy efficiency,load,loading,43,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:43,performance,load,loading,43,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:163,performance,network,network,163,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:424,performance,time,time,424,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:154,reliability,fail,fail,154,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:268,reliability,fail,fail,268,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:331,reliability,fail,failed,331,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:390,reliability,fail,failing,390,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:17,safety,test,tests,17,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:225,safety,test,tests,225,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:355,safety,test,tests,355,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:163,security,network,network,163,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:171,security,access,access,171,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:17,testability,test,tests,17,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:225,testability,test,tests,225,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:355,testability,test,tests,355,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:401,testability,assert,assertion,401,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts? Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:129,energy efficiency,current,current,129,"@falexwolf, @flying-sheep . Just to recap what's left to be resolved here. * I'll reset the default value of `datasetdir` to the current value ""./data"". * Related, how about `datasetdir` instead of `datasetsdir`? It matches more to `cachedir` and `figdir`. Also, by analogy, it's ""potato sack"" not ""potatoes sack"" so ""dataset directory"" sounds more natural that ""datasets directory"" to me. * `ebi_expression_atlas` vs `ebi_sc_expression_atlas`. * Potentially adding a class for settings right now? * I think this becomes more important if `datasetdir` is documented. I bet people will set it with a `str` instead of a `Path` and that'll break things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:233,performance,cach,cachedir,233,"@falexwolf, @flying-sheep . Just to recap what's left to be resolved here. * I'll reset the default value of `datasetdir` to the current value ""./data"". * Related, how about `datasetdir` instead of `datasetsdir`? It matches more to `cachedir` and `figdir`. Also, by analogy, it's ""potato sack"" not ""potatoes sack"" so ""dataset directory"" sounds more natural that ""datasets directory"" to me. * `ebi_expression_atlas` vs `ebi_sc_expression_atlas`. * Potentially adding a class for settings right now? * I think this becomes more important if `datasetdir` is documented. I bet people will set it with a `str` instead of a `Path` and that'll break things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:555,usability,document,documented,555,"@falexwolf, @flying-sheep . Just to recap what's left to be resolved here. * I'll reset the default value of `datasetdir` to the current value ""./data"". * Related, how about `datasetdir` instead of `datasetsdir`? It matches more to `cachedir` and `figdir`. Also, by analogy, it's ""potato sack"" not ""potatoes sack"" so ""dataset directory"" sounds more natural that ""datasets directory"" to me. * `ebi_expression_atlas` vs `ebi_sc_expression_atlas`. * Potentially adding a class for settings right now? * I think this becomes more important if `datasetdir` is documented. I bet people will set it with a `str` instead of a `Path` and that'll break things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:90,deployability,modul,module,90,"OK guys, I'm happy with all that! I suggested `datasetsdir` over `datasetdir` because the module is called `datasets` and it's a place where many datasets end up. But, Isaac, you're the native speaker, so you're choice. Regarding making a settings a class: happy to if you feel you want to do that already in this PR. Really just change `scanpy/settings.py` to `scanpy/_settings.py`, put a `Settings` class in that file and generate an instance upon importing Scanpy. You could also just make `datasetdir` the only property and add all other attributes of the current `scanpy/settings` module as simple attributes. Then, all of this should be 10 min of work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:586,deployability,modul,module,586,"OK guys, I'm happy with all that! I suggested `datasetsdir` over `datasetdir` because the module is called `datasets` and it's a place where many datasets end up. But, Isaac, you're the native speaker, so you're choice. Regarding making a settings a class: happy to if you feel you want to do that already in this PR. Really just change `scanpy/settings.py` to `scanpy/_settings.py`, put a `Settings` class in that file and generate an instance upon importing Scanpy. You could also just make `datasetdir` the only property and add all other attributes of the current `scanpy/settings` module as simple attributes. Then, all of this should be 10 min of work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:560,energy efficiency,current,current,560,"OK guys, I'm happy with all that! I suggested `datasetsdir` over `datasetdir` because the module is called `datasets` and it's a place where many datasets end up. But, Isaac, you're the native speaker, so you're choice. Regarding making a settings a class: happy to if you feel you want to do that already in this PR. Really just change `scanpy/settings.py` to `scanpy/_settings.py`, put a `Settings` class in that file and generate an instance upon importing Scanpy. You could also just make `datasetdir` the only property and add all other attributes of the current `scanpy/settings` module as simple attributes. Then, all of this should be 10 min of work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:90,modifiability,modul,module,90,"OK guys, I'm happy with all that! I suggested `datasetsdir` over `datasetdir` because the module is called `datasets` and it's a place where many datasets end up. But, Isaac, you're the native speaker, so you're choice. Regarding making a settings a class: happy to if you feel you want to do that already in this PR. Really just change `scanpy/settings.py` to `scanpy/_settings.py`, put a `Settings` class in that file and generate an instance upon importing Scanpy. You could also just make `datasetdir` the only property and add all other attributes of the current `scanpy/settings` module as simple attributes. Then, all of this should be 10 min of work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:586,modifiability,modul,module,586,"OK guys, I'm happy with all that! I suggested `datasetsdir` over `datasetdir` because the module is called `datasets` and it's a place where many datasets end up. But, Isaac, you're the native speaker, so you're choice. Regarding making a settings a class: happy to if you feel you want to do that already in this PR. Really just change `scanpy/settings.py` to `scanpy/_settings.py`, put a `Settings` class in that file and generate an instance upon importing Scanpy. You could also just make `datasetdir` the only property and add all other attributes of the current `scanpy/settings` module as simple attributes. Then, all of this should be 10 min of work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:90,safety,modul,module,90,"OK guys, I'm happy with all that! I suggested `datasetsdir` over `datasetdir` because the module is called `datasets` and it's a place where many datasets end up. But, Isaac, you're the native speaker, so you're choice. Regarding making a settings a class: happy to if you feel you want to do that already in this PR. Really just change `scanpy/settings.py` to `scanpy/_settings.py`, put a `Settings` class in that file and generate an instance upon importing Scanpy. You could also just make `datasetdir` the only property and add all other attributes of the current `scanpy/settings` module as simple attributes. Then, all of this should be 10 min of work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:586,safety,modul,module,586,"OK guys, I'm happy with all that! I suggested `datasetsdir` over `datasetdir` because the module is called `datasets` and it's a place where many datasets end up. But, Isaac, you're the native speaker, so you're choice. Regarding making a settings a class: happy to if you feel you want to do that already in this PR. Really just change `scanpy/settings.py` to `scanpy/_settings.py`, put a `Settings` class in that file and generate an instance upon importing Scanpy. You could also just make `datasetdir` the only property and add all other attributes of the current `scanpy/settings` module as simple attributes. Then, all of this should be 10 min of work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:596,testability,simpl,simple,596,"OK guys, I'm happy with all that! I suggested `datasetsdir` over `datasetdir` because the module is called `datasets` and it's a place where many datasets end up. But, Isaac, you're the native speaker, so you're choice. Regarding making a settings a class: happy to if you feel you want to do that already in this PR. Really just change `scanpy/settings.py` to `scanpy/_settings.py`, put a `Settings` class in that file and generate an instance upon importing Scanpy. You could also just make `datasetdir` the only property and add all other attributes of the current `scanpy/settings` module as simple attributes. Then, all of this should be 10 min of work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:596,usability,simpl,simple,596,"OK guys, I'm happy with all that! I suggested `datasetsdir` over `datasetdir` because the module is called `datasets` and it's a place where many datasets end up. But, Isaac, you're the native speaker, so you're choice. Regarding making a settings a class: happy to if you feel you want to do that already in this PR. Really just change `scanpy/settings.py` to `scanpy/_settings.py`, put a `Settings` class in that file and generate an instance upon importing Scanpy. You could also just make `datasetdir` the only property and add all other attributes of the current `scanpy/settings` module as simple attributes. Then, all of this should be 10 min of work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:416,availability,error,error,416,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:452,availability,consist,consistent,452,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:200,deployability,fail,fail,200,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:479,deployability,log,logging,479,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:487,deployability,modul,module,487,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:581,deployability,modul,module,581,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:51,modifiability,variab,variables,51,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:487,modifiability,modul,module,487,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:558,modifiability,variab,variable,558,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:581,modifiability,modul,module,581,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:416,performance,error,error,416,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:200,reliability,fail,fail,200,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:36,safety,except,except,36,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:143,safety,test,tests,143,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:221,safety,test,tested,221,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:416,safety,error,error,416,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:479,safety,log,logging,479,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:487,safety,modul,module,487,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:581,safety,modul,module,581,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:279,security,modif,modify,279,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:479,security,log,logging,479,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:143,testability,test,tests,143,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:221,testability,test,tested,221,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:479,testability,log,logging,479,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:298,usability,document,documentation,298,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:408,usability,stop,stop,408,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:416,usability,error,error,416,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:452,usability,consist,consistent,452,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. . * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:14,deployability,build,build,14,"Assuming this build passes, is this good to go?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:53,modifiability,variab,variables,53,"> Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Yes, now that you made everything a property, I would have expected it to take much longer than 10 minutes. It's great that you did! I'll make some tiny additions and merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:38,safety,except,except,38,"> Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Yes, now that you made everything a property, I would have expected it to take much longer than 10 minutes. It's great that you did! I'll make some tiny additions and merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:82,modifiability,maintain,maintainers,82,"Ah, I cannot push to your fork it seems. I think you would have had to set ""allow maintainers to edit"" or something on the right-hand side. I'm simply merging this and editing after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:82,safety,maintain,maintainers,82,"Ah, I cannot push to your fork it seems. I think you would have had to set ""allow maintainers to edit"" or something on the right-hand side. I'm simply merging this and editing after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:144,testability,simpl,simply,144,"Ah, I cannot push to your fork it seems. I think you would have had to set ""allow maintainers to edit"" or something on the right-hand side. I'm simply merging this and editing after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:144,usability,simpl,simply,144,"Ah, I cannot push to your fork it seems. I think you would have had to set ""allow maintainers to edit"" or something on the right-hand side. I'm simply merging this and editing after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:254,availability,consist,consistent,254,"Oh, huh, I thought allowing modifications was on by default. My bad. . > Yes, now that you made everything a property, I would have expected it to take much longer than 10 minutes. It's great that you did! I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through `?sc.settings.{setting}`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:309,availability,avail,available,309,"Oh, huh, I thought allowing modifications was on by default. My bad. . > Yes, now that you made everything a property, I would have expected it to take much longer than 10 minutes. It's great that you did! I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through `?sc.settings.{setting}`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:309,reliability,availab,available,309,"Oh, huh, I thought allowing modifications was on by default. My bad. . > Yes, now that you made everything a property, I would have expected it to take much longer than 10 minutes. It's great that you did! I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through `?sc.settings.{setting}`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:309,safety,avail,available,309,"Oh, huh, I thought allowing modifications was on by default. My bad. . > Yes, now that you made everything a property, I would have expected it to take much longer than 10 minutes. It's great that you did! I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through `?sc.settings.{setting}`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:28,security,modif,modifications,28,"Oh, huh, I thought allowing modifications was on by default. My bad. . > Yes, now that you made everything a property, I would have expected it to take much longer than 10 minutes. It's great that you did! I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through `?sc.settings.{setting}`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:309,security,availab,available,309,"Oh, huh, I thought allowing modifications was on by default. My bad. . > Yes, now that you made everything a property, I would have expected it to take much longer than 10 minutes. It's great that you did! I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through `?sc.settings.{setting}`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:254,usability,consist,consistent,254,"Oh, huh, I thought allowing modifications was on by default. My bad. . > Yes, now that you made everything a property, I would have expected it to take much longer than 10 minutes. It's great that you did! I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through `?sc.settings.{setting}`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:271,usability,document,documentation,271,"Oh, huh, I thought allowing modifications was on by default. My bad. . > Yes, now that you made everything a property, I would have expected it to take much longer than 10 minutes. It's great that you did! I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through `?sc.settings.{setting}`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:50,availability,consist,consistent,50,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:105,availability,avail,available,105,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:181,availability,consist,consistent,181,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:337,availability,avail,available,337,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:301,deployability,automat,automatic,301,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:394,deployability,api,api,394,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:481,deployability,api,api,481,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:271,energy efficiency,current,current,271,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:394,integrability,api,api,394,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:481,integrability,api,api,481,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:394,interoperability,api,api,394,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:481,interoperability,api,api,481,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:815,performance,time,time,815,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:105,reliability,availab,available,105,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:337,reliability,availab,available,337,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:105,safety,avail,available,105,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:292,safety,compl,complete,292,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:337,safety,avail,available,337,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:105,security,availab,available,105,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:292,security,compl,complete,292,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:337,security,availab,available,337,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:301,testability,automat,automatic,301,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:50,usability,consist,consistent,50,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:67,usability,document,documentation,67,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:181,usability,consist,consistent,181,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:311,usability,document,documentation,311,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}! Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from . https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html. under. https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:297,performance,time,time,297,"Only strange thing is the `getdoc` function. To be more verbose, one could also change all the cryptic abbreviations to:. ```. directory_cache. directory_figures. directory_datasets. directory_write (but this should be deprecated anyway, wasn't a great idea... reminiscent from the very early CLI time of scanpy). ```. or something. It's great that this is super easy without breaking backwards compat now that everything is properties. :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:125,availability,slo,slots,125,">Only strange thing is the getdoc function. It looks like instance methods can't have new attributes assigned (probably have slots). It's possible the `.getdoc` attribute could be added to the classes method (not sure if that's the right way to say that, here's an example):. ```python. class Foo(object):. def bar(self):. return 1. # Setting an attribute on the method of an instance raises an error. Foo().bar.x = 1. # AttributeError: 'method' object has no attribute 'x'. # Setting an attribute on the method of a class seems fine:. Foo.bar.x = 1 . Foo().bar.x. # 1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:395,availability,error,error,395,">Only strange thing is the getdoc function. It looks like instance methods can't have new attributes assigned (probably have slots). It's possible the `.getdoc` attribute could be added to the classes method (not sure if that's the right way to say that, here's an example):. ```python. class Foo(object):. def bar(self):. return 1. # Setting an attribute on the method of an instance raises an error. Foo().bar.x = 1. # AttributeError: 'method' object has no attribute 'x'. # Setting an attribute on the method of a class seems fine:. Foo.bar.x = 1 . Foo().bar.x. # 1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:395,performance,error,error,395,">Only strange thing is the getdoc function. It looks like instance methods can't have new attributes assigned (probably have slots). It's possible the `.getdoc` attribute could be added to the classes method (not sure if that's the right way to say that, here's an example):. ```python. class Foo(object):. def bar(self):. return 1. # Setting an attribute on the method of an instance raises an error. Foo().bar.x = 1. # AttributeError: 'method' object has no attribute 'x'. # Setting an attribute on the method of a class seems fine:. Foo.bar.x = 1 . Foo().bar.x. # 1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:125,reliability,slo,slots,125,">Only strange thing is the getdoc function. It looks like instance methods can't have new attributes assigned (probably have slots). It's possible the `.getdoc` attribute could be added to the classes method (not sure if that's the right way to say that, here's an example):. ```python. class Foo(object):. def bar(self):. return 1. # Setting an attribute on the method of an instance raises an error. Foo().bar.x = 1. # AttributeError: 'method' object has no attribute 'x'. # Setting an attribute on the method of a class seems fine:. Foo.bar.x = 1 . Foo().bar.x. # 1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:395,safety,error,error,395,">Only strange thing is the getdoc function. It looks like instance methods can't have new attributes assigned (probably have slots). It's possible the `.getdoc` attribute could be added to the classes method (not sure if that's the right way to say that, here's an example):. ```python. class Foo(object):. def bar(self):. return 1. # Setting an attribute on the method of an instance raises an error. Foo().bar.x = 1. # AttributeError: 'method' object has no attribute 'x'. # Setting an attribute on the method of a class seems fine:. Foo.bar.x = 1 . Foo().bar.x. # 1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:395,usability,error,error,395,">Only strange thing is the getdoc function. It looks like instance methods can't have new attributes assigned (probably have slots). It's possible the `.getdoc` attribute could be added to the classes method (not sure if that's the right way to say that, here's an example):. ```python. class Foo(object):. def bar(self):. return 1. # Setting an attribute on the method of an instance raises an error. Foo().bar.x = 1. # AttributeError: 'method' object has no attribute 'x'. # Setting an attribute on the method of a class seems fine:. Foo.bar.x = 1 . Foo().bar.x. # 1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/issues/575:614,energy efficiency,current,currently,614,"Thanks for the heads up for the typo. That is fixed now. About the feature request... this is not entirely straightforward. The challenges to do this are:. 1. As densities are calculated relative to the sample (or the category subset from `.obs`), the values are not directly comparable. For example, you would get a value of 0.5 if you subtract the maximum density in one condition from a density of half the maximum in the other condition... but maybe the overall density is higher in the second condition. You could just interpret this as the relative differences within the samples I guess... 2. Densities are currently calculated over cells... to subtract one from the other, you'd have to interpolate this to a grid layout. 3. Ideally you'd want some kind of statistical test on differential densities... that's a whole other question... What do you think about the above points? If this were implemented, it probably wouldn't be a matter of a day or so... as I'm a bit low on time at the moment, I wouldn't be able to implement this in the next month even if we did find a good way forward unfortunately. You are welcome to submit a pull request though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:227,integrability,sub,subset,227,"Thanks for the heads up for the typo. That is fixed now. About the feature request... this is not entirely straightforward. The challenges to do this are:. 1. As densities are calculated relative to the sample (or the category subset from `.obs`), the values are not directly comparable. For example, you would get a value of 0.5 if you subtract the maximum density in one condition from a density of half the maximum in the other condition... but maybe the overall density is higher in the second condition. You could just interpret this as the relative differences within the samples I guess... 2. Densities are currently calculated over cells... to subtract one from the other, you'd have to interpolate this to a grid layout. 3. Ideally you'd want some kind of statistical test on differential densities... that's a whole other question... What do you think about the above points? If this were implemented, it probably wouldn't be a matter of a day or so... as I'm a bit low on time at the moment, I wouldn't be able to implement this in the next month even if we did find a good way forward unfortunately. You are welcome to submit a pull request though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:337,integrability,sub,subtract,337,"Thanks for the heads up for the typo. That is fixed now. About the feature request... this is not entirely straightforward. The challenges to do this are:. 1. As densities are calculated relative to the sample (or the category subset from `.obs`), the values are not directly comparable. For example, you would get a value of 0.5 if you subtract the maximum density in one condition from a density of half the maximum in the other condition... but maybe the overall density is higher in the second condition. You could just interpret this as the relative differences within the samples I guess... 2. Densities are currently calculated over cells... to subtract one from the other, you'd have to interpolate this to a grid layout. 3. Ideally you'd want some kind of statistical test on differential densities... that's a whole other question... What do you think about the above points? If this were implemented, it probably wouldn't be a matter of a day or so... as I'm a bit low on time at the moment, I wouldn't be able to implement this in the next month even if we did find a good way forward unfortunately. You are welcome to submit a pull request though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:652,integrability,sub,subtract,652,"Thanks for the heads up for the typo. That is fixed now. About the feature request... this is not entirely straightforward. The challenges to do this are:. 1. As densities are calculated relative to the sample (or the category subset from `.obs`), the values are not directly comparable. For example, you would get a value of 0.5 if you subtract the maximum density in one condition from a density of half the maximum in the other condition... but maybe the overall density is higher in the second condition. You could just interpret this as the relative differences within the samples I guess... 2. Densities are currently calculated over cells... to subtract one from the other, you'd have to interpolate this to a grid layout. 3. Ideally you'd want some kind of statistical test on differential densities... that's a whole other question... What do you think about the above points? If this were implemented, it probably wouldn't be a matter of a day or so... as I'm a bit low on time at the moment, I wouldn't be able to implement this in the next month even if we did find a good way forward unfortunately. You are welcome to submit a pull request though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:1131,integrability,sub,submit,1131,"Thanks for the heads up for the typo. That is fixed now. About the feature request... this is not entirely straightforward. The challenges to do this are:. 1. As densities are calculated relative to the sample (or the category subset from `.obs`), the values are not directly comparable. For example, you would get a value of 0.5 if you subtract the maximum density in one condition from a density of half the maximum in the other condition... but maybe the overall density is higher in the second condition. You could just interpret this as the relative differences within the samples I guess... 2. Densities are currently calculated over cells... to subtract one from the other, you'd have to interpolate this to a grid layout. 3. Ideally you'd want some kind of statistical test on differential densities... that's a whole other question... What do you think about the above points? If this were implemented, it probably wouldn't be a matter of a day or so... as I'm a bit low on time at the moment, I wouldn't be able to implement this in the next month even if we did find a good way forward unfortunately. You are welcome to submit a pull request though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:983,performance,time,time,983,"Thanks for the heads up for the typo. That is fixed now. About the feature request... this is not entirely straightforward. The challenges to do this are:. 1. As densities are calculated relative to the sample (or the category subset from `.obs`), the values are not directly comparable. For example, you would get a value of 0.5 if you subtract the maximum density in one condition from a density of half the maximum in the other condition... but maybe the overall density is higher in the second condition. You could just interpret this as the relative differences within the samples I guess... 2. Densities are currently calculated over cells... to subtract one from the other, you'd have to interpolate this to a grid layout. 3. Ideally you'd want some kind of statistical test on differential densities... that's a whole other question... What do you think about the above points? If this were implemented, it probably wouldn't be a matter of a day or so... as I'm a bit low on time at the moment, I wouldn't be able to implement this in the next month even if we did find a good way forward unfortunately. You are welcome to submit a pull request though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:777,safety,test,test,777,"Thanks for the heads up for the typo. That is fixed now. About the feature request... this is not entirely straightforward. The challenges to do this are:. 1. As densities are calculated relative to the sample (or the category subset from `.obs`), the values are not directly comparable. For example, you would get a value of 0.5 if you subtract the maximum density in one condition from a density of half the maximum in the other condition... but maybe the overall density is higher in the second condition. You could just interpret this as the relative differences within the samples I guess... 2. Densities are currently calculated over cells... to subtract one from the other, you'd have to interpolate this to a grid layout. 3. Ideally you'd want some kind of statistical test on differential densities... that's a whole other question... What do you think about the above points? If this were implemented, it probably wouldn't be a matter of a day or so... as I'm a bit low on time at the moment, I wouldn't be able to implement this in the next month even if we did find a good way forward unfortunately. You are welcome to submit a pull request though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:777,testability,test,test,777,"Thanks for the heads up for the typo. That is fixed now. About the feature request... this is not entirely straightforward. The challenges to do this are:. 1. As densities are calculated relative to the sample (or the category subset from `.obs`), the values are not directly comparable. For example, you would get a value of 0.5 if you subtract the maximum density in one condition from a density of half the maximum in the other condition... but maybe the overall density is higher in the second condition. You could just interpret this as the relative differences within the samples I guess... 2. Densities are currently calculated over cells... to subtract one from the other, you'd have to interpolate this to a grid layout. 3. Ideally you'd want some kind of statistical test on differential densities... that's a whole other question... What do you think about the above points? If this were implemented, it probably wouldn't be a matter of a day or so... as I'm a bit low on time at the moment, I wouldn't be able to implement this in the next month even if we did find a good way forward unfortunately. You are welcome to submit a pull request though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:393,availability,down,down,393,"Thanks- that doesn't seem to be as easy as I was thinking.. . 1. I think for most applications that I have in mind I would be interested in the relative differences. Are cells distributed differently in two conditions, regardless of whether there are more cells overall in one of the conditions? 2. My bad, I thought the were already calculated over a grid layout.. would that also require to down sample the larger cell population to match the smaller one? 3. That would be very cool, but having this for qualitative assessment would be already useful. Ok, thanks - will do if I come up with a satisfying solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:480,energy efficiency,cool,cool,480,"Thanks- that doesn't seem to be as easy as I was thinking.. . 1. I think for most applications that I have in mind I would be interested in the relative differences. Are cells distributed differently in two conditions, regardless of whether there are more cells overall in one of the conditions? 2. My bad, I thought the were already calculated over a grid layout.. would that also require to down sample the larger cell population to match the smaller one? 3. That would be very cool, but having this for qualitative assessment would be already useful. Ok, thanks - will do if I come up with a satisfying solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:176,interoperability,distribut,distributed,176,"Thanks- that doesn't seem to be as easy as I was thinking.. . 1. I think for most applications that I have in mind I would be interested in the relative differences. Are cells distributed differently in two conditions, regardless of whether there are more cells overall in one of the conditions? 2. My bad, I thought the were already calculated over a grid layout.. would that also require to down sample the larger cell population to match the smaller one? 3. That would be very cool, but having this for qualitative assessment would be already useful. Ok, thanks - will do if I come up with a satisfying solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:13,reliability,doe,doesn,13,"Thanks- that doesn't seem to be as easy as I was thinking.. . 1. I think for most applications that I have in mind I would be interested in the relative differences. Are cells distributed differently in two conditions, regardless of whether there are more cells overall in one of the conditions? 2. My bad, I thought the were already calculated over a grid layout.. would that also require to down sample the larger cell population to match the smaller one? 3. That would be very cool, but having this for qualitative assessment would be already useful. Ok, thanks - will do if I come up with a satisfying solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:518,security,assess,assessment,518,"Thanks- that doesn't seem to be as easy as I was thinking.. . 1. I think for most applications that I have in mind I would be interested in the relative differences. Are cells distributed differently in two conditions, regardless of whether there are more cells overall in one of the conditions? 2. My bad, I thought the were already calculated over a grid layout.. would that also require to down sample the larger cell population to match the smaller one? 3. That would be very cool, but having this for qualitative assessment would be already useful. Ok, thanks - will do if I come up with a satisfying solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:49,deployability,continu,continue,49,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:140,deployability,continu,continue,140,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:233,deployability,scale,scaled,233,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:233,energy efficiency,scale,scaled,233,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:404,energy efficiency,current,currently,404,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:422,energy efficiency,current,currently,422,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:205,modifiability,scal,scaling,205,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:233,modifiability,scal,scaled,233,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:233,performance,scale,scaled,233,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:32,security,session,session,32,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:977,testability,simpl,simple,977,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:959,usability,custom,custom,959,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:977,usability,simpl,simple,977,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:389,availability,cluster,clusters,389,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:465,availability,cluster,cluster,465,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:652,availability,cluster,clusters,652,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:130,deployability,depend,depending,130,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:175,deployability,depend,depends,175,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:389,deployability,cluster,clusters,389,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:465,deployability,cluster,cluster,465,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:652,deployability,cluster,clusters,652,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:130,integrability,depend,depending,130,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:175,integrability,depend,depends,175,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:609,integrability,sub,subtle,609,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:130,modifiability,depend,depending,130,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:175,modifiability,depend,depends,175,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:246,reliability,Doe,Does,246,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:130,safety,depend,depending,130,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:175,safety,depend,depends,175,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:130,testability,depend,depending,130,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:175,testability,depend,depends,175,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:581,usability,visual,visualization,581,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:692,usability,visual,visual,692,Thanks for your thoughts on this! 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? . 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:412,availability,cluster,clusters,412,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:42,deployability,depend,depending,42,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:412,deployability,cluster,clusters,412,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:513,deployability,compos,compositions,513,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:622,deployability,compos,compositional,622,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:464,energy efficiency,optim,optimal,464,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:580,energy efficiency,model,modeled,580,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:781,energy efficiency,heat,heatmap,781,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:1201,energy efficiency,heat,heatmap,1201,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:42,integrability,depend,depending,42,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:460,integrability,sub,sub-optimal,460,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:42,modifiability,depend,depending,42,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:131,modifiability,paramet,parameter,131,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:513,modifiability,compos,compositions,513,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:622,modifiability,compos,compositional,622,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:1312,performance,time,time,1312,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:909,reliability,doe,does,909,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:1210,reliability,Doe,Doesn,1210,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:42,safety,depend,depending,42,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:488,safety,test,testing,488,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:580,security,model,modeled,580,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:42,testability,depend,depending,42,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:488,testability,test,testing,488,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:126,usability,user,user,126,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:276,usability,visual,visualization,276,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:675,usability,visual,visualization,675,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value. 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:193,usability,visual,visualizing,193,"@LuckyMD, have you checked out [datashader](http://datashader.org)? I think it's set up to do a lot of the things you're talking about here. In general, it gives you a principled framework for visualizing many points, including how to aggregate points that overlap.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:42,deployability,depend,dependency,42,Looks super cool... but also like a heavy dependency. Do you think it would be worth using datashader when we are just looking for a simple additional function?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:12,energy efficiency,cool,cool,12,Looks super cool... but also like a heavy dependency. Do you think it would be worth using datashader when we are just looking for a simple additional function?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:42,integrability,depend,dependency,42,Looks super cool... but also like a heavy dependency. Do you think it would be worth using datashader when we are just looking for a simple additional function?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:42,modifiability,depend,dependency,42,Looks super cool... but also like a heavy dependency. Do you think it would be worth using datashader when we are just looking for a simple additional function?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:42,safety,depend,dependency,42,Looks super cool... but also like a heavy dependency. Do you think it would be worth using datashader when we are just looking for a simple additional function?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:42,testability,depend,dependency,42,Looks super cool... but also like a heavy dependency. Do you think it would be worth using datashader when we are just looking for a simple additional function?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:133,testability,simpl,simple,133,Looks super cool... but also like a heavy dependency. Do you think it would be worth using datashader when we are just looking for a simple additional function?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:133,usability,simpl,simple,133,Looks super cool... but also like a heavy dependency. Do you think it would be worth using datashader when we are just looking for a simple additional function?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:19,deployability,depend,dependency,19,"Definitely a heavy dependency, you should see the size of the conda environment you need to test it. I think it'd be useful for playing around with ideas on how you'd like to aggregate and scale the values, since they've already got a bunch of methods implemented. Plus the plots often look pretty good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:189,deployability,scale,scale,189,"Definitely a heavy dependency, you should see the size of the conda environment you need to test it. I think it'd be useful for playing around with ideas on how you'd like to aggregate and scale the values, since they've already got a bunch of methods implemented. Plus the plots often look pretty good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:189,energy efficiency,scale,scale,189,"Definitely a heavy dependency, you should see the size of the conda environment you need to test it. I think it'd be useful for playing around with ideas on how you'd like to aggregate and scale the values, since they've already got a bunch of methods implemented. Plus the plots often look pretty good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:19,integrability,depend,dependency,19,"Definitely a heavy dependency, you should see the size of the conda environment you need to test it. I think it'd be useful for playing around with ideas on how you'd like to aggregate and scale the values, since they've already got a bunch of methods implemented. Plus the plots often look pretty good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:19,modifiability,depend,dependency,19,"Definitely a heavy dependency, you should see the size of the conda environment you need to test it. I think it'd be useful for playing around with ideas on how you'd like to aggregate and scale the values, since they've already got a bunch of methods implemented. Plus the plots often look pretty good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:189,modifiability,scal,scale,189,"Definitely a heavy dependency, you should see the size of the conda environment you need to test it. I think it'd be useful for playing around with ideas on how you'd like to aggregate and scale the values, since they've already got a bunch of methods implemented. Plus the plots often look pretty good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:189,performance,scale,scale,189,"Definitely a heavy dependency, you should see the size of the conda environment you need to test it. I think it'd be useful for playing around with ideas on how you'd like to aggregate and scale the values, since they've already got a bunch of methods implemented. Plus the plots often look pretty good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:19,safety,depend,dependency,19,"Definitely a heavy dependency, you should see the size of the conda environment you need to test it. I think it'd be useful for playing around with ideas on how you'd like to aggregate and scale the values, since they've already got a bunch of methods implemented. Plus the plots often look pretty good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:92,safety,test,test,92,"Definitely a heavy dependency, you should see the size of the conda environment you need to test it. I think it'd be useful for playing around with ideas on how you'd like to aggregate and scale the values, since they've already got a bunch of methods implemented. Plus the plots often look pretty good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:19,testability,depend,dependency,19,"Definitely a heavy dependency, you should see the size of the conda environment you need to test it. I think it'd be useful for playing around with ideas on how you'd like to aggregate and scale the values, since they've already got a bunch of methods implemented. Plus the plots often look pretty good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:92,testability,test,test,92,"Definitely a heavy dependency, you should see the size of the conda environment you need to test it. I think it'd be useful for playing around with ideas on how you'd like to aggregate and scale the values, since they've already got a bunch of methods implemented. Plus the plots often look pretty good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:66,performance,time,time,66,"That is a good suggestion, but might be difficult with regards to time commitment...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:2032,availability,Down,Downsample,2032,"to""], inplace=True). sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000). sc.pp.log1p(adata). return adata. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. def simulate_doublets(adata, frac=.5):. """"""Simulate doublets from count data. . Params. ------. adata. The anndata object to sample from. Must have count data. frac. Fraction of total cells to simulate. """""". m, n = adata.X.shape. n_doublets = int(np.round(m * frac)). pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))). combos = np.random.randint(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). ). dblX = pos * adata.X. # TODO: Downsample total counts. srcs = np.sort(combos.reshape(n_doublets, 2), axis=1). obs = pd.DataFrame(srcs, columns=[""src1"", ""src2""]). var = pd.DataFrame(index=adata.var_names). return sc.AnnData(dblX, obs=obs, var=var). # Load data. # http: // cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_10k_v3/pbmc_10k_v3_filtered_feature_bc_matrix.h5. pbmc = sc.read_10x_h5(""./data/10x/pbmc_10k_v3_filtered_feature_bc_matrix.h5""). pbmc.var[""gene_symbols""] = pbmc.var.index. pbmc.var.set_index(""gene_ids"", inplace=True). dblt = simulate_doublets(pbmc). dblt.var[""gene_symbols""] = pbmc.var[""gene_symbols""]. pbmc.raw = pbmc. dblt.raw = dblt. pbmc = preprocess(pbmc). dblt = preprocess(dblt). sc.pp.pca(pbmc). pca_update(dblt, pbmc). umap = UMAP(). pbmc.obsm[""X_umap""] = umap.fit_transform(pbmc.obsm[""X_pca""]). dblt.obsm[""X_umap""] = umap.transform(dblt.obsm[""X_pca""]). sc.tl.embedding_density(pbmc, ""umap""). sc.tl.embedding_density(dblt, ""umap""). ```. </details>. <details> . <summary> Getting setup for datashader plots (",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:670,energy efficiency,load,loading,670,"I think I've got an example for you, which should be pretty easy for you to play around with in datashader. The example is doublet detection. I'm following the basic outline of the methods which simulate doublets, then project those onto the real data to find which barcode (/cell) the simulated doublets sit next to. Those barcodes are presumed to be doublets. So we'd expect that areas of mostly singlets in the real data would have a lower relative (to the real data) density of points in the simulated. I'm still exploring what the best way to summarize that difference in density is through. Here's an example with some pbmcs from 10x:. <details>. <summary> Setup (loading, simulating, and projecting) </summary>. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import sparse. from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):. adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""). sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True). sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000). sc.pp.log1p(adata). return adata. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. def simulate_doublets(adata, frac=.5):. """"""Simulate doublets from count data. . Params. ------. adata. The anndata object to sample from. Must have count data. frac. Fraction of total cells to simulate. """""". m, n = adata.X.shape. n_doublets = int(np.round(m * frac)). pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))). combos = np.random.randint(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:2252,energy efficiency,Load,Load,2252,"lace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. def simulate_doublets(adata, frac=.5):. """"""Simulate doublets from count data. . Params. ------. adata. The anndata object to sample from. Must have count data. frac. Fraction of total cells to simulate. """""". m, n = adata.X.shape. n_doublets = int(np.round(m * frac)). pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))). combos = np.random.randint(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). ). dblX = pos * adata.X. # TODO: Downsample total counts. srcs = np.sort(combos.reshape(n_doublets, 2), axis=1). obs = pd.DataFrame(srcs, columns=[""src1"", ""src2""]). var = pd.DataFrame(index=adata.var_names). return sc.AnnData(dblX, obs=obs, var=var). # Load data. # http: // cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_10k_v3/pbmc_10k_v3_filtered_feature_bc_matrix.h5. pbmc = sc.read_10x_h5(""./data/10x/pbmc_10k_v3_filtered_feature_bc_matrix.h5""). pbmc.var[""gene_symbols""] = pbmc.var.index. pbmc.var.set_index(""gene_ids"", inplace=True). dblt = simulate_doublets(pbmc). dblt.var[""gene_symbols""] = pbmc.var[""gene_symbols""]. pbmc.raw = pbmc. dblt.raw = dblt. pbmc = preprocess(pbmc). dblt = preprocess(dblt). sc.pp.pca(pbmc). pca_update(dblt, pbmc). umap = UMAP(). pbmc.obsm[""X_umap""] = umap.fit_transform(pbmc.obsm[""X_pca""]). dblt.obsm[""X_umap""] = umap.transform(dblt.obsm[""X_pca""]). sc.tl.embedding_density(pbmc, ""umap""). sc.tl.embedding_density(dblt, ""umap""). ```. </details>. <details> . <summary> Getting setup for datashader plots (much shorter) : </summary>. Make dataframe:. ```python. pbmcdf = pd.DataFrame(pbmc.obsm[""X_umap""], columns=[""x"", ""y""]) # Real data. dbltdf = pd.DataFrame(dblt.obsm[""X_umap""], columns=[""x"", ""y""]) # Simulated doublets. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:2853,integrability,transform,transform,2853,"(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). ). dblX = pos * adata.X. # TODO: Downsample total counts. srcs = np.sort(combos.reshape(n_doublets, 2), axis=1). obs = pd.DataFrame(srcs, columns=[""src1"", ""src2""]). var = pd.DataFrame(index=adata.var_names). return sc.AnnData(dblX, obs=obs, var=var). # Load data. # http: // cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_10k_v3/pbmc_10k_v3_filtered_feature_bc_matrix.h5. pbmc = sc.read_10x_h5(""./data/10x/pbmc_10k_v3_filtered_feature_bc_matrix.h5""). pbmc.var[""gene_symbols""] = pbmc.var.index. pbmc.var.set_index(""gene_ids"", inplace=True). dblt = simulate_doublets(pbmc). dblt.var[""gene_symbols""] = pbmc.var[""gene_symbols""]. pbmc.raw = pbmc. dblt.raw = dblt. pbmc = preprocess(pbmc). dblt = preprocess(dblt). sc.pp.pca(pbmc). pca_update(dblt, pbmc). umap = UMAP(). pbmc.obsm[""X_umap""] = umap.fit_transform(pbmc.obsm[""X_pca""]). dblt.obsm[""X_umap""] = umap.transform(dblt.obsm[""X_pca""]). sc.tl.embedding_density(pbmc, ""umap""). sc.tl.embedding_density(dblt, ""umap""). ```. </details>. <details> . <summary> Getting setup for datashader plots (much shorter) : </summary>. Make dataframe:. ```python. pbmcdf = pd.DataFrame(pbmc.obsm[""X_umap""], columns=[""x"", ""y""]) # Real data. dbltdf = pd.DataFrame(dblt.obsm[""X_umap""], columns=[""x"", ""y""]) # Simulated doublets. pbmcdf[""density""] = pbmc.obs[""umap_density""].values. dbltdf[""density""] = dblt.obs[""umap_density""].values. ```. Get plotting imports and canvas:. ```python. import datashader as ds. from datashader import transfer_functions as tf. from bokeh import palettes. canvas = ds.Canvas(plot_width=300, plot_height=300,. x_range=(pbmcdf[""x""].min() - .5, pbmcdf[""x""].max() + .5), . y_range=(pbmcdf[""y""].min() - .5, pbmcdf[""y""].max() + .5),. x_axis_type='linear', y_axis_type='linear'). ```. </details>. First, something simple. Basically just a 2d histogram with 300 x 300 bins:. ```python. real = canvas.points(pbm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:2853,interoperability,transform,transform,2853,"(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). ). dblX = pos * adata.X. # TODO: Downsample total counts. srcs = np.sort(combos.reshape(n_doublets, 2), axis=1). obs = pd.DataFrame(srcs, columns=[""src1"", ""src2""]). var = pd.DataFrame(index=adata.var_names). return sc.AnnData(dblX, obs=obs, var=var). # Load data. # http: // cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_10k_v3/pbmc_10k_v3_filtered_feature_bc_matrix.h5. pbmc = sc.read_10x_h5(""./data/10x/pbmc_10k_v3_filtered_feature_bc_matrix.h5""). pbmc.var[""gene_symbols""] = pbmc.var.index. pbmc.var.set_index(""gene_ids"", inplace=True). dblt = simulate_doublets(pbmc). dblt.var[""gene_symbols""] = pbmc.var[""gene_symbols""]. pbmc.raw = pbmc. dblt.raw = dblt. pbmc = preprocess(pbmc). dblt = preprocess(dblt). sc.pp.pca(pbmc). pca_update(dblt, pbmc). umap = UMAP(). pbmc.obsm[""X_umap""] = umap.fit_transform(pbmc.obsm[""X_pca""]). dblt.obsm[""X_umap""] = umap.transform(dblt.obsm[""X_pca""]). sc.tl.embedding_density(pbmc, ""umap""). sc.tl.embedding_density(dblt, ""umap""). ```. </details>. <details> . <summary> Getting setup for datashader plots (much shorter) : </summary>. Make dataframe:. ```python. pbmcdf = pd.DataFrame(pbmc.obsm[""X_umap""], columns=[""x"", ""y""]) # Real data. dbltdf = pd.DataFrame(dblt.obsm[""X_umap""], columns=[""x"", ""y""]) # Simulated doublets. pbmcdf[""density""] = pbmc.obs[""umap_density""].values. dbltdf[""density""] = dblt.obs[""umap_density""].values. ```. Get plotting imports and canvas:. ```python. import datashader as ds. from datashader import transfer_functions as tf. from bokeh import palettes. canvas = ds.Canvas(plot_width=300, plot_height=300,. x_range=(pbmcdf[""x""].min() - .5, pbmcdf[""x""].max() + .5), . y_range=(pbmcdf[""y""].min() - .5, pbmcdf[""y""].max() + .5),. x_axis_type='linear', y_axis_type='linear'). ```. </details>. First, something simple. Basically just a 2d histogram with 300 x 300 bins:. ```python. real = canvas.points(pbm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:670,performance,load,loading,670,"I think I've got an example for you, which should be pretty easy for you to play around with in datashader. The example is doublet detection. I'm following the basic outline of the methods which simulate doublets, then project those onto the real data to find which barcode (/cell) the simulated doublets sit next to. Those barcodes are presumed to be doublets. So we'd expect that areas of mostly singlets in the real data would have a lower relative (to the real data) density of points in the simulated. I'm still exploring what the best way to summarize that difference in density is through. Here's an example with some pbmcs from 10x:. <details>. <summary> Setup (loading, simulating, and projecting) </summary>. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import sparse. from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):. adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""). sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True). sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000). sc.pp.log1p(adata). return adata. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. def simulate_doublets(adata, frac=.5):. """"""Simulate doublets from count data. . Params. ------. adata. The anndata object to sample from. Must have count data. frac. Fraction of total cells to simulate. """""". m, n = adata.X.shape. n_doublets = int(np.round(m * frac)). pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))). combos = np.random.randint(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:2252,performance,Load,Load,2252,"lace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. def simulate_doublets(adata, frac=.5):. """"""Simulate doublets from count data. . Params. ------. adata. The anndata object to sample from. Must have count data. frac. Fraction of total cells to simulate. """""". m, n = adata.X.shape. n_doublets = int(np.round(m * frac)). pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))). combos = np.random.randint(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). ). dblX = pos * adata.X. # TODO: Downsample total counts. srcs = np.sort(combos.reshape(n_doublets, 2), axis=1). obs = pd.DataFrame(srcs, columns=[""src1"", ""src2""]). var = pd.DataFrame(index=adata.var_names). return sc.AnnData(dblX, obs=obs, var=var). # Load data. # http: // cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_10k_v3/pbmc_10k_v3_filtered_feature_bc_matrix.h5. pbmc = sc.read_10x_h5(""./data/10x/pbmc_10k_v3_filtered_feature_bc_matrix.h5""). pbmc.var[""gene_symbols""] = pbmc.var.index. pbmc.var.set_index(""gene_ids"", inplace=True). dblt = simulate_doublets(pbmc). dblt.var[""gene_symbols""] = pbmc.var[""gene_symbols""]. pbmc.raw = pbmc. dblt.raw = dblt. pbmc = preprocess(pbmc). dblt = preprocess(dblt). sc.pp.pca(pbmc). pca_update(dblt, pbmc). umap = UMAP(). pbmc.obsm[""X_umap""] = umap.fit_transform(pbmc.obsm[""X_pca""]). dblt.obsm[""X_umap""] = umap.transform(dblt.obsm[""X_pca""]). sc.tl.embedding_density(pbmc, ""umap""). sc.tl.embedding_density(dblt, ""umap""). ```. </details>. <details> . <summary> Getting setup for datashader plots (much shorter) : </summary>. Make dataframe:. ```python. pbmcdf = pd.DataFrame(pbmc.obsm[""X_umap""], columns=[""x"", ""y""]) # Real data. dbltdf = pd.DataFrame(dblt.obsm[""X_umap""], columns=[""x"", ""y""]) # Simulated doublets. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:131,safety,detect,detection,131,"I think I've got an example for you, which should be pretty easy for you to play around with in datashader. The example is doublet detection. I'm following the basic outline of the methods which simulate doublets, then project those onto the real data to find which barcode (/cell) the simulated doublets sit next to. Those barcodes are presumed to be doublets. So we'd expect that areas of mostly singlets in the real data would have a lower relative (to the real data) density of points in the simulated. I'm still exploring what the best way to summarize that difference in density is through. Here's an example with some pbmcs from 10x:. <details>. <summary> Setup (loading, simulating, and projecting) </summary>. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import sparse. from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):. adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""). sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True). sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000). sc.pp.log1p(adata). return adata. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. def simulate_doublets(adata, frac=.5):. """"""Simulate doublets from count data. . Params. ------. adata. The anndata object to sample from. Must have count data. frac. Fraction of total cells to simulate. """""". m, n = adata.X.shape. n_doublets = int(np.round(m * frac)). pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))). combos = np.random.randint(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:131,security,detect,detection,131,"I think I've got an example for you, which should be pretty easy for you to play around with in datashader. The example is doublet detection. I'm following the basic outline of the methods which simulate doublets, then project those onto the real data to find which barcode (/cell) the simulated doublets sit next to. Those barcodes are presumed to be doublets. So we'd expect that areas of mostly singlets in the real data would have a lower relative (to the real data) density of points in the simulated. I'm still exploring what the best way to summarize that difference in density is through. Here's an example with some pbmcs from 10x:. <details>. <summary> Setup (loading, simulating, and projecting) </summary>. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import sparse. from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):. adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""). sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True). sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000). sc.pp.log1p(adata). return adata. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. def simulate_doublets(adata, frac=.5):. """"""Simulate doublets from count data. . Params. ------. adata. The anndata object to sample from. Must have count data. frac. Fraction of total cells to simulate. """""". m, n = adata.X.shape. n_doublets = int(np.round(m * frac)). pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))). combos = np.random.randint(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:195,testability,simul,simulate,195,"I think I've got an example for you, which should be pretty easy for you to play around with in datashader. The example is doublet detection. I'm following the basic outline of the methods which simulate doublets, then project those onto the real data to find which barcode (/cell) the simulated doublets sit next to. Those barcodes are presumed to be doublets. So we'd expect that areas of mostly singlets in the real data would have a lower relative (to the real data) density of points in the simulated. I'm still exploring what the best way to summarize that difference in density is through. Here's an example with some pbmcs from 10x:. <details>. <summary> Setup (loading, simulating, and projecting) </summary>. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import sparse. from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):. adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""). sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True). sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000). sc.pp.log1p(adata). return adata. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. def simulate_doublets(adata, frac=.5):. """"""Simulate doublets from count data. . Params. ------. adata. The anndata object to sample from. Must have count data. frac. Fraction of total cells to simulate. """""". m, n = adata.X.shape. n_doublets = int(np.round(m * frac)). pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))). combos = np.random.randint(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:286,testability,simul,simulated,286,"I think I've got an example for you, which should be pretty easy for you to play around with in datashader. The example is doublet detection. I'm following the basic outline of the methods which simulate doublets, then project those onto the real data to find which barcode (/cell) the simulated doublets sit next to. Those barcodes are presumed to be doublets. So we'd expect that areas of mostly singlets in the real data would have a lower relative (to the real data) density of points in the simulated. I'm still exploring what the best way to summarize that difference in density is through. Here's an example with some pbmcs from 10x:. <details>. <summary> Setup (loading, simulating, and projecting) </summary>. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import sparse. from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):. adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""). sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True). sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000). sc.pp.log1p(adata). return adata. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. def simulate_doublets(adata, frac=.5):. """"""Simulate doublets from count data. . Params. ------. adata. The anndata object to sample from. Must have count data. frac. Fraction of total cells to simulate. """""". m, n = adata.X.shape. n_doublets = int(np.round(m * frac)). pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))). combos = np.random.randint(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:496,testability,simul,simulated,496,"I think I've got an example for you, which should be pretty easy for you to play around with in datashader. The example is doublet detection. I'm following the basic outline of the methods which simulate doublets, then project those onto the real data to find which barcode (/cell) the simulated doublets sit next to. Those barcodes are presumed to be doublets. So we'd expect that areas of mostly singlets in the real data would have a lower relative (to the real data) density of points in the simulated. I'm still exploring what the best way to summarize that difference in density is through. Here's an example with some pbmcs from 10x:. <details>. <summary> Setup (loading, simulating, and projecting) </summary>. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import sparse. from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):. adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""). sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True). sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000). sc.pp.log1p(adata). return adata. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. def simulate_doublets(adata, frac=.5):. """"""Simulate doublets from count data. . Params. ------. adata. The anndata object to sample from. Must have count data. frac. Fraction of total cells to simulate. """""". m, n = adata.X.shape. n_doublets = int(np.round(m * frac)). pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))). combos = np.random.randint(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:679,testability,simul,simulating,679,"I think I've got an example for you, which should be pretty easy for you to play around with in datashader. The example is doublet detection. I'm following the basic outline of the methods which simulate doublets, then project those onto the real data to find which barcode (/cell) the simulated doublets sit next to. Those barcodes are presumed to be doublets. So we'd expect that areas of mostly singlets in the real data would have a lower relative (to the real data) density of points in the simulated. I'm still exploring what the best way to summarize that difference in density is through. Here's an example with some pbmcs from 10x:. <details>. <summary> Setup (loading, simulating, and projecting) </summary>. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import sparse. from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):. adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""). sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True). sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000). sc.pp.log1p(adata). return adata. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. def simulate_doublets(adata, frac=.5):. """"""Simulate doublets from count data. . Params. ------. adata. The anndata object to sample from. Must have count data. frac. Fraction of total cells to simulate. """""". m, n = adata.X.shape. n_doublets = int(np.round(m * frac)). pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))). combos = np.random.randint(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:1512,testability,Simul,Simulate,1512," exploring what the best way to summarize that difference in density is through. Here's an example with some pbmcs from 10x:. <details>. <summary> Setup (loading, simulating, and projecting) </summary>. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import sparse. from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):. adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""). sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True). sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000). sc.pp.log1p(adata). return adata. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. def simulate_doublets(adata, frac=.5):. """"""Simulate doublets from count data. . Params. ------. adata. The anndata object to sample from. Must have count data. frac. Fraction of total cells to simulate. """""". m, n = adata.X.shape. n_doublets = int(np.round(m * frac)). pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))). combos = np.random.randint(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). ). dblX = pos * adata.X. # TODO: Downsample total counts. srcs = np.sort(combos.reshape(n_doublets, 2), axis=1). obs = pd.DataFrame(srcs, columns=[""src1"", ""src2""]). var = pd.DataFrame(index=adata.var_names). return sc.AnnData(dblX, obs=obs, var=var). # Load data. # http: // cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_10k_v3/pbmc_10k_v3_filtered_feature_bc_matrix.h5. pbmc = sc.read_10x_h5(""./data/10x/pbmc_10k_v3_filtered_feature_bc_matrix.h5""). pbmc.var[""gene_symbols""] = pbmc.var.index. pbmc.var.set_index(""gen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:1662,testability,simul,simulate,1662,"up (loading, simulating, and projecting) </summary>. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import sparse. from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):. adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""). sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True). sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000). sc.pp.log1p(adata). return adata. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. def simulate_doublets(adata, frac=.5):. """"""Simulate doublets from count data. . Params. ------. adata. The anndata object to sample from. Must have count data. frac. Fraction of total cells to simulate. """""". m, n = adata.X.shape. n_doublets = int(np.round(m * frac)). pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))). combos = np.random.randint(0, m, (n_doublets * 2)). pos = sparse.csr_matrix(. (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), . shape=(n_doublets, m). ). dblX = pos * adata.X. # TODO: Downsample total counts. srcs = np.sort(combos.reshape(n_doublets, 2), axis=1). obs = pd.DataFrame(srcs, columns=[""src1"", ""src2""]). var = pd.DataFrame(index=adata.var_names). return sc.AnnData(dblX, obs=obs, var=var). # Load data. # http: // cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_10k_v3/pbmc_10k_v3_filtered_feature_bc_matrix.h5. pbmc = sc.read_10x_h5(""./data/10x/pbmc_10k_v3_filtered_feature_bc_matrix.h5""). pbmc.var[""gene_symbols""] = pbmc.var.index. pbmc.var.set_index(""gene_ids"", inplace=True). dblt = simulate_doublets(pbmc). dblt.var[""gene_symbols""] = pbmc.var[""gene_symbols""]. pbmc.raw = pbmc. dblt.raw = dblt. pbmc = p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:3234,testability,Simul,Simulated,3234," var=var). # Load data. # http: // cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_10k_v3/pbmc_10k_v3_filtered_feature_bc_matrix.h5. pbmc = sc.read_10x_h5(""./data/10x/pbmc_10k_v3_filtered_feature_bc_matrix.h5""). pbmc.var[""gene_symbols""] = pbmc.var.index. pbmc.var.set_index(""gene_ids"", inplace=True). dblt = simulate_doublets(pbmc). dblt.var[""gene_symbols""] = pbmc.var[""gene_symbols""]. pbmc.raw = pbmc. dblt.raw = dblt. pbmc = preprocess(pbmc). dblt = preprocess(dblt). sc.pp.pca(pbmc). pca_update(dblt, pbmc). umap = UMAP(). pbmc.obsm[""X_umap""] = umap.fit_transform(pbmc.obsm[""X_pca""]). dblt.obsm[""X_umap""] = umap.transform(dblt.obsm[""X_pca""]). sc.tl.embedding_density(pbmc, ""umap""). sc.tl.embedding_density(dblt, ""umap""). ```. </details>. <details> . <summary> Getting setup for datashader plots (much shorter) : </summary>. Make dataframe:. ```python. pbmcdf = pd.DataFrame(pbmc.obsm[""X_umap""], columns=[""x"", ""y""]) # Real data. dbltdf = pd.DataFrame(dblt.obsm[""X_umap""], columns=[""x"", ""y""]) # Simulated doublets. pbmcdf[""density""] = pbmc.obs[""umap_density""].values. dbltdf[""density""] = dblt.obs[""umap_density""].values. ```. Get plotting imports and canvas:. ```python. import datashader as ds. from datashader import transfer_functions as tf. from bokeh import palettes. canvas = ds.Canvas(plot_width=300, plot_height=300,. x_range=(pbmcdf[""x""].min() - .5, pbmcdf[""x""].max() + .5), . y_range=(pbmcdf[""y""].min() - .5, pbmcdf[""y""].max() + .5),. x_axis_type='linear', y_axis_type='linear'). ```. </details>. First, something simple. Basically just a 2d histogram with 300 x 300 bins:. ```python. real = canvas.points(pbmcdf, 'x', 'y', ds.count()). sim = canvas.points(dbltdf, 'x', 'y', ds.count()). tf.Images(. tf.shade(real, name=""pbmcs""),. tf.shade(sim, name=""doublet""),. tf.shade(sim / (real + sim)),. ). ```. <img width=""857"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/55789263-532a0800-5afd-11e9-8c58-4ecde66a2717.png"">. Using the weights from your method, while making th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:3763,testability,simpl,simple,3763,"mc.obsm[""X_umap""] = umap.fit_transform(pbmc.obsm[""X_pca""]). dblt.obsm[""X_umap""] = umap.transform(dblt.obsm[""X_pca""]). sc.tl.embedding_density(pbmc, ""umap""). sc.tl.embedding_density(dblt, ""umap""). ```. </details>. <details> . <summary> Getting setup for datashader plots (much shorter) : </summary>. Make dataframe:. ```python. pbmcdf = pd.DataFrame(pbmc.obsm[""X_umap""], columns=[""x"", ""y""]) # Real data. dbltdf = pd.DataFrame(dblt.obsm[""X_umap""], columns=[""x"", ""y""]) # Simulated doublets. pbmcdf[""density""] = pbmc.obs[""umap_density""].values. dbltdf[""density""] = dblt.obs[""umap_density""].values. ```. Get plotting imports and canvas:. ```python. import datashader as ds. from datashader import transfer_functions as tf. from bokeh import palettes. canvas = ds.Canvas(plot_width=300, plot_height=300,. x_range=(pbmcdf[""x""].min() - .5, pbmcdf[""x""].max() + .5), . y_range=(pbmcdf[""y""].min() - .5, pbmcdf[""y""].max() + .5),. x_axis_type='linear', y_axis_type='linear'). ```. </details>. First, something simple. Basically just a 2d histogram with 300 x 300 bins:. ```python. real = canvas.points(pbmcdf, 'x', 'y', ds.count()). sim = canvas.points(dbltdf, 'x', 'y', ds.count()). tf.Images(. tf.shade(real, name=""pbmcs""),. tf.shade(sim, name=""doublet""),. tf.shade(sim / (real + sim)),. ). ```. <img width=""857"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/55789263-532a0800-5afd-11e9-8c58-4ecde66a2717.png"">. Using the weights from your method, while making the plots look more similar (though there's something weird going on with non-overlapping areas here):. ```python. real_density = canvas.points(pbmcdf, 'x', 'y', ds.mean(""density"")). sim_density = canvas.points(dbltdf, 'x', 'y', ds.mean(""density"")). tf.Images(. tf.spread(tf.shade(real_density, name=""pbmc""), px=2),. tf.spread(tf.shade(sim_density, name=""doublet""), px=2),. tf.spread(. tf.shade(. sim_density / (sim_density + real_density), . cmap=palettes.Viridis256. ), . px=2,. name=""sim / (real + sim)"". ),. tf.spread(. tf.sh",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:3763,usability,simpl,simple,3763,"mc.obsm[""X_umap""] = umap.fit_transform(pbmc.obsm[""X_pca""]). dblt.obsm[""X_umap""] = umap.transform(dblt.obsm[""X_pca""]). sc.tl.embedding_density(pbmc, ""umap""). sc.tl.embedding_density(dblt, ""umap""). ```. </details>. <details> . <summary> Getting setup for datashader plots (much shorter) : </summary>. Make dataframe:. ```python. pbmcdf = pd.DataFrame(pbmc.obsm[""X_umap""], columns=[""x"", ""y""]) # Real data. dbltdf = pd.DataFrame(dblt.obsm[""X_umap""], columns=[""x"", ""y""]) # Simulated doublets. pbmcdf[""density""] = pbmc.obs[""umap_density""].values. dbltdf[""density""] = dblt.obs[""umap_density""].values. ```. Get plotting imports and canvas:. ```python. import datashader as ds. from datashader import transfer_functions as tf. from bokeh import palettes. canvas = ds.Canvas(plot_width=300, plot_height=300,. x_range=(pbmcdf[""x""].min() - .5, pbmcdf[""x""].max() + .5), . y_range=(pbmcdf[""y""].min() - .5, pbmcdf[""y""].max() + .5),. x_axis_type='linear', y_axis_type='linear'). ```. </details>. First, something simple. Basically just a 2d histogram with 300 x 300 bins:. ```python. real = canvas.points(pbmcdf, 'x', 'y', ds.count()). sim = canvas.points(dbltdf, 'x', 'y', ds.count()). tf.Images(. tf.shade(real, name=""pbmcs""),. tf.shade(sim, name=""doublet""),. tf.shade(sim / (real + sim)),. ). ```. <img width=""857"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/55789263-532a0800-5afd-11e9-8c58-4ecde66a2717.png"">. Using the weights from your method, while making the plots look more similar (though there's something weird going on with non-overlapping areas here):. ```python. real_density = canvas.points(pbmcdf, 'x', 'y', ds.mean(""density"")). sim_density = canvas.points(dbltdf, 'x', 'y', ds.mean(""density"")). tf.Images(. tf.spread(tf.shade(real_density, name=""pbmc""), px=2),. tf.spread(tf.shade(sim_density, name=""doublet""), px=2),. tf.spread(. tf.shade(. sim_density / (sim_density + real_density), . cmap=palettes.Viridis256. ), . px=2,. name=""sim / (real + sim)"". ),. tf.spread(. tf.sh",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:4093,usability,user,user-images,4093," = pd.DataFrame(pbmc.obsm[""X_umap""], columns=[""x"", ""y""]) # Real data. dbltdf = pd.DataFrame(dblt.obsm[""X_umap""], columns=[""x"", ""y""]) # Simulated doublets. pbmcdf[""density""] = pbmc.obs[""umap_density""].values. dbltdf[""density""] = dblt.obs[""umap_density""].values. ```. Get plotting imports and canvas:. ```python. import datashader as ds. from datashader import transfer_functions as tf. from bokeh import palettes. canvas = ds.Canvas(plot_width=300, plot_height=300,. x_range=(pbmcdf[""x""].min() - .5, pbmcdf[""x""].max() + .5), . y_range=(pbmcdf[""y""].min() - .5, pbmcdf[""y""].max() + .5),. x_axis_type='linear', y_axis_type='linear'). ```. </details>. First, something simple. Basically just a 2d histogram with 300 x 300 bins:. ```python. real = canvas.points(pbmcdf, 'x', 'y', ds.count()). sim = canvas.points(dbltdf, 'x', 'y', ds.count()). tf.Images(. tf.shade(real, name=""pbmcs""),. tf.shade(sim, name=""doublet""),. tf.shade(sim / (real + sim)),. ). ```. <img width=""857"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/55789263-532a0800-5afd-11e9-8c58-4ecde66a2717.png"">. Using the weights from your method, while making the plots look more similar (though there's something weird going on with non-overlapping areas here):. ```python. real_density = canvas.points(pbmcdf, 'x', 'y', ds.mean(""density"")). sim_density = canvas.points(dbltdf, 'x', 'y', ds.mean(""density"")). tf.Images(. tf.spread(tf.shade(real_density, name=""pbmc""), px=2),. tf.spread(tf.shade(sim_density, name=""doublet""), px=2),. tf.spread(. tf.shade(. sim_density / (sim_density + real_density), . cmap=palettes.Viridis256. ), . px=2,. name=""sim / (real + sim)"". ),. tf.spread(. tf.shade(. sim_density - real_density, . cmap=palettes.Viridis256. ), . px=2,. name=""sim - real"". ),. ).cols(2). ```. <img width=""857"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/55790698-435ff300-5b00-11e9-8ea7-007e98317713.png"">. Edit: Sorry for the huge info dump! But also, I think I figured out a more reasonab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:4921,usability,user,user-images,4921,"r_functions as tf. from bokeh import palettes. canvas = ds.Canvas(plot_width=300, plot_height=300,. x_range=(pbmcdf[""x""].min() - .5, pbmcdf[""x""].max() + .5), . y_range=(pbmcdf[""y""].min() - .5, pbmcdf[""y""].max() + .5),. x_axis_type='linear', y_axis_type='linear'). ```. </details>. First, something simple. Basically just a 2d histogram with 300 x 300 bins:. ```python. real = canvas.points(pbmcdf, 'x', 'y', ds.count()). sim = canvas.points(dbltdf, 'x', 'y', ds.count()). tf.Images(. tf.shade(real, name=""pbmcs""),. tf.shade(sim, name=""doublet""),. tf.shade(sim / (real + sim)),. ). ```. <img width=""857"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/55789263-532a0800-5afd-11e9-8c58-4ecde66a2717.png"">. Using the weights from your method, while making the plots look more similar (though there's something weird going on with non-overlapping areas here):. ```python. real_density = canvas.points(pbmcdf, 'x', 'y', ds.mean(""density"")). sim_density = canvas.points(dbltdf, 'x', 'y', ds.mean(""density"")). tf.Images(. tf.spread(tf.shade(real_density, name=""pbmc""), px=2),. tf.spread(tf.shade(sim_density, name=""doublet""), px=2),. tf.spread(. tf.shade(. sim_density / (sim_density + real_density), . cmap=palettes.Viridis256. ), . px=2,. name=""sim / (real + sim)"". ),. tf.spread(. tf.shade(. sim_density - real_density, . cmap=palettes.Viridis256. ), . px=2,. name=""sim - real"". ),. ).cols(2). ```. <img width=""857"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/55790698-435ff300-5b00-11e9-8ea7-007e98317713.png"">. Edit: Sorry for the huge info dump! But also, I think I figured out a more reasonable representation:. ```python. real_norm = real / real.sum(). sim_norm = sim / sim.sum(). tf.Images(. tf.shade(real, name=""pbmcs""),. tf.shade(sim, name=""doublet""),. tf.shade((sim_norm - real_norm) / (real_norm + sim_norm), cmap=palettes.RdYlBu11),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/55851343-8cb15080-5b9b-11e9-8ed7-1fb3cf6b34df.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:5373,usability,user,user-images,5373,"r_functions as tf. from bokeh import palettes. canvas = ds.Canvas(plot_width=300, plot_height=300,. x_range=(pbmcdf[""x""].min() - .5, pbmcdf[""x""].max() + .5), . y_range=(pbmcdf[""y""].min() - .5, pbmcdf[""y""].max() + .5),. x_axis_type='linear', y_axis_type='linear'). ```. </details>. First, something simple. Basically just a 2d histogram with 300 x 300 bins:. ```python. real = canvas.points(pbmcdf, 'x', 'y', ds.count()). sim = canvas.points(dbltdf, 'x', 'y', ds.count()). tf.Images(. tf.shade(real, name=""pbmcs""),. tf.shade(sim, name=""doublet""),. tf.shade(sim / (real + sim)),. ). ```. <img width=""857"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/55789263-532a0800-5afd-11e9-8c58-4ecde66a2717.png"">. Using the weights from your method, while making the plots look more similar (though there's something weird going on with non-overlapping areas here):. ```python. real_density = canvas.points(pbmcdf, 'x', 'y', ds.mean(""density"")). sim_density = canvas.points(dbltdf, 'x', 'y', ds.mean(""density"")). tf.Images(. tf.spread(tf.shade(real_density, name=""pbmc""), px=2),. tf.spread(tf.shade(sim_density, name=""doublet""), px=2),. tf.spread(. tf.shade(. sim_density / (sim_density + real_density), . cmap=palettes.Viridis256. ), . px=2,. name=""sim / (real + sim)"". ),. tf.spread(. tf.shade(. sim_density - real_density, . cmap=palettes.Viridis256. ), . px=2,. name=""sim - real"". ),. ).cols(2). ```. <img width=""857"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/55790698-435ff300-5b00-11e9-8ea7-007e98317713.png"">. Edit: Sorry for the huge info dump! But also, I think I figured out a more reasonable representation:. ```python. real_norm = real / real.sum(). sim_norm = sim / sim.sum(). tf.Images(. tf.shade(real, name=""pbmcs""),. tf.shade(sim, name=""doublet""),. tf.shade((sim_norm - real_norm) / (real_norm + sim_norm), cmap=palettes.RdYlBu11),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/55851343-8cb15080-5b9b-11e9-8ed7-1fb3cf6b34df.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:65,energy efficiency,cool,cool,65,"Sorry for the late reply, I was afk for a week. . This is really cool... What I had in mind is most similar to the 2D histogram you show. For the second part you fit the umap to one dataset and use that transformation to project the doublets in, right? What I do is join the two datasets and make a combined umap embedding. I can see how your approach makes more sense for doublet detection, but in a general case it's probably not as valid (i.e., when one dataset isn't ""fake"" data). . I will use this to play around a bit with the package when time permits though... thanks a lot for the extensive code! And we discussed similar things for the normalization of the data for the subtraction above...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:203,integrability,transform,transformation,203,"Sorry for the late reply, I was afk for a week. . This is really cool... What I had in mind is most similar to the 2D histogram you show. For the second part you fit the umap to one dataset and use that transformation to project the doublets in, right? What I do is join the two datasets and make a combined umap embedding. I can see how your approach makes more sense for doublet detection, but in a general case it's probably not as valid (i.e., when one dataset isn't ""fake"" data). . I will use this to play around a bit with the package when time permits though... thanks a lot for the extensive code! And we discussed similar things for the normalization of the data for the subtraction above...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:680,integrability,sub,subtraction,680,"Sorry for the late reply, I was afk for a week. . This is really cool... What I had in mind is most similar to the 2D histogram you show. For the second part you fit the umap to one dataset and use that transformation to project the doublets in, right? What I do is join the two datasets and make a combined umap embedding. I can see how your approach makes more sense for doublet detection, but in a general case it's probably not as valid (i.e., when one dataset isn't ""fake"" data). . I will use this to play around a bit with the package when time permits though... thanks a lot for the extensive code! And we discussed similar things for the normalization of the data for the subtraction above...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:203,interoperability,transform,transformation,203,"Sorry for the late reply, I was afk for a week. . This is really cool... What I had in mind is most similar to the 2D histogram you show. For the second part you fit the umap to one dataset and use that transformation to project the doublets in, right? What I do is join the two datasets and make a combined umap embedding. I can see how your approach makes more sense for doublet detection, but in a general case it's probably not as valid (i.e., when one dataset isn't ""fake"" data). . I will use this to play around a bit with the package when time permits though... thanks a lot for the extensive code! And we discussed similar things for the normalization of the data for the subtraction above...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:533,modifiability,pac,package,533,"Sorry for the late reply, I was afk for a week. . This is really cool... What I had in mind is most similar to the 2D histogram you show. For the second part you fit the umap to one dataset and use that transformation to project the doublets in, right? What I do is join the two datasets and make a combined umap embedding. I can see how your approach makes more sense for doublet detection, but in a general case it's probably not as valid (i.e., when one dataset isn't ""fake"" data). . I will use this to play around a bit with the package when time permits though... thanks a lot for the extensive code! And we discussed similar things for the normalization of the data for the subtraction above...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:590,modifiability,extens,extensive,590,"Sorry for the late reply, I was afk for a week. . This is really cool... What I had in mind is most similar to the 2D histogram you show. For the second part you fit the umap to one dataset and use that transformation to project the doublets in, right? What I do is join the two datasets and make a combined umap embedding. I can see how your approach makes more sense for doublet detection, but in a general case it's probably not as valid (i.e., when one dataset isn't ""fake"" data). . I will use this to play around a bit with the package when time permits though... thanks a lot for the extensive code! And we discussed similar things for the normalization of the data for the subtraction above...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:546,performance,time,time,546,"Sorry for the late reply, I was afk for a week. . This is really cool... What I had in mind is most similar to the 2D histogram you show. For the second part you fit the umap to one dataset and use that transformation to project the doublets in, right? What I do is join the two datasets and make a combined umap embedding. I can see how your approach makes more sense for doublet detection, but in a general case it's probably not as valid (i.e., when one dataset isn't ""fake"" data). . I will use this to play around a bit with the package when time permits though... thanks a lot for the extensive code! And we discussed similar things for the normalization of the data for the subtraction above...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:381,safety,detect,detection,381,"Sorry for the late reply, I was afk for a week. . This is really cool... What I had in mind is most similar to the 2D histogram you show. For the second part you fit the umap to one dataset and use that transformation to project the doublets in, right? What I do is join the two datasets and make a combined umap embedding. I can see how your approach makes more sense for doublet detection, but in a general case it's probably not as valid (i.e., when one dataset isn't ""fake"" data). . I will use this to play around a bit with the package when time permits though... thanks a lot for the extensive code! And we discussed similar things for the normalization of the data for the subtraction above...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:435,safety,valid,valid,435,"Sorry for the late reply, I was afk for a week. . This is really cool... What I had in mind is most similar to the 2D histogram you show. For the second part you fit the umap to one dataset and use that transformation to project the doublets in, right? What I do is join the two datasets and make a combined umap embedding. I can see how your approach makes more sense for doublet detection, but in a general case it's probably not as valid (i.e., when one dataset isn't ""fake"" data). . I will use this to play around a bit with the package when time permits though... thanks a lot for the extensive code! And we discussed similar things for the normalization of the data for the subtraction above...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:381,security,detect,detection,381,"Sorry for the late reply, I was afk for a week. . This is really cool... What I had in mind is most similar to the 2D histogram you show. For the second part you fit the umap to one dataset and use that transformation to project the doublets in, right? What I do is join the two datasets and make a combined umap embedding. I can see how your approach makes more sense for doublet detection, but in a general case it's probably not as valid (i.e., when one dataset isn't ""fake"" data). . I will use this to play around a bit with the package when time permits though... thanks a lot for the extensive code! And we discussed similar things for the normalization of the data for the subtraction above...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/pull/576:564,availability,consist,consistent,564,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:597,availability,sli,slight,597,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:630,availability,consist,consistence,630,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:25,deployability,fail,fail,25,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:777,deployability,fail,fail,777,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:25,reliability,fail,fail,25,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:597,reliability,sli,slight,597,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:777,reliability,fail,fail,777,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:12,safety,test,tests,12,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:681,safety,test,tests,681,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:742,safety,test,tests,742,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:12,testability,test,tests,12,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:681,testability,test,tests,681,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:742,testability,test,tests,742,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:165,usability,visual,visually,165,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:564,usability,consist,consistent,564,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:630,usability,consist,consistence,630,"Great! Some tests should fail as there are probably differences in the neighbor algorithm. This is also why this is a backwards-compat breaking change. Can you just visually inspect https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html and see what's going on? This is another notebook that should still do something meaningful after the change: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170502_paul15/paul15.ipynb. And finally, of course, https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html should give somewhat consistent results. But I expect slight variations and no perfect consistence... Actually, I'd expect the associated tests (https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py) to fail. Can you check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:178,usability,tool,tools,178,"Finally, I want to mention that we also need to export the following functions from UMAP: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/tools/_umap.py#L107",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:314,deployability,fail,fails,314,"@falexwolf . Hi, Alex. Yes, i'm checking these. Actually, it somehow passes [test_pbmc3k](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py). Only [test_paga_paul15_subsampled](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py) fails. It seems that adjacency matrix is a bit different after the change and this affects paga connectivities. But it is preliminary, i'm checking still.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:314,reliability,fail,fails,314,"@falexwolf . Hi, Alex. Yes, i'm checking these. Actually, it somehow passes [test_pbmc3k](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py). Only [test_paga_paul15_subsampled](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py) fails. It seems that adjacency matrix is a bit different after the change and this affects paga connectivities. But it is preliminary, i'm checking still.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:144,safety,test,tests,144,"@falexwolf . Hi, Alex. Yes, i'm checking these. Actually, it somehow passes [test_pbmc3k](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py). Only [test_paga_paul15_subsampled](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py) fails. It seems that adjacency matrix is a bit different after the change and this affects paga connectivities. But it is preliminary, i'm checking still.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:266,safety,test,tests,266,"@falexwolf . Hi, Alex. Yes, i'm checking these. Actually, it somehow passes [test_pbmc3k](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py). Only [test_paga_paul15_subsampled](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py) fails. It seems that adjacency matrix is a bit different after the change and this affects paga connectivities. But it is preliminary, i'm checking still.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:144,testability,test,tests,144,"@falexwolf . Hi, Alex. Yes, i'm checking these. Actually, it somehow passes [test_pbmc3k](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py). Only [test_paga_paul15_subsampled](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py) fails. It seems that adjacency matrix is a bit different after the change and this affects paga connectivities. But it is preliminary, i'm checking still.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:266,testability,test,tests,266,"@falexwolf . Hi, Alex. Yes, i'm checking these. Actually, it somehow passes [test_pbmc3k](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_pbmc3k.py). Only [test_paga_paul15_subsampled](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py) fails. It seems that adjacency matrix is a bit different after the change and this affects paga connectivities. But it is preliminary, i'm checking still.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:114,usability,tool,tools,114,"> Finally, I want to mention that we also need to export the following functions from UMAP:. > . > [scanpy/scanpy/tools/_umap.py](https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/tools/_umap.py#L107). > . > Line 107 in [97c8b54](/theislab/scanpy/commit/97c8b54ec884ac8e8396a80b6782a0d59a17a874). > . > from ..neighbors.umap.umap_ import find_ab_params, simplicial_set_embedding. This is already in the PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:218,usability,tool,tools,218,"> Finally, I want to mention that we also need to export the following functions from UMAP:. > . > [scanpy/scanpy/tools/_umap.py](https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/tools/_umap.py#L107). > . > Line 107 in [97c8b54](/theislab/scanpy/commit/97c8b54ec884ac8e8396a80b6782a0d59a17a874). > . > from ..neighbors.umap.umap_ import find_ab_params, simplicial_set_embedding. This is already in the PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:72,availability,sli,slightly,72,"Great! Yes, I would have expected that the adjacency matrix will differ slightly and hence, `test_paga_paul15` fails. We'll need to rerun and upload https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html with the new version in that case and also update the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:111,deployability,fail,fails,111,"Great! Yes, I would have expected that the adjacency matrix will differ slightly and hence, `test_paga_paul15` fails. We'll need to rerun and upload https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html with the new version in that case and also update the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:229,deployability,version,version,229,"Great! Yes, I would have expected that the adjacency matrix will differ slightly and hence, `test_paga_paul15` fails. We'll need to rerun and upload https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html with the new version in that case and also update the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:259,deployability,updat,update,259,"Great! Yes, I would have expected that the adjacency matrix will differ slightly and hence, `test_paga_paul15` fails. We'll need to rerun and upload https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html with the new version in that case and also update the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:229,integrability,version,version,229,"Great! Yes, I would have expected that the adjacency matrix will differ slightly and hence, `test_paga_paul15` fails. We'll need to rerun and upload https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html with the new version in that case and also update the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:229,modifiability,version,version,229,"Great! Yes, I would have expected that the adjacency matrix will differ slightly and hence, `test_paga_paul15` fails. We'll need to rerun and upload https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html with the new version in that case and also update the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:72,reliability,sli,slightly,72,"Great! Yes, I would have expected that the adjacency matrix will differ slightly and hence, `test_paga_paul15` fails. We'll need to rerun and upload https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html with the new version in that case and also update the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:111,reliability,fail,fails,111,"Great! Yes, I would have expected that the adjacency matrix will differ slightly and hence, `test_paga_paul15` fails. We'll need to rerun and upload https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html with the new version in that case and also update the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:259,safety,updat,update,259,"Great! Yes, I would have expected that the adjacency matrix will differ slightly and hence, `test_paga_paul15` fails. We'll need to rerun and upload https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html with the new version in that case and also update the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:270,safety,test,tests,270,"Great! Yes, I would have expected that the adjacency matrix will differ slightly and hence, `test_paga_paul15` fails. We'll need to rerun and upload https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html with the new version in that case and also update the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:259,security,updat,update,259,"Great! Yes, I would have expected that the adjacency matrix will differ slightly and hence, `test_paga_paul15` fails. We'll need to rerun and upload https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html with the new version in that case and also update the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:270,testability,test,tests,270,"Great! Yes, I would have expected that the adjacency matrix will differ slightly and hence, `test_paga_paul15` fails. We'll need to rerun and upload https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html with the new version in that case and also update the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:581,deployability,depend,dependency,581,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:396,energy efficiency,model,model,396,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:426,energy efficiency,model,model,426,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:440,energy efficiency,model,model,440,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:381,integrability,wrap,wrapping,381,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:446,integrability,transform,transform,446,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:581,integrability,depend,dependency,581,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:446,interoperability,transform,transform,446,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:581,modifiability,depend,dependency,581,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:638,reliability,doe,does,638,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:581,safety,depend,dependency,581,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:396,security,model,model,396,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:426,security,model,model,426,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:440,security,model,model,440,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:581,testability,depend,dependency,581,"One other thing. We'd like to have two new convenience functions:. ```. def neighbors_update(adata, adata_new). def umap_update(adata, adata_new). ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping. ```. model = umap.UMAP(seed=1234). model.fit(X). model.transform(new_X). ```. For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:142,deployability,roll,rolled,142,"@falexwolf @Koncopd any chance you've had time to look into the projection of new data into an existing UMAP yet? Additionally, would this be rolled out with an equivalent `pca_update`? I'd be interested in taking some part of implementing this if you haven't worked it out already.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:42,performance,time,time,42,"@falexwolf @Koncopd any chance you've had time to look into the projection of new data into an existing UMAP yet? Additionally, would this be rolled out with an equivalent `pca_update`? I'd be interested in taking some part of implementing this if you haven't worked it out already.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:66,performance,time,time,66,"Hi, @ivirshup. No, i haven't looked into this yet, so if you have time and ideas, i'll appreciate any help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:102,usability,help,help,102,"Hi, @ivirshup. No, i haven't looked into this yet, so if you have time and ideas, i'll appreciate any help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:1535,availability,mask,masking,1535,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:1094,deployability,api,api,1094,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:161,energy efficiency,predict,prediction,161,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:206,energy efficiency,current,current,206,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:1281,energy efficiency,current,current,1281,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:845,integrability,transform,transformation,845,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:1094,integrability,api,api,1094,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:845,interoperability,transform,transformation,845,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:1094,interoperability,api,api,1094,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:885,performance,disk,disk,885,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:1166,reliability,Doe,Does,1166,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:1303,reliability,doe,doesn,1303,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:1353,reliability,doe,does,1353,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:68,safety,test,test,68,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:161,safety,predict,prediction,161,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:915,safety,compl,complicated,915,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:1464,safety,compl,complicate,1464,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:915,security,compl,complicated,915,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:1464,security,compl,complicate,1464,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:68,testability,test,test,68,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:175,testability,simul,simulation,175,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:777,testability,plan,planning,777,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:1085,usability,learn,learning,1085,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>. <summary> Basic PCA projection </summary>. ```python. def pca_update(tgt, src, inplace=True):. # TODO: Make sure we know the settings (just whether to center?) from src. if not inplace:. tgt = tgt.copy(). if sparse.issparse(tgt.X):. X = tgt.X.toarray(). else:. X = tgt.X.copy(). X -= np.asarray(tgt.X.mean(axis=0)). tgt_pca = np.dot(X, src.varm[""PCs""]). tgt.obsm[""X_pca""] = tgt_pca. return tgt. ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated. * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up? * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:. * Does the syntax still work for cases other than 1-to-1 transfer? . * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`. * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:154,modifiability,concern,concerned,154,"@ivirshup, fantastic! Yes, it would be great if you could work on this. We should also keep it separate from this PR, in this case. This PR should remain concerned with getting rid of the duplicate code. I'll open up an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:154,testability,concern,concerned,154,"@ivirshup, fantastic! Yes, it would be great if you could work on this. We should also keep it separate from this PR, in this case. This PR should remain concerned with getting rid of the duplicate code. I'll open up an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:18,deployability,version,version,18,"Added the initial version of `neighbors_update`, only works with `use_rep='X' ` in `sc.pp.neighbors` for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:18,integrability,version,version,18,"Added the initial version of `neighbors_update`, only works with `use_rep='X' ` in `sc.pp.neighbors` for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:18,modifiability,version,version,18,"Added the initial version of `neighbors_update`, only works with `use_rep='X' ` in `sc.pp.neighbors` for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:107,deployability,updat,updates,107,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is? Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:170,deployability,depend,depend,170,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is? Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:360,energy efficiency,estimat,estimate,360,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is? Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:170,integrability,depend,depend,170,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is? Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:87,interoperability,compatib,compatible,87,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is? Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:170,modifiability,depend,depend,170,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is? Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:107,safety,updat,updates,107,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is? Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:170,safety,depend,depend,170,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is? Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:107,security,updat,updates,107,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is? Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:129,testability,plan,plans,129,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is? Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:170,testability,depend,depend,170,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is? Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:521,availability,state,statement,521,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:655,availability,consist,consistent,655,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:521,integrability,state,statement,521,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:642,reliability,doe,does,642,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:542,safety,test,tests,542,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:553,safety,except,except,553,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:573,safety,test,tests,573,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:465,testability,simpl,simple,465,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:542,testability,test,tests,542,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:573,testability,test,tests,573,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:465,usability,simpl,simple,465,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:655,usability,consist,consistent,655,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:95,safety,test,tests,95,"@falexwolf . Yes, i inspected the notebook, everything looks the same. Also this PR passes all tests now. So, yes, i think it can be merged",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:95,testability,test,tests,95,"@falexwolf . Yes, i inspected the notebook, everything looks the same. Also this PR passes all tests now. So, yes, i think it can be merged",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:0,energy efficiency,Cool,Cool,0,"Cool! But, can you address the comment above? And, what about all these strange conflicts? There shouldn't be any in `scanpy/neighbors/` as all of these files get removed. Could you fix the conflict in `scanpy/tools/_umap.py`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:80,interoperability,conflict,conflicts,80,"Cool! But, can you address the comment above? And, what about all these strange conflicts? There shouldn't be any in `scanpy/neighbors/` as all of these files get removed. Could you fix the conflict in `scanpy/tools/_umap.py`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:190,interoperability,conflict,conflict,190,"Cool! But, can you address the comment above? And, what about all these strange conflicts? There shouldn't be any in `scanpy/neighbors/` as all of these files get removed. Could you fix the conflict in `scanpy/tools/_umap.py`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:210,usability,tool,tools,210,"Cool! But, can you address the comment above? And, what about all these strange conflicts? There shouldn't be any in `scanpy/neighbors/` as all of these files get removed. Could you fix the conflict in `scanpy/tools/_umap.py`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:94,interoperability,conflict,conflict,94,"@falexwolf , yes, i'll remove the commits with rp_forest and update_neighbors and resolve the conflict today. The conflicts are because of [this commit](https://github.com/theislab/scanpy/commit/0401341877e586e36412db1e178601bf1f545037), i believe.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:114,interoperability,conflict,conflicts,114,"@falexwolf , yes, i'll remove the commits with rp_forest and update_neighbors and resolve the conflict today. The conflicts are because of [this commit](https://github.com/theislab/scanpy/commit/0401341877e586e36412db1e178601bf1f545037), i believe.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:188,usability,user,user-images,188,"@falexwolf . It is ready for merge. However before i checked only paul15.ipynb, it is the same. But for paga-paul15 there are serious differences. The upper graph is new. ![image](https://user-images.githubusercontent.com/3065736/56921978-6b22f500-6ac7-11e9-8662-66db054f2c02.png). Denoising the graph. ![image](https://user-images.githubusercontent.com/3065736/56922041-9c9bc080-6ac7-11e9-965d-b1f6b4b00945.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:320,usability,user,user-images,320,"@falexwolf . It is ready for merge. However before i checked only paul15.ipynb, it is the same. But for paga-paul15 there are serious differences. The upper graph is new. ![image](https://user-images.githubusercontent.com/3065736/56921978-6b22f500-6ac7-11e9-8662-66db054f2c02.png). Denoising the graph. ![image](https://user-images.githubusercontent.com/3065736/56922041-9c9bc080-6ac7-11e9-965d-b1f6b4b00945.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:99,deployability,instal,installed,99,"I tested myself and obtained exactly the same results. :). You probably don't have the FA2 package installed, that's why your graph look different... :). I'm merging this! Awesome work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:91,modifiability,pac,package,91,"I tested myself and obtained exactly the same results. :). You probably don't have the FA2 package installed, that's why your graph look different... :). I'm merging this! Awesome work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:2,safety,test,tested,2,"I tested myself and obtained exactly the same results. :). You probably don't have the FA2 package installed, that's why your graph look different... :). I'm merging this! Awesome work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:2,testability,test,tested,2,"I tested myself and obtained exactly the same results. :). You probably don't have the FA2 package installed, that's why your graph look different... :). I'm merging this! Awesome work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:27,deployability,depend,depending,27,"@tomwhite, we're now fully depending on UMAP. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:27,integrability,depend,depending,27,"@tomwhite, we're now fully depending on UMAP. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:27,modifiability,depend,depending,27,"@tomwhite, we're now fully depending on UMAP. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:27,safety,depend,depending,27,"@tomwhite, we're now fully depending on UMAP. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:27,testability,depend,depending,27,"@tomwhite, we're now fully depending on UMAP. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/issues/577:20,integrability,sub,submitted,20,"whoops, now we both submitted a fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/580:135,deployability,fail,failing,135,I'm as puzzled as you are... We've been discussing this a bit here: #549 . I didn't really change anything of note when Travis started failing. And I have no idea why the test would result in a `1.0`. Do you know if there are any instructions on how to rebuild the test environment in conda? Is it just python 3.5 and scanpy from github?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:135,reliability,fail,failing,135,I'm as puzzled as you are... We've been discussing this a bit here: #549 . I didn't really change anything of note when Travis started failing. And I have no idea why the test would result in a `1.0`. Do you know if there are any instructions on how to rebuild the test environment in conda? Is it just python 3.5 and scanpy from github?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:171,safety,test,test,171,I'm as puzzled as you are... We've been discussing this a bit here: #549 . I didn't really change anything of note when Travis started failing. And I have no idea why the test would result in a `1.0`. Do you know if there are any instructions on how to rebuild the test environment in conda? Is it just python 3.5 and scanpy from github?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:265,safety,test,test,265,I'm as puzzled as you are... We've been discussing this a bit here: #549 . I didn't really change anything of note when Travis started failing. And I have no idea why the test would result in a `1.0`. Do you know if there are any instructions on how to rebuild the test environment in conda? Is it just python 3.5 and scanpy from github?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:171,testability,test,test,171,I'm as puzzled as you are... We've been discussing this a bit here: #549 . I didn't really change anything of note when Travis started failing. And I have no idea why the test would result in a `1.0`. Do you know if there are any instructions on how to rebuild the test environment in conda? Is it just python 3.5 and scanpy from github?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:265,testability,test,test,265,I'm as puzzled as you are... We've been discussing this a bit here: #549 . I didn't really change anything of note when Travis started failing. And I have no idea why the test would result in a `1.0`. Do you know if there are any instructions on how to rebuild the test environment in conda? Is it just python 3.5 and scanpy from github?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:250,deployability,continu,continuous-integration,250,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:414,deployability,fail,fails,414,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:433,deployability,build,build,433,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:494,deployability,build,builds,494,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:506,deployability,fail,fail,506,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:549,deployability,fail,failing,549,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:604,deployability,build,builds,604,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:261,integrability,integr,integration,261,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:288,integrability,coupl,couple,288,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:261,interoperability,integr,integration,261,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:261,modifiability,integr,integration,261,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:288,modifiability,coupl,couple,288,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:447,performance,cach,caches,447,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:261,reliability,integr,integration,261,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:414,reliability,fail,fails,414,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:506,reliability,fail,fail,506,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:549,reliability,fail,failing,549,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:245,safety,test,test-continuous-integration,245,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:261,security,integr,integration,261,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:245,testability,test,test-continuous-integration,245,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:288,testability,coupl,couple,288,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:154,usability,Guid,Guide,154,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python). * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:97,availability,error,error,97,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:113,availability,state,statements,113,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:190,availability,state,statements,190,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:205,availability,error,error,205,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:382,deployability,build,builds,382,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:113,integrability,state,statements,113,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:190,integrability,state,statements,190,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:97,performance,error,error,97,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:205,performance,error,error,205,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:338,performance,cach,cache,338,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:97,safety,error,error,97,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:131,safety,test,tests,131,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:205,safety,error,error,205,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:131,testability,test,tests,131,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:97,usability,error,error,97,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:205,usability,error,error,205,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:140,deployability,build,builds,140,"Yeah, I've definitely spent more time than I care to admit debugging travis by remote. Good luck! Do you mind if I restart some of your old builds? I want to try and diff the outputs to see if the setup is different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:33,performance,time,time,33,"Yeah, I've definitely spent more time than I care to admit debugging travis by remote. Good luck! Do you mind if I restart some of your old builds? I want to try and diff the outputs to see if the setup is different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:58,deployability,build,building,58,"I'm not entirely sure what you mean by restarting the old building (restarting travis from old commits?), but go ahead... anything that might help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:142,usability,help,help,142,"I'm not entirely sure what you mean by restarting the old building (restarting travis from old commits?), but go ahead... anything that might help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:4,deployability,build,builds,4,For builds on your fork there should be a button that lets you make travis just run a build again. Here's an example:. ![image](https://user-images.githubusercontent.com/8238804/55601220-9148c400-57ab-11e9-94b3-8ec016e7b801.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:86,deployability,build,build,86,For builds on your fork there should be a button that lets you make travis just run a build again. Here's an example:. ![image](https://user-images.githubusercontent.com/8238804/55601220-9148c400-57ab-11e9-94b3-8ec016e7b801.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:136,usability,user,user-images,136,For builds on your fork there should be a button that lets you make travis just run a build again. Here's an example:. ![image](https://user-images.githubusercontent.com/8238804/55601220-9148c400-57ab-11e9-94b3-8ec016e7b801.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:117,reliability,pra,practice,117,I see... thanks for the heads up. At the moment I'm working on branches of the main repo... but it's probably better practice to develop on my own fork.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/581:92,integrability,filter,filtering,92,Hm. Strange. This shouldn't have been touched. Let me look into this. Presumably due to non-filtering...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:141,availability,error,errors,141,"Probably is, I think the `nan`s are for all zero genes. But I think there's a better solution than a procedural warning and data that causes errors downstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:148,availability,down,downstream,148,"Probably is, I think the `nan`s are for all zero genes. But I think there's a better solution than a procedural warning and data that causes errors downstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:141,performance,error,errors,141,"Probably is, I think the `nan`s are for all zero genes. But I think there's a better solution than a procedural warning and data that causes errors downstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:141,safety,error,errors,141,"Probably is, I think the `nan`s are for all zero genes. But I think there's a better solution than a procedural warning and data that causes errors downstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:141,usability,error,errors,141,"Probably is, I think the `nan`s are for all zero genes. But I think there's a better solution than a procedural warning and data that causes errors downstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:84,availability,down,downstream,84,"The nans (with a proper warning) would be the right way but having data that causes downstream is not an option. As an intermediate solution, I added a note to the docs and made them zeros:. https://github.com/theislab/scanpy/commit/dce2be194a6ff865ecaeb939f3c990f6c3b0e244.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:119,modifiability,interm,intermediate,119,"The nans (with a proper warning) would be the right way but having data that causes downstream is not an option. As an intermediate solution, I added a note to the docs and made them zeros:. https://github.com/theislab/scanpy/commit/dce2be194a6ff865ecaeb939f3c990f6c3b0e244.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:214,safety,test,tests,214,"Quick question to you @ivirshup, can't we simply replace all the `adata_neighbors` stuff with `scanpy.datasets.pbmc68k_reduced`? It already has the neighbor graph etc. in it and is smaller, that is, would speed up tests considerably.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:42,testability,simpl,simply,42,"Quick question to you @ivirshup, can't we simply replace all the `adata_neighbors` stuff with `scanpy.datasets.pbmc68k_reduced`? It already has the neighbor graph etc. in it and is smaller, that is, would speed up tests considerably.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:214,testability,test,tests,214,"Quick question to you @ivirshup, can't we simply replace all the `adata_neighbors` stuff with `scanpy.datasets.pbmc68k_reduced`? It already has the neighbor graph etc. in it and is smaller, that is, would speed up tests considerably.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:42,usability,simpl,simply,42,"Quick question to you @ivirshup, can't we simply replace all the `adata_neighbors` stuff with `scanpy.datasets.pbmc68k_reduced`? It already has the neighbor graph etc. in it and is smaller, that is, would speed up tests considerably.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:29,availability,error,error,29,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:62,availability,operat,operation,62,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:346,availability,failur,failure,346,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:346,deployability,fail,failure,346,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:29,performance,error,error,29,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:346,performance,failur,failure,346,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:706,performance,parallel,parallel,706,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:72,reliability,doe,doesn,72,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:223,reliability,doe,does,223,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:346,reliability,fail,failure,346,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:29,safety,error,error,29,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:270,safety,reme,remember,270,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:514,safety,test,tests,514,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:676,safety,test,tests,676,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:514,testability,test,tests,514,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:676,testability,test,tests,676,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:29,usability,error,error,29,"Yeah, I was thinking even an error. Something that says ""this operation doesn't really make sense with genes with no counts, so we're doing {}"". On the other hand, I figure you can't go that wrong just doing what `sklearn` does, which is zeroes. For sure! I'm trying to remember why I went with pbmc3k in the first place. I think I was getting a failure for pbmc3k but not the smaller one? In any case, this should be covered by `test_pbmc3k.py` notebook now. Two quick related asides:. * It would be good to have tests that actually hit the parts of `neighbors` where non-pairwise distances are found (>4096 cells I think). . * I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:483,deployability,modul,module,483,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:240,integrability,wrap,wrapper,240,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:523,integrability,interfac,interface,523,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:240,interoperability,wrapper,wrapper,240,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:523,interoperability,interfac,interface,523,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:483,modifiability,modul,module,483,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:523,modifiability,interfac,interface,523,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:613,performance,parallel,parallel,613,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:39,safety,test,tests,39,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:86,safety,test,tests,86,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:206,safety,compl,completely,206,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:277,safety,test,tested,277,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:464,safety,test,test,464,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:483,safety,modul,module,483,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:511,safety,test,testing,511,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:583,safety,test,tests,583,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:206,security,compl,completely,206,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:39,testability,test,tests,39,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:86,testability,test,tests,86,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:277,testability,test,tested,277,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:464,testability,test,test,464,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:511,testability,test,testing,511,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:583,testability,test,tests,583,"Great! I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now? No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/582:84,safety,test,tested,84,"Hi! I just wrote a quick solution in https://github.com/theislab/scanpy/pull/586. I tested it manually and it seems to work (I used louvain code as template). I assumed it could be interesting to work on a separate leiden function, due to possible argument clashes with louvain, instead of merging the two functions together.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/issues/582:84,testability,test,tested,84,"Hi! I just wrote a quick solution in https://github.com/theislab/scanpy/pull/586. I tested it manually and it seems to work (I used louvain code as template). I assumed it could be interesting to work on a separate leiden function, due to possible argument clashes with louvain, instead of merging the two functions together.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/pull/583:59,availability,state,statements,59,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:374,availability,state,statements,374,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:419,availability,state,statements,419,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:733,availability,state,statements,733,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:225,deployability,fail,fail,225,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:347,deployability,fail,fail,347,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:483,deployability,fail,failing,483,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:508,deployability,continu,continue,508,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:795,deployability,fail,fail,795,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:59,integrability,state,statements,59,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:374,integrability,state,statements,374,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:419,integrability,state,statements,419,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:733,integrability,state,statements,733,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:557,performance,time,time,557,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:225,reliability,fail,fail,225,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:347,reliability,fail,fail,347,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:483,reliability,fail,failing,483,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:795,reliability,fail,fail,795,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:79,safety,test,tests,79,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:219,safety,test,tests,219,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:275,safety,test,test,275,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:297,safety,test,testing,297,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:341,safety,test,tests,341,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:751,safety,test,tests,751,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:789,safety,test,tests,789,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:79,testability,test,tests,79,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:219,testability,test,tests,219,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:275,testability,test,test,275,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:297,testability,test,testing,297,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:341,testability,test,tests,341,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:751,testability,test,tests,751,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:789,testability,test,tests,789,"Okay... I have no idea what else I can do... . I put print statements into the tests and saw that the assignment of names of the `adata.uns['rank_genes_groups']['names']` recarrays is not in the expected order when the tests fail. That's why I put in explicit names into the test data and started testing by these names. For some reason the tests fail when I take the print statements out, but they pass when the print statements are in there... so I can't even look at why they are failing anymore... I may continue to play with this when I have some more time tonight, but I need to focus on some other things atm. If you have any ideas @flying-sheep @ivirshup @falexwolf, I'd be super keen to hear them. Should I just leave print statements in the tests so that it's super verbose when tests fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:31,deployability,instal,installed,31,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:306,deployability,instal,installed,306,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:117,modifiability,maintain,maintain,117,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:104,reliability,doe,doesn,104,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:117,safety,maintain,maintain,117,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:385,safety,test,tests,385,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:317,testability,Simpl,Simply,317,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:385,testability,test,tests,385,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:457,testability,simpl,simply,457,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:112,usability,help,help,112,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:317,usability,Simpl,Simply,317,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:457,usability,simpl,simply,457,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:482,availability,state,statements,482,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:22,deployability,instal,installed,22,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:145,deployability,version,versions,145,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:165,deployability,depend,dependencies,165,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:145,integrability,version,versions,145,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:165,integrability,depend,dependencies,165,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:482,integrability,state,statements,482,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:145,modifiability,version,versions,145,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:165,modifiability,depend,dependencies,165,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:165,safety,depend,dependencies,165,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:68,testability,plan,plan,68,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:80,testability,emul,emulate,80,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:165,testability,depend,dependencies,165,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:74,interoperability,specif,specified,74,"@gokceneraslan that might have been an issue before... however I have now specified the tests via column and row names, and generated named recarrays, so I'm still looking now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:88,safety,test,tests,88,"@gokceneraslan that might have been an issue before... however I have now specified the tests via column and row names, and generated named recarrays, so I'm still looking now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:88,testability,test,tests,88,"@gokceneraslan that might have been an issue before... however I have now specified the tests via column and row names, and generated named recarrays, so I'm still looking now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/issues/584:169,integrability,sub,subset,169,"If you want to get the number of cells, try something like:. ```python. adata.obs.groupby([""louvain"", ""donor_sex""]).apply(len). ```. If you want the expression for that subset:. ```python. cell_ids = (. adata.obs. .loc[lambda x: x[""louvain""] == ""9""]. .loc[lambda x: x[""donor_sex""] == ""female""]. ).index. adata[cell_ids, :]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:53,usability,help,helpful,53,For just two covariates I find `pd.crosstab()` quite helpful as well.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:74,availability,error,error,74,"@LuckyMD is this the correct way of using pd.crosstab() ? I am getting an error as seen below:. adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-95-d09a5110597d> in <module>(). ----> 1 adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). AttributeError: 'AnnData' object has no attribute 'crosstab'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:352,deployability,modul,module,352,"@LuckyMD is this the correct way of using pd.crosstab() ? I am getting an error as seen below:. adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-95-d09a5110597d> in <module>(). ----> 1 adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). AttributeError: 'AnnData' object has no attribute 'crosstab'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:352,modifiability,modul,module,352,"@LuckyMD is this the correct way of using pd.crosstab() ? I am getting an error as seen below:. adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-95-d09a5110597d> in <module>(). ----> 1 adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). AttributeError: 'AnnData' object has no attribute 'crosstab'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:74,performance,error,error,74,"@LuckyMD is this the correct way of using pd.crosstab() ? I am getting an error as seen below:. adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-95-d09a5110597d> in <module>(). ----> 1 adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). AttributeError: 'AnnData' object has no attribute 'crosstab'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:74,safety,error,error,74,"@LuckyMD is this the correct way of using pd.crosstab() ? I am getting an error as seen below:. adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-95-d09a5110597d> in <module>(). ----> 1 adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). AttributeError: 'AnnData' object has no attribute 'crosstab'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:325,safety,input,input-,325,"@LuckyMD is this the correct way of using pd.crosstab() ? I am getting an error as seen below:. adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-95-d09a5110597d> in <module>(). ----> 1 adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). AttributeError: 'AnnData' object has no attribute 'crosstab'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:352,safety,modul,module,352,"@LuckyMD is this the correct way of using pd.crosstab() ? I am getting an error as seen below:. adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-95-d09a5110597d> in <module>(). ----> 1 adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). AttributeError: 'AnnData' object has no attribute 'crosstab'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:281,testability,Trace,Traceback,281,"@LuckyMD is this the correct way of using pd.crosstab() ? I am getting an error as seen below:. adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-95-d09a5110597d> in <module>(). ----> 1 adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). AttributeError: 'AnnData' object has no attribute 'crosstab'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:74,usability,error,error,74,"@LuckyMD is this the correct way of using pd.crosstab() ? I am getting an error as seen below:. adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-95-d09a5110597d> in <module>(). ----> 1 adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). AttributeError: 'AnnData' object has no attribute 'crosstab'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:325,usability,input,input-,325,"@LuckyMD is this the correct way of using pd.crosstab() ? I am getting an error as seen below:. adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-95-d09a5110597d> in <module>(). ----> 1 adata_fibro.crosstab(""patient_id"",""louvain"", rownames=['louvain'], colnames=['patient_id']). AttributeError: 'AnnData' object has no attribute 'crosstab'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:96,availability,cluster,cluster,96,@ivirshup as you suggested I tried using this function for getting counts of # of cells in each cluster for each patient. But in my case I have 17 patients and 8 clusters so I am losing some information in the middle and at the end of the list. I thought it was a caveat of printing to the console in my iPython notebook so I tried to save the output and then write it to a file but I am still missing some values. Do you know if there is some kind of limit on this type of calculation ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:162,availability,cluster,clusters,162,@ivirshup as you suggested I tried using this function for getting counts of # of cells in each cluster for each patient. But in my case I have 17 patients and 8 clusters so I am losing some information in the middle and at the end of the list. I thought it was a caveat of printing to the console in my iPython notebook so I tried to save the output and then write it to a file but I am still missing some values. Do you know if there is some kind of limit on this type of calculation ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:96,deployability,cluster,cluster,96,@ivirshup as you suggested I tried using this function for getting counts of # of cells in each cluster for each patient. But in my case I have 17 patients and 8 clusters so I am losing some information in the middle and at the end of the list. I thought it was a caveat of printing to the console in my iPython notebook so I tried to save the output and then write it to a file but I am still missing some values. Do you know if there is some kind of limit on this type of calculation ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:162,deployability,cluster,clusters,162,@ivirshup as you suggested I tried using this function for getting counts of # of cells in each cluster for each patient. But in my case I have 17 patients and 8 clusters so I am losing some information in the middle and at the end of the list. I thought it was a caveat of printing to the console in my iPython notebook so I tried to save the output and then write it to a file but I am still missing some values. Do you know if there is some kind of limit on this type of calculation ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:104,usability,document,documentation,104,"`pd.crosstab` is a function from `pandas`, and would only work on a pandas dataframe. You can find some documentation on that [here](http://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html#cross-tabulations). I'm not sure why you would be losing any info. I'd also note this seems less related to scanpy, and more related to pandas, since `adata.obs` is just a pandas dataframe. Have you tried looking through the [pandas user guide](https://pandas.pydata.org/pandas-docs/stable/) to figure out how to do what you want?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:436,usability,user,user,436,"`pd.crosstab` is a function from `pandas`, and would only work on a pandas dataframe. You can find some documentation on that [here](http://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html#cross-tabulations). I'm not sure why you would be losing any info. I'd also note this seems less related to scanpy, and more related to pandas, since `adata.obs` is just a pandas dataframe. Have you tried looking through the [pandas user guide](https://pandas.pydata.org/pandas-docs/stable/) to figure out how to do what you want?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:441,usability,guid,guide,441,"`pd.crosstab` is a function from `pandas`, and would only work on a pandas dataframe. You can find some documentation on that [here](http://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html#cross-tabulations). I'm not sure why you would be losing any info. I'd also note this seems less related to scanpy, and more related to pandas, since `adata.obs` is just a pandas dataframe. Have you tried looking through the [pandas user guide](https://pandas.pydata.org/pandas-docs/stable/) to figure out how to do what you want?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:26,safety,input,input,26,"@ivirshup Thanks for your input and suggestion. I was looking more into Scanpy documentation rather than pandas but I will check that out in the future. @LuckyMD thanks for the example, it works for me now and using pd.crosstab() is very handy so thanks a lot for the suggestion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:26,usability,input,input,26,"@ivirshup Thanks for your input and suggestion. I was looking more into Scanpy documentation rather than pandas but I will check that out in the future. @LuckyMD thanks for the example, it works for me now and using pd.crosstab() is very handy so thanks a lot for the suggestion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:79,usability,document,documentation,79,"@ivirshup Thanks for your input and suggestion. I was looking more into Scanpy documentation rather than pandas but I will check that out in the future. @LuckyMD thanks for the example, it works for me now and using pd.crosstab() is very handy so thanks a lot for the suggestion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/pull/585:136,availability,error,error,136,"This can’t be it, `__init__.py` is [optional since Python 3.3](https://www.python.org/dev/peps/pep-0420/). I also don’t see that import error, `import scanpy` works perfectly. Can you specify how you got that? ```py. >>> import scanpy, anndata. >>> scanpy.external.tl.palantir(anndata.AnnData()). ImportError: . please install palantir: . git clone git://github.com/dpeerlab/Palantir.git. cd Palantir. sudo -H pip3 install . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:319,deployability,instal,install,319,"This can’t be it, `__init__.py` is [optional since Python 3.3](https://www.python.org/dev/peps/pep-0420/). I also don’t see that import error, `import scanpy` works perfectly. Can you specify how you got that? ```py. >>> import scanpy, anndata. >>> scanpy.external.tl.palantir(anndata.AnnData()). ImportError: . please install palantir: . git clone git://github.com/dpeerlab/Palantir.git. cd Palantir. sudo -H pip3 install . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:415,deployability,instal,install,415,"This can’t be it, `__init__.py` is [optional since Python 3.3](https://www.python.org/dev/peps/pep-0420/). I also don’t see that import error, `import scanpy` works perfectly. Can you specify how you got that? ```py. >>> import scanpy, anndata. >>> scanpy.external.tl.palantir(anndata.AnnData()). ImportError: . please install palantir: . git clone git://github.com/dpeerlab/Palantir.git. cd Palantir. sudo -H pip3 install . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:184,interoperability,specif,specify,184,"This can’t be it, `__init__.py` is [optional since Python 3.3](https://www.python.org/dev/peps/pep-0420/). I also don’t see that import error, `import scanpy` works perfectly. Can you specify how you got that? ```py. >>> import scanpy, anndata. >>> scanpy.external.tl.palantir(anndata.AnnData()). ImportError: . please install palantir: . git clone git://github.com/dpeerlab/Palantir.git. cd Palantir. sudo -H pip3 install . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:136,performance,error,error,136,"This can’t be it, `__init__.py` is [optional since Python 3.3](https://www.python.org/dev/peps/pep-0420/). I also don’t see that import error, `import scanpy` works perfectly. Can you specify how you got that? ```py. >>> import scanpy, anndata. >>> scanpy.external.tl.palantir(anndata.AnnData()). ImportError: . please install palantir: . git clone git://github.com/dpeerlab/Palantir.git. cd Palantir. sudo -H pip3 install . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:136,safety,error,error,136,"This can’t be it, `__init__.py` is [optional since Python 3.3](https://www.python.org/dev/peps/pep-0420/). I also don’t see that import error, `import scanpy` works perfectly. Can you specify how you got that? ```py. >>> import scanpy, anndata. >>> scanpy.external.tl.palantir(anndata.AnnData()). ImportError: . please install palantir: . git clone git://github.com/dpeerlab/Palantir.git. cd Palantir. sudo -H pip3 install . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:136,usability,error,error,136,"This can’t be it, `__init__.py` is [optional since Python 3.3](https://www.python.org/dev/peps/pep-0420/). I also don’t see that import error, `import scanpy` works perfectly. Can you specify how you got that? ```py. >>> import scanpy, anndata. >>> scanpy.external.tl.palantir(anndata.AnnData()). ImportError: . please install palantir: . git clone git://github.com/dpeerlab/Palantir.git. cd Palantir. sudo -H pip3 install . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:57,deployability,updat,update,57,I had this problem importing `import scanpy as sc`. I'll update you if this problem persists.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:57,safety,updat,update,57,I had this problem importing `import scanpy as sc`. I'll update you if this problem persists.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:57,security,updat,update,57,I had this problem importing `import scanpy as sc`. I'll update you if this problem persists.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:35,availability,error,error,35,"@fbrundu I just had the exact same error after installing from cloned master, so just wondering if it's working for you now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:47,deployability,instal,installing,47,"@fbrundu I just had the exact same error after installing from cloned master, so just wondering if it's working for you now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:35,performance,error,error,35,"@fbrundu I just had the exact same error after installing from cloned master, so just wondering if it's working for you now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:35,safety,error,error,35,"@fbrundu I just had the exact same error after installing from cloned master, so just wondering if it's working for you now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:35,usability,error,error,35,"@fbrundu I just had the exact same error after installing from cloned master, so just wondering if it's working for you now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:23,availability,error,error,23,"@jarny I have the same error still, but when testing on travis it doesn't fail so I have no clue. Locally I create the `__init__.py` file though to make it work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:74,deployability,fail,fail,74,"@jarny I have the same error still, but when testing on travis it doesn't fail so I have no clue. Locally I create the `__init__.py` file though to make it work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:23,performance,error,error,23,"@jarny I have the same error still, but when testing on travis it doesn't fail so I have no clue. Locally I create the `__init__.py` file though to make it work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:66,reliability,doe,doesn,66,"@jarny I have the same error still, but when testing on travis it doesn't fail so I have no clue. Locally I create the `__init__.py` file though to make it work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:74,reliability,fail,fail,74,"@jarny I have the same error still, but when testing on travis it doesn't fail so I have no clue. Locally I create the `__init__.py` file though to make it work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:23,safety,error,error,23,"@jarny I have the same error still, but when testing on travis it doesn't fail so I have no clue. Locally I create the `__init__.py` file though to make it work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:45,safety,test,testing,45,"@jarny I have the same error still, but when testing on travis it doesn't fail so I have no clue. Locally I create the `__init__.py` file though to make it work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:45,testability,test,testing,45,"@jarny I have the same error still, but when testing on travis it doesn't fail so I have no clue. Locally I create the `__init__.py` file though to make it work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:23,usability,error,error,23,"@jarny I have the same error still, but when testing on travis it doesn't fail so I have no clue. Locally I create the `__init__.py` file though to make it work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/586:24,safety,test,test,24,Would you mind adding a test?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:24,testability,test,test,24,Would you mind adding a test?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:38,integrability,abstract,abstracting,38,Great! Please create helper functions abstracting away identical code blocks in the two functions. E.g. those lines are completely identical to the ones in `louvain`:. https://github.com/theislab/scanpy/blob/4760cbdf264c88ab48e17efaab5b559ab064be49/scanpy/tools/_leiden.py#L108-L120. And the same might apply to the other code block.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:38,modifiability,abstract,abstracting,38,Great! Please create helper functions abstracting away identical code blocks in the two functions. E.g. those lines are completely identical to the ones in `louvain`:. https://github.com/theislab/scanpy/blob/4760cbdf264c88ab48e17efaab5b559ab064be49/scanpy/tools/_leiden.py#L108-L120. And the same might apply to the other code block.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:120,safety,compl,completely,120,Great! Please create helper functions abstracting away identical code blocks in the two functions. E.g. those lines are completely identical to the ones in `louvain`:. https://github.com/theislab/scanpy/blob/4760cbdf264c88ab48e17efaab5b559ab064be49/scanpy/tools/_leiden.py#L108-L120. And the same might apply to the other code block.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:55,security,ident,identical,55,Great! Please create helper functions abstracting away identical code blocks in the two functions. E.g. those lines are completely identical to the ones in `louvain`:. https://github.com/theislab/scanpy/blob/4760cbdf264c88ab48e17efaab5b559ab064be49/scanpy/tools/_leiden.py#L108-L120. And the same might apply to the other code block.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:120,security,compl,completely,120,Great! Please create helper functions abstracting away identical code blocks in the two functions. E.g. those lines are completely identical to the ones in `louvain`:. https://github.com/theislab/scanpy/blob/4760cbdf264c88ab48e17efaab5b559ab064be49/scanpy/tools/_leiden.py#L108-L120. And the same might apply to the other code block.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:131,security,ident,identical,131,Great! Please create helper functions abstracting away identical code blocks in the two functions. E.g. those lines are completely identical to the ones in `louvain`:. https://github.com/theislab/scanpy/blob/4760cbdf264c88ab48e17efaab5b559ab064be49/scanpy/tools/_leiden.py#L108-L120. And the same might apply to the other code block.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:21,usability,help,helper,21,Great! Please create helper functions abstracting away identical code blocks in the two functions. E.g. those lines are completely identical to the ones in `louvain`:. https://github.com/theislab/scanpy/blob/4760cbdf264c88ab48e17efaab5b559ab064be49/scanpy/tools/_leiden.py#L108-L120. And the same might apply to the other code block.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:256,usability,tool,tools,256,Great! Please create helper functions abstracting away identical code blocks in the two functions. E.g. those lines are completely identical to the ones in `louvain`:. https://github.com/theislab/scanpy/blob/4760cbdf264c88ab48e17efaab5b559ab064be49/scanpy/tools/_leiden.py#L108-L120. And the same might apply to the other code block.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:216,deployability,fail,fails,216,"I added helper functions. I am working on the tests. Apparently there's a test https://github.com/theislab/scanpy/blob/fc24dfc62c049a0d0c9cc491d4647d03b52bfb10/scanpy/tests/test_rank_genes_groups_logreg.py#L22. that fails locally in my machine. It is because after `rank_genes_groups` categories are naturally sorted. I don't think this is due to my changes, but let me know how can I help. I am unsure why it is failing on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:413,deployability,fail,failing,413,"I added helper functions. I am working on the tests. Apparently there's a test https://github.com/theislab/scanpy/blob/fc24dfc62c049a0d0c9cc491d4647d03b52bfb10/scanpy/tests/test_rank_genes_groups_logreg.py#L22. that fails locally in my machine. It is because after `rank_genes_groups` categories are naturally sorted. I don't think this is due to my changes, but let me know how can I help. I am unsure why it is failing on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:216,reliability,fail,fails,216,"I added helper functions. I am working on the tests. Apparently there's a test https://github.com/theislab/scanpy/blob/fc24dfc62c049a0d0c9cc491d4647d03b52bfb10/scanpy/tests/test_rank_genes_groups_logreg.py#L22. that fails locally in my machine. It is because after `rank_genes_groups` categories are naturally sorted. I don't think this is due to my changes, but let me know how can I help. I am unsure why it is failing on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:413,reliability,fail,failing,413,"I added helper functions. I am working on the tests. Apparently there's a test https://github.com/theislab/scanpy/blob/fc24dfc62c049a0d0c9cc491d4647d03b52bfb10/scanpy/tests/test_rank_genes_groups_logreg.py#L22. that fails locally in my machine. It is because after `rank_genes_groups` categories are naturally sorted. I don't think this is due to my changes, but let me know how can I help. I am unsure why it is failing on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:46,safety,test,tests,46,"I added helper functions. I am working on the tests. Apparently there's a test https://github.com/theislab/scanpy/blob/fc24dfc62c049a0d0c9cc491d4647d03b52bfb10/scanpy/tests/test_rank_genes_groups_logreg.py#L22. that fails locally in my machine. It is because after `rank_genes_groups` categories are naturally sorted. I don't think this is due to my changes, but let me know how can I help. I am unsure why it is failing on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:74,safety,test,test,74,"I added helper functions. I am working on the tests. Apparently there's a test https://github.com/theislab/scanpy/blob/fc24dfc62c049a0d0c9cc491d4647d03b52bfb10/scanpy/tests/test_rank_genes_groups_logreg.py#L22. that fails locally in my machine. It is because after `rank_genes_groups` categories are naturally sorted. I don't think this is due to my changes, but let me know how can I help. I am unsure why it is failing on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:167,safety,test,tests,167,"I added helper functions. I am working on the tests. Apparently there's a test https://github.com/theislab/scanpy/blob/fc24dfc62c049a0d0c9cc491d4647d03b52bfb10/scanpy/tests/test_rank_genes_groups_logreg.py#L22. that fails locally in my machine. It is because after `rank_genes_groups` categories are naturally sorted. I don't think this is due to my changes, but let me know how can I help. I am unsure why it is failing on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:46,testability,test,tests,46,"I added helper functions. I am working on the tests. Apparently there's a test https://github.com/theislab/scanpy/blob/fc24dfc62c049a0d0c9cc491d4647d03b52bfb10/scanpy/tests/test_rank_genes_groups_logreg.py#L22. that fails locally in my machine. It is because after `rank_genes_groups` categories are naturally sorted. I don't think this is due to my changes, but let me know how can I help. I am unsure why it is failing on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:74,testability,test,test,74,"I added helper functions. I am working on the tests. Apparently there's a test https://github.com/theislab/scanpy/blob/fc24dfc62c049a0d0c9cc491d4647d03b52bfb10/scanpy/tests/test_rank_genes_groups_logreg.py#L22. that fails locally in my machine. It is because after `rank_genes_groups` categories are naturally sorted. I don't think this is due to my changes, but let me know how can I help. I am unsure why it is failing on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:167,testability,test,tests,167,"I added helper functions. I am working on the tests. Apparently there's a test https://github.com/theislab/scanpy/blob/fc24dfc62c049a0d0c9cc491d4647d03b52bfb10/scanpy/tests/test_rank_genes_groups_logreg.py#L22. that fails locally in my machine. It is because after `rank_genes_groups` categories are naturally sorted. I don't think this is due to my changes, but let me know how can I help. I am unsure why it is failing on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:8,usability,help,helper,8,"I added helper functions. I am working on the tests. Apparently there's a test https://github.com/theislab/scanpy/blob/fc24dfc62c049a0d0c9cc491d4647d03b52bfb10/scanpy/tests/test_rank_genes_groups_logreg.py#L22. that fails locally in my machine. It is because after `rank_genes_groups` categories are naturally sorted. I don't think this is due to my changes, but let me know how can I help. I am unsure why it is failing on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:385,usability,help,help,385,"I added helper functions. I am working on the tests. Apparently there's a test https://github.com/theislab/scanpy/blob/fc24dfc62c049a0d0c9cc491d4647d03b52bfb10/scanpy/tests/test_rank_genes_groups_logreg.py#L22. that fails locally in my machine. It is because after `rank_genes_groups` categories are naturally sorted. I don't think this is due to my changes, but let me know how can I help. I am unsure why it is failing on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:56,modifiability,paramet,parameter,56,I added tests for both louvain and leiden with restrict parameter. Please review the test code to be sure it is clear and working.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:8,safety,test,tests,8,I added tests for both louvain and leiden with restrict parameter. Please review the test code to be sure it is clear and working.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:74,safety,review,review,74,I added tests for both louvain and leiden with restrict parameter. Please review the test code to be sure it is clear and working.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:85,safety,test,test,85,I added tests for both louvain and leiden with restrict parameter. Please review the test code to be sure it is clear and working.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:8,testability,test,tests,8,I added tests for both louvain and leiden with restrict parameter. Please review the test code to be sure it is clear and working.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:74,testability,review,review,74,I added tests for both louvain and leiden with restrict parameter. Please review the test code to be sure it is clear and working.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:85,testability,test,test,85,I added tests for both louvain and leiden with restrict parameter. Please review the test code to be sure it is clear and working.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:112,usability,clear,clear,112,I added tests for both louvain and leiden with restrict parameter. Please review the test code to be sure it is clear and working.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:180,usability,tool,tools,180,"Only one thing that I would ask for: please don't put things in utils when it's not absolutely necessary. Here, it would be very natural to add a `_utils_clustering.py` in `scanpy/tools.py`. `scanpy/utils.py` is way too overloaded with all kinds of things... And I will need to clean it up at some point. Thank you for moving the helper functions there. Or, foreseeing that `leiden` might surpass `louvain` in the future, you could just add the helper functions to `_leiden` and import into `_louvain` from there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:330,usability,help,helper,330,"Only one thing that I would ask for: please don't put things in utils when it's not absolutely necessary. Here, it would be very natural to add a `_utils_clustering.py` in `scanpy/tools.py`. `scanpy/utils.py` is way too overloaded with all kinds of things... And I will need to clean it up at some point. Thank you for moving the helper functions there. Or, foreseeing that `leiden` might surpass `louvain` in the future, you could just add the helper functions to `_leiden` and import into `_louvain` from there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:445,usability,help,helper,445,"Only one thing that I would ask for: please don't put things in utils when it's not absolutely necessary. Here, it would be very natural to add a `_utils_clustering.py` in `scanpy/tools.py`. `scanpy/utils.py` is way too overloaded with all kinds of things... And I will need to clean it up at some point. Thank you for moving the helper functions there. Or, foreseeing that `leiden` might surpass `louvain` in the future, you could just add the helper functions to `_leiden` and import into `_louvain` from there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:124,availability,cluster,clustering,124,"@falexwolf I added `_utils_clustering.py` since I think it's the more maintainable way to do it (e.g. if in the future, new clustering methods are added).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:124,deployability,cluster,clustering,124,"@falexwolf I added `_utils_clustering.py` since I think it's the more maintainable way to do it (e.g. if in the future, new clustering methods are added).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:70,modifiability,maintain,maintainable,70,"@falexwolf I added `_utils_clustering.py` since I think it's the more maintainable way to do it (e.g. if in the future, new clustering methods are added).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:70,safety,maintain,maintainable,70,"@falexwolf I added `_utils_clustering.py` since I think it's the more maintainable way to do it (e.g. if in the future, new clustering methods are added).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:42,availability,redund,redundant,42,"I've got one minor comment left (one last redundant print statement), but otherwise I'm good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:58,availability,state,statement,58,"I've got one minor comment left (one last redundant print statement), but otherwise I'm good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:42,deployability,redundan,redundant,42,"I've got one minor comment left (one last redundant print statement), but otherwise I'm good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:58,integrability,state,statement,58,"I've got one minor comment left (one last redundant print statement), but otherwise I'm good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:42,reliability,redundan,redundant,42,"I've got one minor comment left (one last redundant print statement), but otherwise I'm good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:42,safety,redund,redundant,42,"I've got one minor comment left (one last redundant print statement), but otherwise I'm good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:10,deployability,updat,update,10,we should update the tutorials and notebooks to use ` leiden` instead of `louvain`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:10,safety,updat,update,10,we should update the tutorials and notebooks to use ` leiden` instead of `louvain`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:10,security,updat,update,10,we should update the tutorials and notebooks to use ` leiden` instead of `louvain`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:54,deployability,instal,installation,54,"Yes, we should as soon as many people report seemless installation of the leiden package. I'm still using louvain as I never had any problems with it, but I agree that we should migrate when leiden is stable, mature and easily-installable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:227,deployability,instal,installable,227,"Yes, we should as soon as many people report seemless installation of the leiden package. I'm still using louvain as I never had any problems with it, but I agree that we should migrate when leiden is stable, mature and easily-installable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:81,modifiability,pac,package,81,"Yes, we should as soon as many people report seemless installation of the leiden package. I'm still using louvain as I never had any problems with it, but I agree that we should migrate when leiden is stable, mature and easily-installable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:101,availability,slo,slower,101,"Has anyone compared the two for speed? I guess Leiden has an extra step and therefore might be a bit slower. If they both scale well to >1 million cells, then I agree... otherwise the issues of louvain shouldn't be as bad for KNN-graphs as for more graphs with more heavy tailed degree distributions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:122,deployability,scale,scale,122,"Has anyone compared the two for speed? I guess Leiden has an extra step and therefore might be a bit slower. If they both scale well to >1 million cells, then I agree... otherwise the issues of louvain shouldn't be as bad for KNN-graphs as for more graphs with more heavy tailed degree distributions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:122,energy efficiency,scale,scale,122,"Has anyone compared the two for speed? I guess Leiden has an extra step and therefore might be a bit slower. If they both scale well to >1 million cells, then I agree... otherwise the issues of louvain shouldn't be as bad for KNN-graphs as for more graphs with more heavy tailed degree distributions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:286,interoperability,distribut,distributions,286,"Has anyone compared the two for speed? I guess Leiden has an extra step and therefore might be a bit slower. If they both scale well to >1 million cells, then I agree... otherwise the issues of louvain shouldn't be as bad for KNN-graphs as for more graphs with more heavy tailed degree distributions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:122,modifiability,scal,scale,122,"Has anyone compared the two for speed? I guess Leiden has an extra step and therefore might be a bit slower. If they both scale well to >1 million cells, then I agree... otherwise the issues of louvain shouldn't be as bad for KNN-graphs as for more graphs with more heavy tailed degree distributions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:122,performance,scale,scale,122,"Has anyone compared the two for speed? I guess Leiden has an extra step and therefore might be a bit slower. If they both scale well to >1 million cells, then I agree... otherwise the issues of louvain shouldn't be as bad for KNN-graphs as for more graphs with more heavy tailed degree distributions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:101,reliability,slo,slower,101,"Has anyone compared the two for speed? I guess Leiden has an extra step and therefore might be a bit slower. If they both scale well to >1 million cells, then I agree... otherwise the issues of louvain shouldn't be as bad for KNN-graphs as for more graphs with more heavy tailed degree distributions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:221,deployability,depend,depending,221,"I looked at it a while ago (for one test dataset, probably), and got the impression that `louvain` was faster. That said, they're both very fast. I would note that solutions from either can be pretty unstable, frequently depending on size of the community. @LuckyMD When you say heavy tailed, are you thinking of the unweighted KNN graph case or both?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:221,integrability,depend,depending,221,"I looked at it a while ago (for one test dataset, probably), and got the impression that `louvain` was faster. That said, they're both very fast. I would note that solutions from either can be pretty unstable, frequently depending on size of the community. @LuckyMD When you say heavy tailed, are you thinking of the unweighted KNN graph case or both?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:221,modifiability,depend,depending,221,"I looked at it a while ago (for one test dataset, probably), and got the impression that `louvain` was faster. That said, they're both very fast. I would note that solutions from either can be pretty unstable, frequently depending on size of the community. @LuckyMD When you say heavy tailed, are you thinking of the unweighted KNN graph case or both?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:36,safety,test,test,36,"I looked at it a while ago (for one test dataset, probably), and got the impression that `louvain` was faster. That said, they're both very fast. I would note that solutions from either can be pretty unstable, frequently depending on size of the community. @LuckyMD When you say heavy tailed, are you thinking of the unweighted KNN graph case or both?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:221,safety,depend,depending,221,"I looked at it a while ago (for one test dataset, probably), and got the impression that `louvain` was faster. That said, they're both very fast. I would note that solutions from either can be pretty unstable, frequently depending on size of the community. @LuckyMD When you say heavy tailed, are you thinking of the unweighted KNN graph case or both?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:36,testability,test,test,36,"I looked at it a while ago (for one test dataset, probably), and got the impression that `louvain` was faster. That said, they're both very fast. I would note that solutions from either can be pretty unstable, frequently depending on size of the community. @LuckyMD When you say heavy tailed, are you thinking of the unweighted KNN graph case or both?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:221,testability,depend,depending,221,"I looked at it a while ago (for one test dataset, probably), and got the impression that `louvain` was faster. That said, they're both very fast. I would note that solutions from either can be pretty unstable, frequently depending on size of the community. @LuckyMD When you say heavy tailed, are you thinking of the unweighted KNN graph case or both?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:169,deployability,Modul,Modularity,169,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:189,deployability,configurat,configuration,189,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:208,energy efficiency,model,model,208,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:227,energy efficiency,optim,optimally,227,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:169,integrability,Modular,Modularity,169,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:189,integrability,configur,configuration,189,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:169,modifiability,Modul,Modularity,169,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:189,modifiability,configur,configuration,189,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:72,performance,content,content,72,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:313,performance,network,network,313,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:342,performance,network,networks,342,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:169,safety,Modul,Modularity,169,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:237,safety,detect,detects,237,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:189,security,configur,configuration,189,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:208,security,model,model,208,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:237,security,detect,detects,237,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:313,security,network,network,313,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:342,security,network,networks,342,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:169,testability,Modula,Modularity,169,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:44,availability,cluster,clustering,44,I wasn't aware we use a weighted matrix for clustering as default. Are the weights in the `connectivities` matrix related to the euclidean distances between cells?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:44,deployability,cluster,clustering,44,I wasn't aware we use a weighted matrix for clustering as default. Are the weights in the `connectivities` matrix related to the euclidean distances between cells?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:177,deployability,scale,scaled,177,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:177,energy efficiency,scale,scaled,177,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:415,energy efficiency,adapt,adapting-to-real-world-data,415,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:415,integrability,adapt,adapting-to-real-world-data,415,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:415,interoperability,adapt,adapting-to-real-world-data,415,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:177,modifiability,scal,scaled,177,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:415,modifiability,adapt,adapting-to-real-world-data,415,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:177,performance,scale,scaled,177,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:364,usability,learn,learn,364,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:50,testability,understand,understand,50,Thanks for the link. This is definitely easier to understand than the paper ;).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:64,availability,cluster,clustering,64,If we use the `connectivities` weights from UMAP by default for clustering... how different is that to clustering on the UMAP embedding directly? Is there a rationale for not just using a binarized KNN-graph for this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:103,availability,cluster,clustering,103,If we use the `connectivities` weights from UMAP by default for clustering... how different is that to clustering on the UMAP embedding directly? Is there a rationale for not just using a binarized KNN-graph for this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:64,deployability,cluster,clustering,64,If we use the `connectivities` weights from UMAP by default for clustering... how different is that to clustering on the UMAP embedding directly? Is there a rationale for not just using a binarized KNN-graph for this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:103,deployability,cluster,clustering,103,If we use the `connectivities` weights from UMAP by default for clustering... how different is that to clustering on the UMAP embedding directly? Is there a rationale for not just using a binarized KNN-graph for this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:429,availability,cluster,clustering,429,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:569,availability,cluster,clustering,569,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:738,availability,error,errors,738,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:429,deployability,cluster,clustering,429,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:569,deployability,cluster,clustering,569,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:866,energy efficiency,optim,optimization,866,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:593,interoperability,prox,proxies,593,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:738,performance,error,errors,738,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:866,performance,optimiz,optimization,866,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:738,safety,error,errors,738,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:126,usability,tool,tools,126,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:291,usability,tool,tools,291,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:738,usability,error,errors,738,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:776,usability,statu,status,776,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)? Why did you decide to change the default in Leiden. (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD. > how different is that to clustering on the UMAP embedding directly? It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:158,availability,cluster,clustering,158,"@falexwolf I recall that image in the figure, thanks! I wasn't aware that the embedding was the difficult part. I would also like to know what the benefit of clustering the weighted graph is in practice.... It definitely makes the clustering procedure more difficult to explain in one sentence in the methods section of a paper...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:231,availability,cluster,clustering,231,"@falexwolf I recall that image in the figure, thanks! I wasn't aware that the embedding was the difficult part. I would also like to know what the benefit of clustering the weighted graph is in practice.... It definitely makes the clustering procedure more difficult to explain in one sentence in the methods section of a paper...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:158,deployability,cluster,clustering,158,"@falexwolf I recall that image in the figure, thanks! I wasn't aware that the embedding was the difficult part. I would also like to know what the benefit of clustering the weighted graph is in practice.... It definitely makes the clustering procedure more difficult to explain in one sentence in the methods section of a paper...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:231,deployability,cluster,clustering,231,"@falexwolf I recall that image in the figure, thanks! I wasn't aware that the embedding was the difficult part. I would also like to know what the benefit of clustering the weighted graph is in practice.... It definitely makes the clustering procedure more difficult to explain in one sentence in the methods section of a paper...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:194,reliability,pra,practice,194,"@falexwolf I recall that image in the figure, thanks! I wasn't aware that the embedding was the difficult part. I would also like to know what the benefit of clustering the weighted graph is in practice.... It definitely makes the clustering procedure more difficult to explain in one sentence in the methods section of a paper...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:211,availability,cluster,clustering,211,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:745,availability,sli,slightly,745,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1311,availability,cluster,clustering,1311,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1343,availability,robust,robust,1343,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1532,availability,cluster,clusters,1532,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1594,availability,cluster,clustering,1594,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:211,deployability,cluster,clustering,211,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:883,deployability,log,log-ish,883,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1013,deployability,log,log,1013,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1017,deployability,scale,scaled,1017,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1311,deployability,cluster,clustering,1311,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1416,deployability,depend,dependent,1416,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1497,deployability,depend,dependence,1497,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1532,deployability,cluster,clusters,1532,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1594,deployability,cluster,clustering,1594,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1017,energy efficiency,scale,scaled,1017,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:798,integrability,coupl,couple,798,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:864,integrability,sub,sub-linear,864,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1416,integrability,depend,dependent,1416,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1497,integrability,depend,dependence,1497,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:798,modifiability,coupl,couple,798,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1017,modifiability,scal,scaled,1017,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1416,modifiability,depend,dependent,1416,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1497,modifiability,depend,dependence,1497,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1622,modifiability,paramet,parameter,1622,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:599,performance,time,times,599,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1017,performance,scale,scaled,1017,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:745,reliability,sli,slightly,745,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1343,reliability,robust,robust,1343,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:816,safety,test,tested,816,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:883,safety,log,log-ish,883,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1013,safety,log,log,1013,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1343,safety,robust,robust,1343,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1416,safety,depend,dependent,1416,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1497,safety,depend,dependence,1497,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:696,security,sign,signal,696,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:883,security,log,log-ish,883,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1013,security,log,log,1013,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:798,testability,coupl,couple,798,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:816,testability,test,tested,816,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:883,testability,log,log-ish,883,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1013,testability,log,log,1013,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1416,testability,depend,dependent,1416,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1497,testability,depend,dependence,1497,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:299,usability,minim,minimum,299,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:484,usability,person,personally,484,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:690,usability,clear,clear,690,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1100,usability,user,user-images,1100,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1300,usability,experien,experience,1300,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361? I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:298,availability,cluster,clustering,298,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:496,availability,cluster,clustering,496,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:761,availability,cluster,clustering,761,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:854,availability,cluster,clusters,854,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:894,availability,robust,robust,894,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:298,deployability,cluster,clustering,298,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:496,deployability,cluster,clustering,496,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:761,deployability,cluster,clustering,761,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:820,deployability,observ,observation,820,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:854,deployability,cluster,clusters,854,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:141,performance,network,network,141,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:372,reliability,doe,does,372,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:420,reliability,doe,does,420,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:725,reliability,Doe,Does,725,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:894,reliability,robust,robust,894,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:894,safety,robust,robust,894,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:141,security,network,network,141,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:820,testability,observ,observation,820,"I'm not sure I agree with your interpretation of your total degree plot. To me, increasing `k` is meant to have the effect of densifying the network, and thus obtaining a lower resolution view of the manifold. It is somewhat analogous to choosing a lower resolution value for `leiden` or `louvain` clustering. What you see is that in the weighted case, the overall degree does not really increase (thus possibly neither does the overall density), so that increasing `k` may have little effect on clustering at all. This is the most I can get from this plot... as density is really about local changes and not the global degree increase. But I would still ask whether it is a good thing that increasing `k` has little effect? Does increasing `k` then change the clustering results (in the weighed case?). I wonder if the observation that you find smaller clusters better in the weighted case is robust. That would suggest that weights can counteract resolution limit issues, which would be very interesting...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:426,availability,cluster,clusters,426,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:668,availability,cluster,clustering,668,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:760,availability,cluster,cluster,760,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1206,availability,cluster,cluster,1206,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1565,availability,cluster,clusters,1565,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1603,availability,cluster,cluster,1603,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1694,availability,cluster,clustering,1694,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:426,deployability,cluster,clusters,426,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:545,deployability,build,build,545,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:668,deployability,cluster,clustering,668,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:760,deployability,cluster,cluster,760,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:873,deployability,log,logscale,873,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1206,deployability,cluster,cluster,1206,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1227,deployability,scale,scales,1227,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1565,deployability,cluster,clusters,1565,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1603,deployability,cluster,cluster,1603,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1694,deployability,cluster,clustering,1694,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1227,energy efficiency,scale,scales,1227,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:247,modifiability,paramet,parameters,247,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1227,modifiability,scal,scales,1227,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:555,performance,network,networks,555,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:597,performance,time,times,597,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:652,performance,network,networks,652,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1227,performance,scale,scales,1227,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:873,safety,log,logscale,873,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1664,safety,detect,detected,1664,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:555,security,network,networks,555,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:652,security,network,networks,652,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:873,security,log,logscale,873,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1547,security,ident,identifying,1547,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1664,security,detect,detected,1664,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:297,testability,simpl,simplicity,297,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:873,testability,log,logscale,873,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:297,usability,simpl,simplicity,297,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:959,usability,user,user-images,959,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1070,usability,user,user-images,1070,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1198,usability,minim,minimum,1198,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1288,usability,user,user-images,1288,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1399,usability,user,user-images,1399,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png). ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png). ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:116,availability,cluster,clusters,116,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:298,availability,cluster,cluster,298,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:509,availability,cluster,clustering,509,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:733,availability,cluster,clustering,733,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:116,deployability,cluster,clusters,116,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:170,deployability,scale,scale,170,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:191,deployability,version,version,191,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:298,deployability,cluster,cluster,298,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:509,deployability,cluster,clustering,509,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:720,deployability,scale,scale,720,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:733,deployability,cluster,clustering,733,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:170,energy efficiency,scale,scale,170,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:720,energy efficiency,scale,scale,720,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:191,integrability,version,version,191,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:170,modifiability,scal,scale,170,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:191,modifiability,version,version,191,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:695,modifiability,paramet,parameter,695,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:720,modifiability,scal,scale,720,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:170,performance,scale,scale,170,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:720,performance,scale,scale,720,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:275,safety,detect,detected,275,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:275,security,detect,detected,275,"This looks a lot more convincing... It's a bit hard to read the second last plot though... The black parts are also clusters around size 2-10, no? Or am I misreading the scale? Do you have a version with a few more annotations on the colour bar? How often are megakaryocytes detected as a separate cluster in the unweighted case? It looks like unweighted case is definitely worse for higher resolutions with >10 neighbours though. And coming back to the `k` discussion.. From my perspective, If you treat the clustering and knn graph generation as two separate steps, you may want `k` to have an effect. If you treat it as the same process, then I follow your argumentation that having a single parameter to affects the scale of the clustering suffices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:238,availability,cluster,cluster,238,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:309,availability,cluster,cluster,309,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:376,availability,cluster,cluster,376,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:532,availability,cluster,clustering,532,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:568,availability,robust,robust,568,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:631,availability,cluster,clustering,631,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:682,availability,cluster,clusters,682,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:831,availability,consist,consistent,831,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:105,deployability,version,versions,105,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:135,deployability,log,log,135,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:139,deployability,scale,scale,139,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:238,deployability,cluster,cluster,238,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:309,deployability,cluster,cluster,309,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:376,deployability,cluster,cluster,376,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:532,deployability,cluster,clustering,532,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:631,deployability,cluster,clustering,631,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:682,deployability,cluster,clusters,682,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:139,energy efficiency,scale,scale,139,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:105,integrability,version,versions,105,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:105,modifiability,version,versions,105,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:139,modifiability,scal,scale,139,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:729,modifiability,paramet,parameters,729,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:770,modifiability,paramet,parameters,770,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:139,performance,scale,scale,139,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:390,performance,time,time,390,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:446,performance,time,times,446,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:568,reliability,robust,robust,568,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:135,safety,log,log,135,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:353,safety,detect,detected,353,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:568,safety,robust,robust,568,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:135,security,log,log,135,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:353,security,detect,detected,353,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:603,security,sign,signal,603,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:669,security,sign,signal,669,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:135,testability,log,log,135,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:93,usability,interact,interactive,93,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:230,usability,minim,minimum,230,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:301,usability,minim,minimum,301,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:831,usability,consist,consistent,831,"I took about 20 minutes on it, but couldn't figure out how to add more annotations. I've got interactive versions with hover over, but log scale is bugged in those libraries... I believe the bins that are the darkest shade in the minimum cluster size for the unweighted graph actually correspond to a minimum cluster size of 1 cell. Megakaryocytes were detected as a distinct cluster every time that k was 10 in the unweighted case, but no other times. I think that when we make a call on ""this is a kind of cell"" from unsupervised clustering, those results should be robust. That is, if there's strong signal in the data and your clustering algorithm can pick up that signal, good clusters shouldn't change much if you vary the parameters a little. If you can pick any parameters from a wide range and get results that are pretty consistent, that seems like good data and a good method to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:37,availability,cluster,clusters,37,"I follow your argumentation on ""good clusters"". However, I also like the concept that putting k=35 means you make it harder to detect clusters of size < 35, as you 'over-connect' those clusters in a way. The weighted case is less interpretable in that way. However, here it clearly outperforms the unweighted case. I am still a little on the fence (due to interpretability), but I'd be okay with weighting I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:134,availability,cluster,clusters,134,"I follow your argumentation on ""good clusters"". However, I also like the concept that putting k=35 means you make it harder to detect clusters of size < 35, as you 'over-connect' those clusters in a way. The weighted case is less interpretable in that way. However, here it clearly outperforms the unweighted case. I am still a little on the fence (due to interpretability), but I'd be okay with weighting I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:185,availability,cluster,clusters,185,"I follow your argumentation on ""good clusters"". However, I also like the concept that putting k=35 means you make it harder to detect clusters of size < 35, as you 'over-connect' those clusters in a way. The weighted case is less interpretable in that way. However, here it clearly outperforms the unweighted case. I am still a little on the fence (due to interpretability), but I'd be okay with weighting I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:37,deployability,cluster,clusters,37,"I follow your argumentation on ""good clusters"". However, I also like the concept that putting k=35 means you make it harder to detect clusters of size < 35, as you 'over-connect' those clusters in a way. The weighted case is less interpretable in that way. However, here it clearly outperforms the unweighted case. I am still a little on the fence (due to interpretability), but I'd be okay with weighting I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:134,deployability,cluster,clusters,134,"I follow your argumentation on ""good clusters"". However, I also like the concept that putting k=35 means you make it harder to detect clusters of size < 35, as you 'over-connect' those clusters in a way. The weighted case is less interpretable in that way. However, here it clearly outperforms the unweighted case. I am still a little on the fence (due to interpretability), but I'd be okay with weighting I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:185,deployability,cluster,clusters,185,"I follow your argumentation on ""good clusters"". However, I also like the concept that putting k=35 means you make it harder to detect clusters of size < 35, as you 'over-connect' those clusters in a way. The weighted case is less interpretable in that way. However, here it clearly outperforms the unweighted case. I am still a little on the fence (due to interpretability), but I'd be okay with weighting I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:127,safety,detect,detect,127,"I follow your argumentation on ""good clusters"". However, I also like the concept that putting k=35 means you make it harder to detect clusters of size < 35, as you 'over-connect' those clusters in a way. The weighted case is less interpretable in that way. However, here it clearly outperforms the unweighted case. I am still a little on the fence (due to interpretability), but I'd be okay with weighting I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:127,security,detect,detect,127,"I follow your argumentation on ""good clusters"". However, I also like the concept that putting k=35 means you make it harder to detect clusters of size < 35, as you 'over-connect' those clusters in a way. The weighted case is less interpretable in that way. However, here it clearly outperforms the unweighted case. I am still a little on the fence (due to interpretability), but I'd be okay with weighting I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:274,usability,clear,clearly,274,"I follow your argumentation on ""good clusters"". However, I also like the concept that putting k=35 means you make it harder to detect clusters of size < 35, as you 'over-connect' those clusters in a way. The weighted case is less interpretable in that way. However, here it clearly outperforms the unweighted case. I am still a little on the fence (due to interpretability), but I'd be okay with weighting I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/issues/587:3,availability,operat,operating,3,My operating system is windows 10. Appreciate all the help!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:54,usability,help,help,54,My operating system is windows 10. Appreciate all the help!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:23,deployability,version,version,23,Could you provide some version information on your conda environment? Pretty much the output of `conda list`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:23,integrability,version,version,23,Could you provide some version information on your conda environment? Pretty much the output of `conda list`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:23,modifiability,version,version,23,Could you provide some version information on your conda environment? Pretty much the output of `conda list`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:62,availability,error,error,62,"But also, that looks like an h5py issue. Do you still get the error if you try `import h5py` in that environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:62,performance,error,error,62,"But also, that looks like an h5py issue. Do you still get the error if you try `import h5py` in that environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:62,safety,error,error,62,"But also, that looks like an h5py issue. Do you still get the error if you try `import h5py` in that environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:62,usability,error,error,62,"But also, that looks like an h5py issue. Do you still get the error if you try `import h5py` in that environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:258,availability,error,error,258,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:27,deployability,Instal,Installing,27,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:763,deployability,modul,module,763,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1086,deployability,Continu,Continuum,1086,"n. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1379,deployability,Continu,Continuum,1379,"_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1549,deployability,version,versions,1549,"or faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1803,deployability,Continu,Continuum,1803,"03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2223,deployability,Continu,Continuum,2223,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2519,deployability,log,logg,2519,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2769,deployability,contain,contains,2769,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:247,integrability,sub,subsequent,247,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:563,integrability,sub,subsequent,563,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1042,integrability,sub,subsequent,1042,"ting h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1549,integrability,version,versions,1549,"or faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2496,interoperability,format,format,2496,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:3073,interoperability,format,formats,3073,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:480,modifiability,variab,variable,480,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:496,modifiability,variab,variables-axis,496,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:763,modifiability,modul,module,763,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:951,modifiability,variab,variable,951,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:967,modifiability,variab,variables-axis,967,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1115,modifiability,pac,packages,1115," at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1408,modifiability,pac,packages,1408,"e `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 els",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1549,modifiability,version,versions,1549,"or faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1832,modifiability,pac,packages,1832,"feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually chang",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2252,modifiability,pac,packages,2252,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:258,performance,error,error,258,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:519,performance,cach,cache,519,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:541,performance,cach,cache,541,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:998,performance,cach,cache,998,"i! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1020,performance,cach,cache,1020,"er. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1190,performance,cach,cache,1190,"your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 fileke",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1309,performance,cach,cache,1309,"Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, shee",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1315,performance,cach,cache,1315,"\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1487,performance,cach,cache,1487,"ames (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1626,performance,cach,cache,1626,"-------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1632,performance,cach,cache,1632,"-------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not fin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1942,performance,cach,cache,1942,"r the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2119,performance,cach,cache,2119,"s\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2125,performance,cach,cache,2125,"py\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a man",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2363,performance,cach,cache,2363,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2566,performance,cach,cache,2566,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:3016,reliability,pra,practice,3016,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:258,safety,error,error,258,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:736,safety,input,input-,736,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:763,safety,modul,module,763,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2519,safety,log,logg,2519,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2519,security,log,logg,2519,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:692,testability,Trace,Traceback,692,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2519,testability,log,logg,2519,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:57,usability,help,helped,57,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:258,usability,error,error,258,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:312,usability,User,Users,312,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:328,usability,Document,Documents,328,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:736,usability,input,input-,736,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:781,usability,User,Users,781,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:797,usability,Document,Documents,797,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file. . Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py. adata = sc.read_10x_mtx(. 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). cache=True) # write a cache file for faster subsequent reading. ```. ```pytb. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). <ipython-input-17-e7dd3543f8df> in <module>(). 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. 5 . 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2645,usability,User,Users,2645,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2659,usability,Document,Documents,2659,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:3137,usability,document,documentation,3137,"cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 447 else:. 448 if not is_present:. --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)). 450 logg.msg('reading', filename, v=4). 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz. ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats??? Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:72,availability,avail,available,72,"Here's a link to the docs [https://scanpy.readthedocs.io/](), it's also available at the top of the github repo. EDIT: Whoops, posted some wrong info about how 10x pre v3 datasets are read before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:72,reliability,availab,available,72,"Here's a link to the docs [https://scanpy.readthedocs.io/](), it's also available at the top of the github repo. EDIT: Whoops, posted some wrong info about how 10x pre v3 datasets are read before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:72,safety,avail,available,72,"Here's a link to the docs [https://scanpy.readthedocs.io/](), it's also available at the top of the github repo. EDIT: Whoops, posted some wrong info about how 10x pre v3 datasets are read before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:72,security,availab,available,72,"Here's a link to the docs [https://scanpy.readthedocs.io/](), it's also available at the top of the github repo. EDIT: Whoops, posted some wrong info about how 10x pre v3 datasets are read before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:38,availability,error,error,38,I am still getting the file not found error because when downloading it's lacking the `gz` extension. Will manually change but sharing so you are aware! @ivirshup,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:57,availability,down,downloading,57,I am still getting the file not found error because when downloading it's lacking the `gz` extension. Will manually change but sharing so you are aware! @ivirshup,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:91,modifiability,extens,extension,91,I am still getting the file not found error because when downloading it's lacking the `gz` extension. Will manually change but sharing so you are aware! @ivirshup,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:38,performance,error,error,38,I am still getting the file not found error because when downloading it's lacking the `gz` extension. Will manually change but sharing so you are aware! @ivirshup,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:38,safety,error,error,38,I am still getting the file not found error because when downloading it's lacking the `gz` extension. Will manually change but sharing so you are aware! @ivirshup,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:38,usability,error,error,38,I am still getting the file not found error because when downloading it's lacking the `gz` extension. Will manually change but sharing so you are aware! @ivirshup,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/588:278,usability,tool,tools,278,"Hm, they're mentioned in the note at the very top, and there are cross-references within the page. They have a separate page just as plotting. I'm happy to give them a separate heading, just like plotting has. I wouldn't want them to be mixed in. I expect that list of external tools to potentially become quite long at some point. I'd also prefer them to stay on a separate page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:341,usability,prefer,prefer,341,"Hm, they're mentioned in the note at the very top, and there are cross-references within the page. They have a separate page just as plotting. I'm happy to give them a separate heading, just like plotting has. I wouldn't want them to be mixed in. I expect that list of external tools to potentially become quite long at some point. I'd also prefer them to stay on a separate page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:109,availability,avail,available,109,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:200,availability,avail,available,200,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:249,availability,avail,available,249,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:418,deployability,integr,integrate,418,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:720,deployability,API,API,720,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:749,deployability,API,API,749,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:263,integrability,batch,batch,263,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:418,integrability,integr,integrate,418,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:720,integrability,API,API,720,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:749,integrability,API,API,749,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:418,interoperability,integr,integrate,418,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:720,interoperability,API,API,720,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:749,interoperability,API,API,749,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:418,modifiability,integr,integrate,418,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:263,performance,batch,batch,263,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:806,performance,content,contents,806,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:109,reliability,availab,available,109,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:200,reliability,availab,available,200,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:249,reliability,availab,available,249,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:418,reliability,integr,integrate,418,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:109,safety,avail,available,109,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:200,safety,avail,available,200,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:249,safety,avail,available,249,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:109,security,availab,available,109,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:200,security,availab,available,200,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:249,security,availab,available,249,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:418,security,integr,integrate,418,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:418,testability,integr,integrate,418,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:99,usability,tool,tools,99,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:288,usability,user,user,288,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:538,usability,user,users,538,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:832,usability,navigat,navigate,832,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not. 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:36,deployability,modul,module,36,I'm not against giving the external module higher visibility. I'm absolutely for it. I just don't want to mix all functions together. You mean: adding `External API` in the top-level navigation bar? That sounds very right!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:161,deployability,API,API,161,I'm not against giving the external module higher visibility. I'm absolutely for it. I just don't want to mix all functions together. You mean: adding `External API` in the top-level navigation bar? That sounds very right!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:161,integrability,API,API,161,I'm not against giving the external module higher visibility. I'm absolutely for it. I just don't want to mix all functions together. You mean: adding `External API` in the top-level navigation bar? That sounds very right!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:161,interoperability,API,API,161,I'm not against giving the external module higher visibility. I'm absolutely for it. I just don't want to mix all functions together. You mean: adding `External API` in the top-level navigation bar? That sounds very right!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:36,modifiability,modul,module,36,I'm not against giving the external module higher visibility. I'm absolutely for it. I just don't want to mix all functions together. You mean: adding `External API` in the top-level navigation bar? That sounds very right!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:36,safety,modul,module,36,I'm not against giving the external module higher visibility. I'm absolutely for it. I just don't want to mix all functions together. You mean: adding `External API` in the top-level navigation bar? That sounds very right!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:183,usability,navigat,navigation,183,I'm not against giving the external module higher visibility. I'm absolutely for it. I just don't want to mix all functions together. You mean: adding `External API` in the top-level navigation bar? That sounds very right!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:83,deployability,API,API,83,"Great! Yeah, and having it's table of contents following the general layout from. `API`. On Thu, Apr 4, 2019 at 3:02 PM Alex Wolf <notifications@github.com> wrote:. > I'm not against giving the external module higher visibility. I'm. > absolutely for it. I just don't want to mix all functions together. >. > You mean: adding External API in the top-level navigation bar? That. > sounds very right! >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/588#issuecomment-479741789>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AH221AXVtyyTekvt2ecg7pCxk7oKovyOks5vdXlYgaJpZM4cb1Lm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:203,deployability,modul,module,203,"Great! Yeah, and having it's table of contents following the general layout from. `API`. On Thu, Apr 4, 2019 at 3:02 PM Alex Wolf <notifications@github.com> wrote:. > I'm not against giving the external module higher visibility. I'm. > absolutely for it. I just don't want to mix all functions together. >. > You mean: adding External API in the top-level navigation bar? That. > sounds very right! >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/588#issuecomment-479741789>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AH221AXVtyyTekvt2ecg7pCxk7oKovyOks5vdXlYgaJpZM4cb1Lm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:335,deployability,API,API,335,"Great! Yeah, and having it's table of contents following the general layout from. `API`. On Thu, Apr 4, 2019 at 3:02 PM Alex Wolf <notifications@github.com> wrote:. > I'm not against giving the external module higher visibility. I'm. > absolutely for it. I just don't want to mix all functions together. >. > You mean: adding External API in the top-level navigation bar? That. > sounds very right! >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/588#issuecomment-479741789>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AH221AXVtyyTekvt2ecg7pCxk7oKovyOks5vdXlYgaJpZM4cb1Lm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:83,integrability,API,API,83,"Great! Yeah, and having it's table of contents following the general layout from. `API`. On Thu, Apr 4, 2019 at 3:02 PM Alex Wolf <notifications@github.com> wrote:. > I'm not against giving the external module higher visibility. I'm. > absolutely for it. I just don't want to mix all functions together. >. > You mean: adding External API in the top-level navigation bar? That. > sounds very right! >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/588#issuecomment-479741789>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AH221AXVtyyTekvt2ecg7pCxk7oKovyOks5vdXlYgaJpZM4cb1Lm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:335,integrability,API,API,335,"Great! Yeah, and having it's table of contents following the general layout from. `API`. On Thu, Apr 4, 2019 at 3:02 PM Alex Wolf <notifications@github.com> wrote:. > I'm not against giving the external module higher visibility. I'm. > absolutely for it. I just don't want to mix all functions together. >. > You mean: adding External API in the top-level navigation bar? That. > sounds very right! >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/588#issuecomment-479741789>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AH221AXVtyyTekvt2ecg7pCxk7oKovyOks5vdXlYgaJpZM4cb1Lm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:83,interoperability,API,API,83,"Great! Yeah, and having it's table of contents following the general layout from. `API`. On Thu, Apr 4, 2019 at 3:02 PM Alex Wolf <notifications@github.com> wrote:. > I'm not against giving the external module higher visibility. I'm. > absolutely for it. I just don't want to mix all functions together. >. > You mean: adding External API in the top-level navigation bar? That. > sounds very right! >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/588#issuecomment-479741789>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AH221AXVtyyTekvt2ecg7pCxk7oKovyOks5vdXlYgaJpZM4cb1Lm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:335,interoperability,API,API,335,"Great! Yeah, and having it's table of contents following the general layout from. `API`. On Thu, Apr 4, 2019 at 3:02 PM Alex Wolf <notifications@github.com> wrote:. > I'm not against giving the external module higher visibility. I'm. > absolutely for it. I just don't want to mix all functions together. >. > You mean: adding External API in the top-level navigation bar? That. > sounds very right! >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/588#issuecomment-479741789>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AH221AXVtyyTekvt2ecg7pCxk7oKovyOks5vdXlYgaJpZM4cb1Lm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:203,modifiability,modul,module,203,"Great! Yeah, and having it's table of contents following the general layout from. `API`. On Thu, Apr 4, 2019 at 3:02 PM Alex Wolf <notifications@github.com> wrote:. > I'm not against giving the external module higher visibility. I'm. > absolutely for it. I just don't want to mix all functions together. >. > You mean: adding External API in the top-level navigation bar? That. > sounds very right! >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/588#issuecomment-479741789>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AH221AXVtyyTekvt2ecg7pCxk7oKovyOks5vdXlYgaJpZM4cb1Lm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:38,performance,content,contents,38,"Great! Yeah, and having it's table of contents following the general layout from. `API`. On Thu, Apr 4, 2019 at 3:02 PM Alex Wolf <notifications@github.com> wrote:. > I'm not against giving the external module higher visibility. I'm. > absolutely for it. I just don't want to mix all functions together. >. > You mean: adding External API in the top-level navigation bar? That. > sounds very right! >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/588#issuecomment-479741789>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AH221AXVtyyTekvt2ecg7pCxk7oKovyOks5vdXlYgaJpZM4cb1Lm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:203,safety,modul,module,203,"Great! Yeah, and having it's table of contents following the general layout from. `API`. On Thu, Apr 4, 2019 at 3:02 PM Alex Wolf <notifications@github.com> wrote:. > I'm not against giving the external module higher visibility. I'm. > absolutely for it. I just don't want to mix all functions together. >. > You mean: adding External API in the top-level navigation bar? That. > sounds very right! >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/588#issuecomment-479741789>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AH221AXVtyyTekvt2ecg7pCxk7oKovyOks5vdXlYgaJpZM4cb1Lm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:444,security,auth,authored,444,"Great! Yeah, and having it's table of contents following the general layout from. `API`. On Thu, Apr 4, 2019 at 3:02 PM Alex Wolf <notifications@github.com> wrote:. > I'm not against giving the external module higher visibility. I'm. > absolutely for it. I just don't want to mix all functions together. >. > You mean: adding External API in the top-level navigation bar? That. > sounds very right! >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/588#issuecomment-479741789>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AH221AXVtyyTekvt2ecg7pCxk7oKovyOks5vdXlYgaJpZM4cb1Lm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:661,security,auth,auth,661,"Great! Yeah, and having it's table of contents following the general layout from. `API`. On Thu, Apr 4, 2019 at 3:02 PM Alex Wolf <notifications@github.com> wrote:. > I'm not against giving the external module higher visibility. I'm. > absolutely for it. I just don't want to mix all functions together. >. > You mean: adding External API in the top-level navigation bar? That. > sounds very right! >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/588#issuecomment-479741789>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AH221AXVtyyTekvt2ecg7pCxk7oKovyOks5vdXlYgaJpZM4cb1Lm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:356,usability,navigat,navigation,356,"Great! Yeah, and having it's table of contents following the general layout from. `API`. On Thu, Apr 4, 2019 at 3:02 PM Alex Wolf <notifications@github.com> wrote:. > I'm not against giving the external module higher visibility. I'm. > absolutely for it. I just don't want to mix all functions together. >. > You mean: adding External API in the top-level navigation bar? That. > sounds very right! >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/588#issuecomment-479741789>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AH221AXVtyyTekvt2ecg7pCxk7oKovyOks5vdXlYgaJpZM4cb1Lm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/590:25,usability,visual,visual,25,"Hmm, I think it’s a nice visual indicator that there’s just nothing at all. Don’t you agree?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:32,usability,indicat,indicator,32,"Hmm, I think it’s a nice visual indicator that there’s just nothing at all. Don’t you agree?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:138,reliability,doe,doesn,138,"I'm not sure really, it's also all white in other colormaps. The reason actually is a bug:. `mean_obs /= mean_obs.max(0).fillna(0)`. This doesn't fill NAs. It should be . `mean_obs = (mean_obs / mean_obs.max(0)).fillna(0)`. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:193,interoperability,share,shared,193,@gokceneraslan Can you add a PR with the fix? @flying-sheep Any good idea on how to set a color for NAs without having to add one more parameter to the visualizations (this problem is probably shared by matrixplot and dotplot),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:135,modifiability,paramet,parameter,135,@gokceneraslan Can you add a PR with the fix? @flying-sheep Any good idea on how to set a color for NAs without having to add one more parameter to the visualizations (this problem is probably shared by matrixplot and dotplot),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:152,usability,visual,visualizations,152,@gokceneraslan Can you add a PR with the fix? @flying-sheep Any good idea on how to set a color for NAs without having to add one more parameter to the visualizations (this problem is probably shared by matrixplot and dotplot),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:70,deployability,api,api,70,heatmap passes additional kwargs to [`imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html) which accepts `cmap`. You can pass a [ColorMap](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap) you’ve called [`set_bad`](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap.set_bad) on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:184,deployability,api,api,184,heatmap passes additional kwargs to [`imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html) which accepts `cmap`. You can pass a [ColorMap](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap) you’ve called [`set_bad`](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap.set_bad) on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:305,deployability,api,api,305,heatmap passes additional kwargs to [`imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html) which accepts `cmap`. You can pass a [ColorMap](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap) you’ve called [`set_bad`](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap.set_bad) on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:0,energy efficiency,heat,heatmap,0,heatmap passes additional kwargs to [`imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html) which accepts `cmap`. You can pass a [ColorMap](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap) you’ve called [`set_bad`](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap.set_bad) on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:70,integrability,api,api,70,heatmap passes additional kwargs to [`imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html) which accepts `cmap`. You can pass a [ColorMap](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap) you’ve called [`set_bad`](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap.set_bad) on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:184,integrability,api,api,184,heatmap passes additional kwargs to [`imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html) which accepts `cmap`. You can pass a [ColorMap](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap) you’ve called [`set_bad`](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap.set_bad) on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:305,integrability,api,api,305,heatmap passes additional kwargs to [`imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html) which accepts `cmap`. You can pass a [ColorMap](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap) you’ve called [`set_bad`](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap.set_bad) on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:70,interoperability,api,api,70,heatmap passes additional kwargs to [`imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html) which accepts `cmap`. You can pass a [ColorMap](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap) you’ve called [`set_bad`](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap.set_bad) on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:184,interoperability,api,api,184,heatmap passes additional kwargs to [`imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html) which accepts `cmap`. You can pass a [ColorMap](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap) you’ve called [`set_bad`](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap.set_bad) on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/issues/590:305,interoperability,api,api,305,heatmap passes additional kwargs to [`imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html) which accepts `cmap`. You can pass a [ColorMap](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap) you’ve called [`set_bad`](https://matplotlib.org/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap.set_bad) on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/pull/591:31,testability,simpl,simply,31,Of course that's fine. You can simply push such things to master. ;),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/591
https://github.com/scverse/scanpy/pull/591:31,usability,simpl,simply,31,Of course that's fine. You can simply push such things to master. ;),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/591
https://github.com/scverse/scanpy/issues/593:15,usability,help,helped,15,@fidelram this helped! I checked what are the marker_genes and went back and got it to work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/pull/594:101,safety,test,test,101,"You're right `fraction` is better! Can you change it directly on this branch? Also, I did not find a test for this. Can you add one? You can essentially copy your example and the test that's there for `normalize_per_cell`. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/594
https://github.com/scverse/scanpy/pull/594:179,safety,test,test,179,"You're right `fraction` is better! Can you change it directly on this branch? Also, I did not find a test for this. Can you add one? You can essentially copy your example and the test that's there for `normalize_per_cell`. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/594
https://github.com/scverse/scanpy/pull/594:101,testability,test,test,101,"You're right `fraction` is better! Can you change it directly on this branch? Also, I did not find a test for this. Can you add one? You can essentially copy your example and the test that's there for `normalize_per_cell`. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/594
https://github.com/scverse/scanpy/pull/594:179,testability,test,test,179,"You're right `fraction` is better! Can you change it directly on this branch? Also, I did not find a test for this. Can you add one? You can essentially copy your example and the test that's there for `normalize_per_cell`. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/594
https://github.com/scverse/scanpy/pull/594:45,safety,test,tests,45,@falexwolf . It is strange that there are no tests for this. I thought i added them... I will try to do all these thing in two days. Sorry for the delays.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/594
https://github.com/scverse/scanpy/pull/594:45,testability,test,tests,45,@falexwolf . It is strange that there are no tests for this. I thought i added them... I will try to do all these thing in two days. Sorry for the delays.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/594
https://github.com/scverse/scanpy/pull/594:125,safety,test,tests,125,"Ok, thank you! Maybe I just didn't find them. If so, please point me to them. Merging this in the meanwhile, you can add the tests in a new PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/594
https://github.com/scverse/scanpy/pull/594:125,testability,test,tests,125,"Ok, thank you! Maybe I just didn't find them. If so, please point me to them. Merging this in the meanwhile, you can add the tests in a new PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/594
https://github.com/scverse/scanpy/issues/595:239,availability,error,error,239,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:77,deployability,instal,installation,77,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:176,deployability,instal,installation,176,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:189,deployability,depend,dependency,189,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:189,integrability,depend,dependency,189,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:189,modifiability,depend,dependency,189,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:239,performance,error,error,239,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:189,safety,depend,dependency,189,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:239,safety,error,error,239,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:189,testability,depend,dependency,189,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:239,usability,error,error,239,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:259,usability,interact,interactive,259,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:99,interoperability,specif,specifically,99,"Yes, switching the backend should definitely work. Pay attention to the infos in @ivirshup’s link, specifically:. > If you use the use() function, this must be done before importing matplotlib.pyplot. Calling use() after pyplot has been imported will have no effect. which also applies to importing scanpy, seaborn, and so on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:24,usability,help,help,24,"@panxiaoguang, did this help solve your problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:26,usability,help,help,26,"> @panxiaoguang, did this help solve your problem? Thank you very much ,it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/596:364,availability,cluster,clusters,364,"I think we should do that in a way that doesn’t tempt the user to use literal colors like you just did. The color names people think of have a lot of bad properties for colorblind people and contrast. Maybe if we redefine common color names? When users specify “red” there, they only want some kind of red, not `#ff0000`. In one of my analyses, there’s predefined clusters which I recolor like this:. ![grafik](https://user-images.githubusercontent.com/291575/55707300-9001dc00-59e3-11e9-93b5-2dfac3814edf.png). It’s in R and what I do is that I give names to a lot of colorbrewer colors:. ```r. prettier_colors <- c(. setNames(brewer.pal(8, 'Dark2')[c(1,4,6,8)], c('turquoise', 'magenta', 'yellow', 'black')),. setNames(brewer.pal(9, 'Set1')[-c(4,6)], c('red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey')). ). ```. In python that would be:. ```py. import numpy as np. from matplotlib import cm. prettier_colors = dict(zip(. [. 'turquoise', 'magenta', 'yellow', 'black',. 'red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey',. ], np.concatenate([. np.array(cm.Dark2.colors)[[0,3,5,7]],. np.array(cm.Set1.colors)[np.setdiff1d(np.arange(9), [3, 5])],. ]). )). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:364,deployability,cluster,clusters,364,"I think we should do that in a way that doesn’t tempt the user to use literal colors like you just did. The color names people think of have a lot of bad properties for colorblind people and contrast. Maybe if we redefine common color names? When users specify “red” there, they only want some kind of red, not `#ff0000`. In one of my analyses, there’s predefined clusters which I recolor like this:. ![grafik](https://user-images.githubusercontent.com/291575/55707300-9001dc00-59e3-11e9-93b5-2dfac3814edf.png). It’s in R and what I do is that I give names to a lot of colorbrewer colors:. ```r. prettier_colors <- c(. setNames(brewer.pal(8, 'Dark2')[c(1,4,6,8)], c('turquoise', 'magenta', 'yellow', 'black')),. setNames(brewer.pal(9, 'Set1')[-c(4,6)], c('red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey')). ). ```. In python that would be:. ```py. import numpy as np. from matplotlib import cm. prettier_colors = dict(zip(. [. 'turquoise', 'magenta', 'yellow', 'black',. 'red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey',. ], np.concatenate([. np.array(cm.Dark2.colors)[[0,3,5,7]],. np.array(cm.Set1.colors)[np.setdiff1d(np.arange(9), [3, 5])],. ]). )). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:771,energy efficiency,green,green,771,"I think we should do that in a way that doesn’t tempt the user to use literal colors like you just did. The color names people think of have a lot of bad properties for colorblind people and contrast. Maybe if we redefine common color names? When users specify “red” there, they only want some kind of red, not `#ff0000`. In one of my analyses, there’s predefined clusters which I recolor like this:. ![grafik](https://user-images.githubusercontent.com/291575/55707300-9001dc00-59e3-11e9-93b5-2dfac3814edf.png). It’s in R and what I do is that I give names to a lot of colorbrewer colors:. ```r. prettier_colors <- c(. setNames(brewer.pal(8, 'Dark2')[c(1,4,6,8)], c('turquoise', 'magenta', 'yellow', 'black')),. setNames(brewer.pal(9, 'Set1')[-c(4,6)], c('red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey')). ). ```. In python that would be:. ```py. import numpy as np. from matplotlib import cm. prettier_colors = dict(zip(. [. 'turquoise', 'magenta', 'yellow', 'black',. 'red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey',. ], np.concatenate([. np.array(cm.Dark2.colors)[[0,3,5,7]],. np.array(cm.Set1.colors)[np.setdiff1d(np.arange(9), [3, 5])],. ]). )). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:1000,energy efficiency,green,green,1000,"I think we should do that in a way that doesn’t tempt the user to use literal colors like you just did. The color names people think of have a lot of bad properties for colorblind people and contrast. Maybe if we redefine common color names? When users specify “red” there, they only want some kind of red, not `#ff0000`. In one of my analyses, there’s predefined clusters which I recolor like this:. ![grafik](https://user-images.githubusercontent.com/291575/55707300-9001dc00-59e3-11e9-93b5-2dfac3814edf.png). It’s in R and what I do is that I give names to a lot of colorbrewer colors:. ```r. prettier_colors <- c(. setNames(brewer.pal(8, 'Dark2')[c(1,4,6,8)], c('turquoise', 'magenta', 'yellow', 'black')),. setNames(brewer.pal(9, 'Set1')[-c(4,6)], c('red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey')). ). ```. In python that would be:. ```py. import numpy as np. from matplotlib import cm. prettier_colors = dict(zip(. [. 'turquoise', 'magenta', 'yellow', 'black',. 'red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey',. ], np.concatenate([. np.array(cm.Dark2.colors)[[0,3,5,7]],. np.array(cm.Set1.colors)[np.setdiff1d(np.arange(9), [3, 5])],. ]). )). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:253,interoperability,specif,specify,253,"I think we should do that in a way that doesn’t tempt the user to use literal colors like you just did. The color names people think of have a lot of bad properties for colorblind people and contrast. Maybe if we redefine common color names? When users specify “red” there, they only want some kind of red, not `#ff0000`. In one of my analyses, there’s predefined clusters which I recolor like this:. ![grafik](https://user-images.githubusercontent.com/291575/55707300-9001dc00-59e3-11e9-93b5-2dfac3814edf.png). It’s in R and what I do is that I give names to a lot of colorbrewer colors:. ```r. prettier_colors <- c(. setNames(brewer.pal(8, 'Dark2')[c(1,4,6,8)], c('turquoise', 'magenta', 'yellow', 'black')),. setNames(brewer.pal(9, 'Set1')[-c(4,6)], c('red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey')). ). ```. In python that would be:. ```py. import numpy as np. from matplotlib import cm. prettier_colors = dict(zip(. [. 'turquoise', 'magenta', 'yellow', 'black',. 'red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey',. ], np.concatenate([. np.array(cm.Dark2.colors)[[0,3,5,7]],. np.array(cm.Set1.colors)[np.setdiff1d(np.arange(9), [3, 5])],. ]). )). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:40,reliability,doe,doesn,40,"I think we should do that in a way that doesn’t tempt the user to use literal colors like you just did. The color names people think of have a lot of bad properties for colorblind people and contrast. Maybe if we redefine common color names? When users specify “red” there, they only want some kind of red, not `#ff0000`. In one of my analyses, there’s predefined clusters which I recolor like this:. ![grafik](https://user-images.githubusercontent.com/291575/55707300-9001dc00-59e3-11e9-93b5-2dfac3814edf.png). It’s in R and what I do is that I give names to a lot of colorbrewer colors:. ```r. prettier_colors <- c(. setNames(brewer.pal(8, 'Dark2')[c(1,4,6,8)], c('turquoise', 'magenta', 'yellow', 'black')),. setNames(brewer.pal(9, 'Set1')[-c(4,6)], c('red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey')). ). ```. In python that would be:. ```py. import numpy as np. from matplotlib import cm. prettier_colors = dict(zip(. [. 'turquoise', 'magenta', 'yellow', 'black',. 'red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey',. ], np.concatenate([. np.array(cm.Dark2.colors)[[0,3,5,7]],. np.array(cm.Set1.colors)[np.setdiff1d(np.arange(9), [3, 5])],. ]). )). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:58,usability,user,user,58,"I think we should do that in a way that doesn’t tempt the user to use literal colors like you just did. The color names people think of have a lot of bad properties for colorblind people and contrast. Maybe if we redefine common color names? When users specify “red” there, they only want some kind of red, not `#ff0000`. In one of my analyses, there’s predefined clusters which I recolor like this:. ![grafik](https://user-images.githubusercontent.com/291575/55707300-9001dc00-59e3-11e9-93b5-2dfac3814edf.png). It’s in R and what I do is that I give names to a lot of colorbrewer colors:. ```r. prettier_colors <- c(. setNames(brewer.pal(8, 'Dark2')[c(1,4,6,8)], c('turquoise', 'magenta', 'yellow', 'black')),. setNames(brewer.pal(9, 'Set1')[-c(4,6)], c('red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey')). ). ```. In python that would be:. ```py. import numpy as np. from matplotlib import cm. prettier_colors = dict(zip(. [. 'turquoise', 'magenta', 'yellow', 'black',. 'red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey',. ], np.concatenate([. np.array(cm.Dark2.colors)[[0,3,5,7]],. np.array(cm.Set1.colors)[np.setdiff1d(np.arange(9), [3, 5])],. ]). )). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:247,usability,user,users,247,"I think we should do that in a way that doesn’t tempt the user to use literal colors like you just did. The color names people think of have a lot of bad properties for colorblind people and contrast. Maybe if we redefine common color names? When users specify “red” there, they only want some kind of red, not `#ff0000`. In one of my analyses, there’s predefined clusters which I recolor like this:. ![grafik](https://user-images.githubusercontent.com/291575/55707300-9001dc00-59e3-11e9-93b5-2dfac3814edf.png). It’s in R and what I do is that I give names to a lot of colorbrewer colors:. ```r. prettier_colors <- c(. setNames(brewer.pal(8, 'Dark2')[c(1,4,6,8)], c('turquoise', 'magenta', 'yellow', 'black')),. setNames(brewer.pal(9, 'Set1')[-c(4,6)], c('red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey')). ). ```. In python that would be:. ```py. import numpy as np. from matplotlib import cm. prettier_colors = dict(zip(. [. 'turquoise', 'magenta', 'yellow', 'black',. 'red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey',. ], np.concatenate([. np.array(cm.Dark2.colors)[[0,3,5,7]],. np.array(cm.Set1.colors)[np.setdiff1d(np.arange(9), [3, 5])],. ]). )). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:419,usability,user,user-images,419,"I think we should do that in a way that doesn’t tempt the user to use literal colors like you just did. The color names people think of have a lot of bad properties for colorblind people and contrast. Maybe if we redefine common color names? When users specify “red” there, they only want some kind of red, not `#ff0000`. In one of my analyses, there’s predefined clusters which I recolor like this:. ![grafik](https://user-images.githubusercontent.com/291575/55707300-9001dc00-59e3-11e9-93b5-2dfac3814edf.png). It’s in R and what I do is that I give names to a lot of colorbrewer colors:. ```r. prettier_colors <- c(. setNames(brewer.pal(8, 'Dark2')[c(1,4,6,8)], c('turquoise', 'magenta', 'yellow', 'black')),. setNames(brewer.pal(9, 'Set1')[-c(4,6)], c('red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey')). ). ```. In python that would be:. ```py. import numpy as np. from matplotlib import cm. prettier_colors = dict(zip(. [. 'turquoise', 'magenta', 'yellow', 'black',. 'red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey',. ], np.concatenate([. np.array(cm.Dark2.colors)[[0,3,5,7]],. np.array(cm.Set1.colors)[np.setdiff1d(np.arange(9), [3, 5])],. ]). )). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:258,availability,down,down,258,"My main point is that having an implicit mapping between colors and the categories is not that user or developer friendly. It seems to me like it'd be simpler to just have the mapping be explicit. This wouldn't change much from right now, except for cutting down on some boiler plate in a bunch of plotting functions. That example was just to demonstrate that it could even be simpler to have an explicit mapping, since we don't have to do:. ```python. dict(zip(adata.obs[key].cat.categories, adata.uns[key + ""_colors""])). # and instead could just do:. adata.uns[key + ""_colors""]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:239,safety,except,except,239,"My main point is that having an implicit mapping between colors and the categories is not that user or developer friendly. It seems to me like it'd be simpler to just have the mapping be explicit. This wouldn't change much from right now, except for cutting down on some boiler plate in a bunch of plotting functions. That example was just to demonstrate that it could even be simpler to have an explicit mapping, since we don't have to do:. ```python. dict(zip(adata.obs[key].cat.categories, adata.uns[key + ""_colors""])). # and instead could just do:. adata.uns[key + ""_colors""]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:151,testability,simpl,simpler,151,"My main point is that having an implicit mapping between colors and the categories is not that user or developer friendly. It seems to me like it'd be simpler to just have the mapping be explicit. This wouldn't change much from right now, except for cutting down on some boiler plate in a bunch of plotting functions. That example was just to demonstrate that it could even be simpler to have an explicit mapping, since we don't have to do:. ```python. dict(zip(adata.obs[key].cat.categories, adata.uns[key + ""_colors""])). # and instead could just do:. adata.uns[key + ""_colors""]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:377,testability,simpl,simpler,377,"My main point is that having an implicit mapping between colors and the categories is not that user or developer friendly. It seems to me like it'd be simpler to just have the mapping be explicit. This wouldn't change much from right now, except for cutting down on some boiler plate in a bunch of plotting functions. That example was just to demonstrate that it could even be simpler to have an explicit mapping, since we don't have to do:. ```python. dict(zip(adata.obs[key].cat.categories, adata.uns[key + ""_colors""])). # and instead could just do:. adata.uns[key + ""_colors""]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:95,usability,user,user,95,"My main point is that having an implicit mapping between colors and the categories is not that user or developer friendly. It seems to me like it'd be simpler to just have the mapping be explicit. This wouldn't change much from right now, except for cutting down on some boiler plate in a bunch of plotting functions. That example was just to demonstrate that it could even be simpler to have an explicit mapping, since we don't have to do:. ```python. dict(zip(adata.obs[key].cat.categories, adata.uns[key + ""_colors""])). # and instead could just do:. adata.uns[key + ""_colors""]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:151,usability,simpl,simpler,151,"My main point is that having an implicit mapping between colors and the categories is not that user or developer friendly. It seems to me like it'd be simpler to just have the mapping be explicit. This wouldn't change much from right now, except for cutting down on some boiler plate in a bunch of plotting functions. That example was just to demonstrate that it could even be simpler to have an explicit mapping, since we don't have to do:. ```python. dict(zip(adata.obs[key].cat.categories, adata.uns[key + ""_colors""])). # and instead could just do:. adata.uns[key + ""_colors""]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:377,usability,simpl,simpler,377,"My main point is that having an implicit mapping between colors and the categories is not that user or developer friendly. It seems to me like it'd be simpler to just have the mapping be explicit. This wouldn't change much from right now, except for cutting down on some boiler plate in a bunch of plotting functions. That example was just to demonstrate that it could even be simpler to have an explicit mapping, since we don't have to do:. ```python. dict(zip(adata.obs[key].cat.categories, adata.uns[key + ""_colors""])). # and instead could just do:. adata.uns[key + ""_colors""]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:166,integrability,interfac,interface,166,"Sure, I agree with you generally. I’m just saying that an easy, userfriendly way should make it easy to get beautiful colors, not ugly ones. So in the PR tuning that interface to be nicer, we should do something about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:166,interoperability,interfac,interface,166,"Sure, I agree with you generally. I’m just saying that an easy, userfriendly way should make it easy to get beautiful colors, not ugly ones. So in the PR tuning that interface to be nicer, we should do something about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:166,modifiability,interfac,interface,166,"Sure, I agree with you generally. I’m just saying that an easy, userfriendly way should make it easy to get beautiful colors, not ugly ones. So in the PR tuning that interface to be nicer, we should do something about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:64,usability,user,userfriendly,64,"Sure, I agree with you generally. I’m just saying that an easy, userfriendly way should make it easy to get beautiful colors, not ugly ones. So in the PR tuning that interface to be nicer, we should do something about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:334,deployability,manag,manages,334,"Sorry about the late response here... I can imagine that we'd have more than the color attributes for each category. Just think of a `.groupby` followed by a summary statistics. This gives rise to dataframes that are ordered as `.cat.categories`. The array-way of storing color attributes is inspired by the way that `pd.Categorical` manages the string attributes of categories. Also, seaborn accepts this array-way of passing palettes. You can just do `palette=adata.uns['..._colors']` in any of the seaborn functions. At least I think I've done that many times already. I think that the current way of storing colors is ugly and bad. One improvement would be to have one dict for colors only. `adata.uns['colors'] = {'cat_var1': [...], 'cat_var2': ...}`. or one dict for all attributes of each categorical. `adata.uns['louvain'] = {'colors': [...], 'annotations': [...], 'mean_expr': [...], 'markers': [...]}`. Evidently, in the latter case, one would rather like to have a more powerful `pd.Categorical` class that can do more than just attach a string label to an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:334,energy efficiency,manag,manages,334,"Sorry about the late response here... I can imagine that we'd have more than the color attributes for each category. Just think of a `.groupby` followed by a summary statistics. This gives rise to dataframes that are ordered as `.cat.categories`. The array-way of storing color attributes is inspired by the way that `pd.Categorical` manages the string attributes of categories. Also, seaborn accepts this array-way of passing palettes. You can just do `palette=adata.uns['..._colors']` in any of the seaborn functions. At least I think I've done that many times already. I think that the current way of storing colors is ugly and bad. One improvement would be to have one dict for colors only. `adata.uns['colors'] = {'cat_var1': [...], 'cat_var2': ...}`. or one dict for all attributes of each categorical. `adata.uns['louvain'] = {'colors': [...], 'annotations': [...], 'mean_expr': [...], 'markers': [...]}`. Evidently, in the latter case, one would rather like to have a more powerful `pd.Categorical` class that can do more than just attach a string label to an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:589,energy efficiency,current,current,589,"Sorry about the late response here... I can imagine that we'd have more than the color attributes for each category. Just think of a `.groupby` followed by a summary statistics. This gives rise to dataframes that are ordered as `.cat.categories`. The array-way of storing color attributes is inspired by the way that `pd.Categorical` manages the string attributes of categories. Also, seaborn accepts this array-way of passing palettes. You can just do `palette=adata.uns['..._colors']` in any of the seaborn functions. At least I think I've done that many times already. I think that the current way of storing colors is ugly and bad. One improvement would be to have one dict for colors only. `adata.uns['colors'] = {'cat_var1': [...], 'cat_var2': ...}`. or one dict for all attributes of each categorical. `adata.uns['louvain'] = {'colors': [...], 'annotations': [...], 'mean_expr': [...], 'markers': [...]}`. Evidently, in the latter case, one would rather like to have a more powerful `pd.Categorical` class that can do more than just attach a string label to an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:981,energy efficiency,power,powerful,981,"Sorry about the late response here... I can imagine that we'd have more than the color attributes for each category. Just think of a `.groupby` followed by a summary statistics. This gives rise to dataframes that are ordered as `.cat.categories`. The array-way of storing color attributes is inspired by the way that `pd.Categorical` manages the string attributes of categories. Also, seaborn accepts this array-way of passing palettes. You can just do `palette=adata.uns['..._colors']` in any of the seaborn functions. At least I think I've done that many times already. I think that the current way of storing colors is ugly and bad. One improvement would be to have one dict for colors only. `adata.uns['colors'] = {'cat_var1': [...], 'cat_var2': ...}`. or one dict for all attributes of each categorical. `adata.uns['louvain'] = {'colors': [...], 'annotations': [...], 'mean_expr': [...], 'markers': [...]}`. Evidently, in the latter case, one would rather like to have a more powerful `pd.Categorical` class that can do more than just attach a string label to an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:557,performance,time,times,557,"Sorry about the late response here... I can imagine that we'd have more than the color attributes for each category. Just think of a `.groupby` followed by a summary statistics. This gives rise to dataframes that are ordered as `.cat.categories`. The array-way of storing color attributes is inspired by the way that `pd.Categorical` manages the string attributes of categories. Also, seaborn accepts this array-way of passing palettes. You can just do `palette=adata.uns['..._colors']` in any of the seaborn functions. At least I think I've done that many times already. I think that the current way of storing colors is ugly and bad. One improvement would be to have one dict for colors only. `adata.uns['colors'] = {'cat_var1': [...], 'cat_var2': ...}`. or one dict for all attributes of each categorical. `adata.uns['louvain'] = {'colors': [...], 'annotations': [...], 'mean_expr': [...], 'markers': [...]}`. Evidently, in the latter case, one would rather like to have a more powerful `pd.Categorical` class that can do more than just attach a string label to an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:334,safety,manag,manages,334,"Sorry about the late response here... I can imagine that we'd have more than the color attributes for each category. Just think of a `.groupby` followed by a summary statistics. This gives rise to dataframes that are ordered as `.cat.categories`. The array-way of storing color attributes is inspired by the way that `pd.Categorical` manages the string attributes of categories. Also, seaborn accepts this array-way of passing palettes. You can just do `palette=adata.uns['..._colors']` in any of the seaborn functions. At least I think I've done that many times already. I think that the current way of storing colors is ugly and bad. One improvement would be to have one dict for colors only. `adata.uns['colors'] = {'cat_var1': [...], 'cat_var2': ...}`. or one dict for all attributes of each categorical. `adata.uns['louvain'] = {'colors': [...], 'annotations': [...], 'mean_expr': [...], 'markers': [...]}`. Evidently, in the latter case, one would rather like to have a more powerful `pd.Categorical` class that can do more than just attach a string label to an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/598:136,availability,slo,slot,136,I also meet the same issue today. It seems that the problem is caused by col_graphs group in the loom file. If you remove Seurat3 graph slot and write out to loom. It would be done,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:136,reliability,slo,slot,136,I also meet the same issue today. It seems that the problem is caused by col_graphs group in the loom file. If you remove Seurat3 graph slot and write out to loom. It would be done,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:54,availability,slo,slot,54,"Hi, thanks for your answer. How do you remove a graph slot from a Seurat object? When I try, I get this error:. ```. > dataset@graphs <- NULL. Error in (function (cl, name, valueClass) : . assignment of an object of class “NULL” is not valid for @‘graphs’ in an object of class “Seurat”; is(value, ""list"") is not TRUE. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:104,availability,error,error,104,"Hi, thanks for your answer. How do you remove a graph slot from a Seurat object? When I try, I get this error:. ```. > dataset@graphs <- NULL. Error in (function (cl, name, valueClass) : . assignment of an object of class “NULL” is not valid for @‘graphs’ in an object of class “Seurat”; is(value, ""list"") is not TRUE. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:143,availability,Error,Error,143,"Hi, thanks for your answer. How do you remove a graph slot from a Seurat object? When I try, I get this error:. ```. > dataset@graphs <- NULL. Error in (function (cl, name, valueClass) : . assignment of an object of class “NULL” is not valid for @‘graphs’ in an object of class “Seurat”; is(value, ""list"") is not TRUE. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:104,performance,error,error,104,"Hi, thanks for your answer. How do you remove a graph slot from a Seurat object? When I try, I get this error:. ```. > dataset@graphs <- NULL. Error in (function (cl, name, valueClass) : . assignment of an object of class “NULL” is not valid for @‘graphs’ in an object of class “Seurat”; is(value, ""list"") is not TRUE. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:143,performance,Error,Error,143,"Hi, thanks for your answer. How do you remove a graph slot from a Seurat object? When I try, I get this error:. ```. > dataset@graphs <- NULL. Error in (function (cl, name, valueClass) : . assignment of an object of class “NULL” is not valid for @‘graphs’ in an object of class “Seurat”; is(value, ""list"") is not TRUE. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:54,reliability,slo,slot,54,"Hi, thanks for your answer. How do you remove a graph slot from a Seurat object? When I try, I get this error:. ```. > dataset@graphs <- NULL. Error in (function (cl, name, valueClass) : . assignment of an object of class “NULL” is not valid for @‘graphs’ in an object of class “Seurat”; is(value, ""list"") is not TRUE. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:104,safety,error,error,104,"Hi, thanks for your answer. How do you remove a graph slot from a Seurat object? When I try, I get this error:. ```. > dataset@graphs <- NULL. Error in (function (cl, name, valueClass) : . assignment of an object of class “NULL” is not valid for @‘graphs’ in an object of class “Seurat”; is(value, ""list"") is not TRUE. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:143,safety,Error,Error,143,"Hi, thanks for your answer. How do you remove a graph slot from a Seurat object? When I try, I get this error:. ```. > dataset@graphs <- NULL. Error in (function (cl, name, valueClass) : . assignment of an object of class “NULL” is not valid for @‘graphs’ in an object of class “Seurat”; is(value, ""list"") is not TRUE. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:236,safety,valid,valid,236,"Hi, thanks for your answer. How do you remove a graph slot from a Seurat object? When I try, I get this error:. ```. > dataset@graphs <- NULL. Error in (function (cl, name, valueClass) : . assignment of an object of class “NULL” is not valid for @‘graphs’ in an object of class “Seurat”; is(value, ""list"") is not TRUE. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:104,usability,error,error,104,"Hi, thanks for your answer. How do you remove a graph slot from a Seurat object? When I try, I get this error:. ```. > dataset@graphs <- NULL. Error in (function (cl, name, valueClass) : . assignment of an object of class “NULL” is not valid for @‘graphs’ in an object of class “Seurat”; is(value, ""list"") is not TRUE. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:143,usability,Error,Error,143,"Hi, thanks for your answer. How do you remove a graph slot from a Seurat object? When I try, I get this error:. ```. > dataset@graphs <- NULL. Error in (function (cl, name, valueClass) : . assignment of an object of class “NULL” is not valid for @‘graphs’ in an object of class “Seurat”; is(value, ""list"") is not TRUE. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:16,availability,slo,slot,16,"Hi, . the graph slot is assumed to be list . Just do that:. `pbmc_small@graphs <- list()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:16,reliability,slo,slot,16,"Hi, . the graph slot is assumed to be list . Just do that:. `pbmc_small@graphs <- list()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:46,availability,slo,slot,46,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:60,availability,error,error,60,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:96,availability,error,error,96,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1783,availability,Mask,MaskedArray,1783,"name=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:267,deployability,modul,module,267,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1563,energy efficiency,core,core,1563,"/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1886,energy efficiency,core,core,1886,"dex, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:2141,energy efficiency,core,core,2141,"tions. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:2443,energy efficiency,core,core,2443,"tions. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:2754,energy efficiency,core,core,2754,"tions. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:2859,integrability,sub,subarr,2859,"tions. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:2985,integrability,sub,subarr,2985,"tions. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:267,modifiability,modul,module,267,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:352,modifiability,pac,packages,352,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:493,modifiability,layer,layers,493,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:500,modifiability,layer,layers,500,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:585,modifiability,pac,packages,585,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:658,modifiability,layer,layers,658,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:730,modifiability,layer,layers,730,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:737,modifiability,layer,layers,737,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:938,modifiability,pac,packages,938,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1023,modifiability,layer,layers,1023,"ld remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1300,modifiability,pac,packages,1300,"('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1547,modifiability,pac,packages,1547,"8 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.v",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1870,modifiability,pac,packages,1870,"ata', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:2125,modifiability,pac,packages,2125,"tions. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:2427,modifiability,pac,packages,2427,"tions. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:2738,modifiability,pac,packages,2738,"tions. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:60,performance,error,error,60,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:96,performance,error,error,96,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:46,reliability,slo,slot,46,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:60,safety,error,error,60,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:96,safety,error,error,96,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:187,safety,Except,Exception,187,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:241,safety,input,input-,241,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:267,safety,modul,module,267,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:2929,safety,Except,Exception,2929,"tions. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:3036,safety,Except,Exception,3036,"tions. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:197,testability,Trace,Traceback,197,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:60,usability,error,error,60,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:96,usability,error,error,96,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:241,usability,input,input-,241,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-2-aae861244dfa> in <module>. ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 21",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:59,availability,slo,slot,59,It seems that something wrong happened for the Seurat meta slot. The code told that this error happened when AnnData tried to construct obs attribute. . I am afraid this beyond my scope since I cannot access your data for further debugging,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:89,availability,error,error,89,It seems that something wrong happened for the Seurat meta slot. The code told that this error happened when AnnData tried to construct obs attribute. . I am afraid this beyond my scope since I cannot access your data for further debugging,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:89,performance,error,error,89,It seems that something wrong happened for the Seurat meta slot. The code told that this error happened when AnnData tried to construct obs attribute. . I am afraid this beyond my scope since I cannot access your data for further debugging,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:59,reliability,slo,slot,59,It seems that something wrong happened for the Seurat meta slot. The code told that this error happened when AnnData tried to construct obs attribute. . I am afraid this beyond my scope since I cannot access your data for further debugging,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:89,safety,error,error,89,It seems that something wrong happened for the Seurat meta slot. The code told that this error happened when AnnData tried to construct obs attribute. . I am afraid this beyond my scope since I cannot access your data for further debugging,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:201,security,access,access,201,It seems that something wrong happened for the Seurat meta slot. The code told that this error happened when AnnData tried to construct obs attribute. . I am afraid this beyond my scope since I cannot access your data for further debugging,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:89,usability,error,error,89,It seems that something wrong happened for the Seurat meta slot. The code told that this error happened when AnnData tried to construct obs attribute. . I am afraid this beyond my scope since I cannot access your data for further debugging,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:22,availability,error,error,22,I am getting the same error. > Exception: Data must be 1-dimensional,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:22,performance,error,error,22,I am getting the same error. > Exception: Data must be 1-dimensional,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:22,safety,error,error,22,I am getting the same error. > Exception: Data must be 1-dimensional,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:31,safety,Except,Exception,31,I am getting the same error. > Exception: Data must be 1-dimensional,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:22,usability,error,error,22,I am getting the same error. > Exception: Data must be 1-dimensional,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:139,usability,user,user-images,139,"@cakirb . Your file is good. . I don't see any problem for reading the loom file you provided, at least by scanpy v1.4.1. ![image](https://user-images.githubusercontent.com/27558284/57023359-5cbe1000-6c64-11e9-8ff7-9bdadacddc12.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:79,deployability,version,versions,79,"Hi, that's interesting. I'm also using scanpy v1.4.1. Which anndata and loompy versions are you using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:79,integrability,version,versions,79,"Hi, that's interesting. I'm also using scanpy v1.4.1. Which anndata and loompy versions are you using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:79,modifiability,version,versions,79,"Hi, that's interesting. I'm also using scanpy v1.4.1. Which anndata and loompy versions are you using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:97,deployability,version,versions,97,"Hi @PedroRaposo, unfortunately not. My colleague told me that this issue could be related to the versions of scanpy, anndata or loompy. I have the same scanpy version with the successful test above. Maybe, it is related to loompy and anndata version but I'm not sure...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:159,deployability,version,version,159,"Hi @PedroRaposo, unfortunately not. My colleague told me that this issue could be related to the versions of scanpy, anndata or loompy. I have the same scanpy version with the successful test above. Maybe, it is related to loompy and anndata version but I'm not sure...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:242,deployability,version,version,242,"Hi @PedroRaposo, unfortunately not. My colleague told me that this issue could be related to the versions of scanpy, anndata or loompy. I have the same scanpy version with the successful test above. Maybe, it is related to loompy and anndata version but I'm not sure...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:97,integrability,version,versions,97,"Hi @PedroRaposo, unfortunately not. My colleague told me that this issue could be related to the versions of scanpy, anndata or loompy. I have the same scanpy version with the successful test above. Maybe, it is related to loompy and anndata version but I'm not sure...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:159,integrability,version,version,159,"Hi @PedroRaposo, unfortunately not. My colleague told me that this issue could be related to the versions of scanpy, anndata or loompy. I have the same scanpy version with the successful test above. Maybe, it is related to loompy and anndata version but I'm not sure...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:242,integrability,version,version,242,"Hi @PedroRaposo, unfortunately not. My colleague told me that this issue could be related to the versions of scanpy, anndata or loompy. I have the same scanpy version with the successful test above. Maybe, it is related to loompy and anndata version but I'm not sure...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:97,modifiability,version,versions,97,"Hi @PedroRaposo, unfortunately not. My colleague told me that this issue could be related to the versions of scanpy, anndata or loompy. I have the same scanpy version with the successful test above. Maybe, it is related to loompy and anndata version but I'm not sure...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:159,modifiability,version,version,159,"Hi @PedroRaposo, unfortunately not. My colleague told me that this issue could be related to the versions of scanpy, anndata or loompy. I have the same scanpy version with the successful test above. Maybe, it is related to loompy and anndata version but I'm not sure...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:242,modifiability,version,version,242,"Hi @PedroRaposo, unfortunately not. My colleague told me that this issue could be related to the versions of scanpy, anndata or loompy. I have the same scanpy version with the successful test above. Maybe, it is related to loompy and anndata version but I'm not sure...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:187,safety,test,test,187,"Hi @PedroRaposo, unfortunately not. My colleague told me that this issue could be related to the versions of scanpy, anndata or loompy. I have the same scanpy version with the successful test above. Maybe, it is related to loompy and anndata version but I'm not sure...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:187,testability,test,test,187,"Hi @PedroRaposo, unfortunately not. My colleague told me that this issue could be related to the versions of scanpy, anndata or loompy. I have the same scanpy version with the successful test above. Maybe, it is related to loompy and anndata version but I'm not sure...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:18,deployability,instal,installing,18,"Hi, @cakirb . Try installing `loompy` using `pip install -U loompy`, and make sure you are not using version 2.0.2. see . https://github.com/theislab/scvelo/issues/20#issuecomment-442186279. **EDITED**: I am encountering the same problem as yours. > Exception: Data must be 1-dimensional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:49,deployability,instal,install,49,"Hi, @cakirb . Try installing `loompy` using `pip install -U loompy`, and make sure you are not using version 2.0.2. see . https://github.com/theislab/scvelo/issues/20#issuecomment-442186279. **EDITED**: I am encountering the same problem as yours. > Exception: Data must be 1-dimensional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:101,deployability,version,version,101,"Hi, @cakirb . Try installing `loompy` using `pip install -U loompy`, and make sure you are not using version 2.0.2. see . https://github.com/theislab/scvelo/issues/20#issuecomment-442186279. **EDITED**: I am encountering the same problem as yours. > Exception: Data must be 1-dimensional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:101,integrability,version,version,101,"Hi, @cakirb . Try installing `loompy` using `pip install -U loompy`, and make sure you are not using version 2.0.2. see . https://github.com/theislab/scvelo/issues/20#issuecomment-442186279. **EDITED**: I am encountering the same problem as yours. > Exception: Data must be 1-dimensional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:101,modifiability,version,version,101,"Hi, @cakirb . Try installing `loompy` using `pip install -U loompy`, and make sure you are not using version 2.0.2. see . https://github.com/theislab/scvelo/issues/20#issuecomment-442186279. **EDITED**: I am encountering the same problem as yours. > Exception: Data must be 1-dimensional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:250,safety,Except,Exception,250,"Hi, @cakirb . Try installing `loompy` using `pip install -U loompy`, and make sure you are not using version 2.0.2. see . https://github.com/theislab/scvelo/issues/20#issuecomment-442186279. **EDITED**: I am encountering the same problem as yours. > Exception: Data must be 1-dimensional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:73,availability,error,error,73,"I have tried using latest version of anndata(0.16.9), still got the same error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:26,deployability,version,version,26,"I have tried using latest version of anndata(0.16.9), still got the same error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:26,integrability,version,version,26,"I have tried using latest version of anndata(0.16.9), still got the same error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:26,modifiability,version,version,26,"I have tried using latest version of anndata(0.16.9), still got the same error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:73,performance,error,error,73,"I have tried using latest version of anndata(0.16.9), still got the same error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:73,safety,error,error,73,"I have tried using latest version of anndata(0.16.9), still got the same error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:73,usability,error,error,73,"I have tried using latest version of anndata(0.16.9), still got the same error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:21,deployability,version,version,21,My too. I had loompy version 2.0.17 and now I installed the version 2.0.16 and still I'm getting the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:46,deployability,instal,installed,46,My too. I had loompy version 2.0.17 and now I installed the version 2.0.16 and still I'm getting the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:60,deployability,version,version,60,My too. I had loompy version 2.0.17 and now I installed the version 2.0.16 and still I'm getting the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:21,integrability,version,version,21,My too. I had loompy version 2.0.17 and now I installed the version 2.0.16 and still I'm getting the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:60,integrability,version,version,60,My too. I had loompy version 2.0.17 and now I installed the version 2.0.16 and still I'm getting the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:21,modifiability,version,version,21,My too. I had loompy version 2.0.17 and now I installed the version 2.0.16 and still I'm getting the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:60,modifiability,version,version,60,My too. I had loompy version 2.0.17 and now I installed the version 2.0.16 and still I'm getting the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:79,deployability,version,version,79,"Hi, @ahy1221 . Could you provide more details of your environment and packages version? Many thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:79,integrability,version,version,79,"Hi, @ahy1221 . Could you provide more details of your environment and packages version? Many thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:70,modifiability,pac,packages,70,"Hi, @ahy1221 . Could you provide more details of your environment and packages version? Many thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:79,modifiability,version,version,79,"Hi, @ahy1221 . Could you provide more details of your environment and packages version? Many thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:366,deployability,fail,fails,366,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:243,energy efficiency,load,loading,243,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:293,modifiability,interm,intermediate,293,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:243,performance,load,loading,243,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:366,reliability,fail,fails,366,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:501,reliability,rpo,rpois,501,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:80,safety,compl,complete,80,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:171,safety,compl,complete,171,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:80,security,compl,complete,80,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:171,security,compl,complete,171,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:442,testability,simpl,simple,442,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:12,usability,help,help,12,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:17,usability,close,close,17,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:136,usability,experien,experiencing,136,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:278,usability,minim,minimal,278,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:442,usability,simpl,simple,442,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:628,usability,help,help,628,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue? This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:17,deployability,modul,modules,17,"For me, python's modules versions was the problem. Now it works (for your information, you can see this thread https://github.com/theislab/anndata/issues/152).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:25,deployability,version,versions,25,"For me, python's modules versions was the problem. Now it works (for your information, you can see this thread https://github.com/theislab/anndata/issues/152).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:25,integrability,version,versions,25,"For me, python's modules versions was the problem. Now it works (for your information, you can see this thread https://github.com/theislab/anndata/issues/152).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:17,modifiability,modul,modules,17,"For me, python's modules versions was the problem. Now it works (for your information, you can see this thread https://github.com/theislab/anndata/issues/152).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:25,modifiability,version,versions,25,"For me, python's modules versions was the problem. Now it works (for your information, you can see this thread https://github.com/theislab/anndata/issues/152).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:17,safety,modul,modules,17,"For me, python's modules versions was the problem. Now it works (for your information, you can see this thread https://github.com/theislab/anndata/issues/152).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:13,deployability,Updat,Updating,13,@PedroRaposo Updating all these packages works! Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:32,modifiability,pac,packages,32,@PedroRaposo Updating all these packages works! Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:13,safety,Updat,Updating,13,@PedroRaposo Updating all these packages works! Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:13,security,Updat,Updating,13,@PedroRaposo Updating all these packages works! Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:31,usability,close,close,31,It works for me too!! So I can close the issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:210,safety,input,input,210,When I try to read loom file created bu seurat through sc.read_loom or scv.read or loompy.connect. everything becomes still the cell is always busy with * sign and nothing gets executed basically it hangs. any input?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:155,security,sign,sign,155,When I try to read loom file created bu seurat through sc.read_loom or scv.read or loompy.connect. everything becomes still the cell is always busy with * sign and nothing gets executed basically it hangs. any input?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:210,usability,input,input,210,When I try to read loom file created bu seurat through sc.read_loom or scv.read or loompy.connect. everything becomes still the cell is always busy with * sign and nothing gets executed basically it hangs. any input?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:90,availability,slo,slot,90,"After removing the graphs and loading the loom file into scanpy with the now empty graphs slot, is there a way to manually add it back in? For example, before removing the graphs attribute, I call as.matrix() and saved it as a CSV (probably a better way to do this to maintain the sparse property). I can now read this CSV back into Python (e.g. with pandas), but what is the correct way to reload it into the resulting AnnData object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:30,energy efficiency,load,loading,30,"After removing the graphs and loading the loom file into scanpy with the now empty graphs slot, is there a way to manually add it back in? For example, before removing the graphs attribute, I call as.matrix() and saved it as a CSV (probably a better way to do this to maintain the sparse property). I can now read this CSV back into Python (e.g. with pandas), but what is the correct way to reload it into the resulting AnnData object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:268,modifiability,maintain,maintain,268,"After removing the graphs and loading the loom file into scanpy with the now empty graphs slot, is there a way to manually add it back in? For example, before removing the graphs attribute, I call as.matrix() and saved it as a CSV (probably a better way to do this to maintain the sparse property). I can now read this CSV back into Python (e.g. with pandas), but what is the correct way to reload it into the resulting AnnData object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:30,performance,load,loading,30,"After removing the graphs and loading the loom file into scanpy with the now empty graphs slot, is there a way to manually add it back in? For example, before removing the graphs attribute, I call as.matrix() and saved it as a CSV (probably a better way to do this to maintain the sparse property). I can now read this CSV back into Python (e.g. with pandas), but what is the correct way to reload it into the resulting AnnData object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:90,reliability,slo,slot,90,"After removing the graphs and loading the loom file into scanpy with the now empty graphs slot, is there a way to manually add it back in? For example, before removing the graphs attribute, I call as.matrix() and saved it as a CSV (probably a better way to do this to maintain the sparse property). I can now read this CSV back into Python (e.g. with pandas), but what is the correct way to reload it into the resulting AnnData object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:268,safety,maintain,maintain,268,"After removing the graphs and loading the loom file into scanpy with the now empty graphs slot, is there a way to manually add it back in? For example, before removing the graphs attribute, I call as.matrix() and saved it as a CSV (probably a better way to do this to maintain the sparse property). I can now read this CSV back into Python (e.g. with pandas), but what is the correct way to reload it into the resulting AnnData object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/599:72,deployability,contain,contain,72,"Try. ```. adata = adata[adata[: , 'elav'].X > 0.5, :] . ```. `var` only contain the metadata for the genes. The expression is housed in `X`, just as the metadata for the observations (cells) are stored in `obs`. And you should be able to index adata with gene names directly as `adata[:, <list of names>]`. if they exist in the `var_names` list, `adata.var_names`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:170,deployability,observ,observations,170,"Try. ```. adata = adata[adata[: , 'elav'].X > 0.5, :] . ```. `var` only contain the metadata for the genes. The expression is housed in `X`, just as the metadata for the observations (cells) are stored in `obs`. And you should be able to index adata with gene names directly as `adata[:, <list of names>]`. if they exist in the `var_names` list, `adata.var_names`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:170,testability,observ,observations,170,"Try. ```. adata = adata[adata[: , 'elav'].X > 0.5, :] . ```. `var` only contain the metadata for the genes. The expression is housed in `X`, just as the metadata for the observations (cells) are stored in `obs`. And you should be able to index adata with gene names directly as `adata[:, <list of names>]`. if they exist in the `var_names` list, `adata.var_names`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:79,availability,error,error,79,"For me,. adata[(adata[:,'elav'].X>0).flatten(), : ] works. otherwise, it gives error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:79,performance,error,error,79,"For me,. adata[(adata[:,'elav'].X>0).flatten(), : ] works. otherwise, it gives error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:79,safety,error,error,79,"For me,. adata[(adata[:,'elav'].X>0).flatten(), : ] works. otherwise, it gives error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:79,usability,error,error,79,"For me,. adata[(adata[:,'elav'].X>0).flatten(), : ] works. otherwise, it gives error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:118,integrability,filter,filtering-cells-by-expression-of-specific-gene,118,This issue has been mentioned on **Scanpy**. There might be relevant details there:. https://scanpy.discourse.group/t/filtering-cells-by-expression-of-specific-gene/256/2.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:151,interoperability,specif,specific-gene,151,This issue has been mentioned on **Scanpy**. There might be relevant details there:. https://scanpy.discourse.group/t/filtering-cells-by-expression-of-specific-gene/256/2.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:194,availability,error,error,194,"Is there a way to filter for a set of genes, where if any one of the genes in a list are expressed, those cells will be plotted? I've tried switching Xparx's solution to a list, but receive the error ""ValueError: Buffer has wrong number of dimensions (expected 1, got 0)"". I've also tired chansigit's method, but find that flatten returns as not found?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:18,integrability,filter,filter,18,"Is there a way to filter for a set of genes, where if any one of the genes in a list are expressed, those cells will be plotted? I've tried switching Xparx's solution to a list, but receive the error ""ValueError: Buffer has wrong number of dimensions (expected 1, got 0)"". I've also tired chansigit's method, but find that flatten returns as not found?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
