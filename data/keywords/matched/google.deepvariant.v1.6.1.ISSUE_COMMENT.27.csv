id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/685:345,safety,error,error,345,"Ok that seems to be working with absolute path (at least it's making the examples right now). Also, the index of the bam was corrupted! it seems that you declare the BAM but then deepvariant goes to the index. Which makes sense of course, but maybe it would be a good idea to say somewhere in the doc to pay attention to the index? Or return an error like ""index not found"" rather than ""bam not found""? and why dry run didn't catch this? . Anyway, thank you .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:345,usability,error,error,345,"Ok that seems to be working with absolute path (at least it's making the examples right now). Also, the index of the bam was corrupted! it seems that you declare the BAM but then deepvariant goes to the index. Which makes sense of course, but maybe it would be a good idea to say somewhere in the doc to pay attention to the index? Or return an error like ""index not found"" rather than ""bam not found""? and why dry run didn't catch this? . Anyway, thank you .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:688,availability,error,error,688,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1035,deployability,fail,failed,1035,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1061,modifiability,variab,variable,1061,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:185,performance,time,time,185,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:207,performance,time,time,207,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:688,performance,error,error,688,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1027,performance,time,time,1027,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1218,performance,time,times,1218,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1035,reliability,fail,failed,1035,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:671,safety,compl,complain,671,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:688,safety,error,error,688,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:797,safety,input,input,797,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1130,safety,input,input,1130,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:671,security,compl,complain,671,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:215,testability,understand,understand,215,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1159,testability,understand,understandable,1159,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:688,usability,error,error,688,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:797,usability,input,input,797,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:834,usability,help,helps,834,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:911,usability,document,document,911,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1130,usability,input,input,1130,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1294,usability,command,commands,1294,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1431,usability,help,helps,1431,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```. [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'. ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:149,availability,echo,echo,149,"@Axze-rgb,. In bash when you declare a variable as:. ```. BAM=HiFi_vaga.sorted.bam. ```. Then you need to use it as `${BAM}`, for example:. ```bash. echo ${BAM}. # vs. echo BAM. ```. Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:168,availability,echo,echo,168,"@Axze-rgb,. In bash when you declare a variable as:. ```. BAM=HiFi_vaga.sorted.bam. ```. Then you need to use it as `${BAM}`, for example:. ```bash. echo ${BAM}. # vs. echo BAM. ```. Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:393,availability,error,error,393,"@Axze-rgb,. In bash when you declare a variable as:. ```. BAM=HiFi_vaga.sorted.bam. ```. Then you need to use it as `${BAM}`, for example:. ```bash. echo ${BAM}. # vs. echo BAM. ```. Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:526,availability,error,errors,526,"@Axze-rgb,. In bash when you declare a variable as:. ```. BAM=HiFi_vaga.sorted.bam. ```. Then you need to use it as `${BAM}`, for example:. ```bash. echo ${BAM}. # vs. echo BAM. ```. Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:39,modifiability,variab,variable,39,"@Axze-rgb,. In bash when you declare a variable as:. ```. BAM=HiFi_vaga.sorted.bam. ```. Then you need to use it as `${BAM}`, for example:. ```bash. echo ${BAM}. # vs. echo BAM. ```. Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:255,modifiability,variab,variables,255,"@Axze-rgb,. In bash when you declare a variable as:. ```. BAM=HiFi_vaga.sorted.bam. ```. Then you need to use it as `${BAM}`, for example:. ```bash. echo ${BAM}. # vs. echo BAM. ```. Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:393,performance,error,error,393,"@Axze-rgb,. In bash when you declare a variable as:. ```. BAM=HiFi_vaga.sorted.bam. ```. Then you need to use it as `${BAM}`, for example:. ```bash. echo ${BAM}. # vs. echo BAM. ```. Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:526,performance,error,errors,526,"@Axze-rgb,. In bash when you declare a variable as:. ```. BAM=HiFi_vaga.sorted.bam. ```. Then you need to use it as `${BAM}`, for example:. ```bash. echo ${BAM}. # vs. echo BAM. ```. Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:393,safety,error,error,393,"@Axze-rgb,. In bash when you declare a variable as:. ```. BAM=HiFi_vaga.sorted.bam. ```. Then you need to use it as `${BAM}`, for example:. ```bash. echo ${BAM}. # vs. echo BAM. ```. Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:526,safety,error,errors,526,"@Axze-rgb,. In bash when you declare a variable as:. ```. BAM=HiFi_vaga.sorted.bam. ```. Then you need to use it as `${BAM}`, for example:. ```bash. echo ${BAM}. # vs. echo BAM. ```. Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:393,usability,error,error,393,"@Axze-rgb,. In bash when you declare a variable as:. ```. BAM=HiFi_vaga.sorted.bam. ```. Then you need to use it as `${BAM}`, for example:. ```bash. echo ${BAM}. # vs. echo BAM. ```. Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:526,usability,error,errors,526,"@Axze-rgb,. In bash when you declare a variable as:. ```. BAM=HiFi_vaga.sorted.bam. ```. Then you need to use it as `${BAM}`, for example:. ```bash. echo ${BAM}. # vs. echo BAM. ```. Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:635,usability,help,helps,635,"@Axze-rgb,. In bash when you declare a variable as:. ```. BAM=HiFi_vaga.sorted.bam. ```. Then you need to use it as `${BAM}`, for example:. ```bash. echo ${BAM}. # vs. echo BAM. ```. Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/686:634,integrability,sub,subscribed,634,"Saw this the other day. https://github.com/marcelauliano/MitoHiFi/tree/master. On Wed, 26 Jul 2023, 08:05 crazysummerW, ***@***.***> wrote:. > Hello,. > I would like to know if it is possible to use DeepVariant to analyze. > PacBio mitochondrial data. If not, do you have any suitable tools to. > recommend? >. > Looking forward to your reply. > Thanks. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/686>, or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BAYQV2XGN5AXQVMVB6LKOIDXSC6TTANCNFSM6AAAAAA2YDZTZU>. > . > You are receiving this because you are subscribed to this thread.Message. > ID: ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:660,integrability,Messag,Message,660,"Saw this the other day. https://github.com/marcelauliano/MitoHiFi/tree/master. On Wed, 26 Jul 2023, 08:05 crazysummerW, ***@***.***> wrote:. > Hello,. > I would like to know if it is possible to use DeepVariant to analyze. > PacBio mitochondrial data. If not, do you have any suitable tools to. > recommend? >. > Looking forward to your reply. > Thanks. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/686>, or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BAYQV2XGN5AXQVMVB6LKOIDXSC6TTANCNFSM6AAAAAA2YDZTZU>. > . > You are receiving this because you are subscribed to this thread.Message. > ID: ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:660,interoperability,Messag,Message,660,"Saw this the other day. https://github.com/marcelauliano/MitoHiFi/tree/master. On Wed, 26 Jul 2023, 08:05 crazysummerW, ***@***.***> wrote:. > Hello,. > I would like to know if it is possible to use DeepVariant to analyze. > PacBio mitochondrial data. If not, do you have any suitable tools to. > recommend? >. > Looking forward to your reply. > Thanks. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/686>, or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BAYQV2XGN5AXQVMVB6LKOIDXSC6TTANCNFSM6AAAAAA2YDZTZU>. > . > You are receiving this because you are subscribed to this thread.Message. > ID: ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:225,modifiability,Pac,PacBio,225,"Saw this the other day. https://github.com/marcelauliano/MitoHiFi/tree/master. On Wed, 26 Jul 2023, 08:05 crazysummerW, ***@***.***> wrote:. > Hello,. > I would like to know if it is possible to use DeepVariant to analyze. > PacBio mitochondrial data. If not, do you have any suitable tools to. > recommend? >. > Looking forward to your reply. > Thanks. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/686>, or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BAYQV2XGN5AXQVMVB6LKOIDXSC6TTANCNFSM6AAAAAA2YDZTZU>. > . > You are receiving this because you are subscribed to this thread.Message. > ID: ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:531,security,auth,auth,531,"Saw this the other day. https://github.com/marcelauliano/MitoHiFi/tree/master. On Wed, 26 Jul 2023, 08:05 crazysummerW, ***@***.***> wrote:. > Hello,. > I would like to know if it is possible to use DeepVariant to analyze. > PacBio mitochondrial data. If not, do you have any suitable tools to. > recommend? >. > Looking forward to your reply. > Thanks. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/686>, or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BAYQV2XGN5AXQVMVB6LKOIDXSC6TTANCNFSM6AAAAAA2YDZTZU>. > . > You are receiving this because you are subscribed to this thread.Message. > ID: ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:285,usability,tool,tools,285,"Saw this the other day. https://github.com/marcelauliano/MitoHiFi/tree/master. On Wed, 26 Jul 2023, 08:05 crazysummerW, ***@***.***> wrote:. > Hello,. > I would like to know if it is possible to use DeepVariant to analyze. > PacBio mitochondrial data. If not, do you have any suitable tools to. > recommend? >. > Looking forward to your reply. > Thanks. >. > . > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/686>, or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/BAYQV2XGN5AXQVMVB6LKOIDXSC6TTANCNFSM6AAAAAA2YDZTZU>. > . > You are receiving this because you are subscribed to this thread.Message. > ID: ***@***.***>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:227,availability,reliab,reliable,227,"Hello, . Thanks for your reply. I don't want to perform assembly. I just want to obtain a VCF file for the mitochondria based on the aligned BAM file. I am unsure if the results for chrM in the VCF generated by DeepVariant are reliable. PacBio mitochondrial data:. ![1690423792423](https://github.com/google/deepvariant/assets/70870741/f6a18fa3-a432-4d53-9824-20a9e309298c). If DeepVariant is not suitable for calling mitochondrial variants on PacBio mitochondrial data, are there any other software recommendations for calling variants at mitochondrial loci without assembly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:237,modifiability,Pac,PacBio,237,"Hello, . Thanks for your reply. I don't want to perform assembly. I just want to obtain a VCF file for the mitochondria based on the aligned BAM file. I am unsure if the results for chrM in the VCF generated by DeepVariant are reliable. PacBio mitochondrial data:. ![1690423792423](https://github.com/google/deepvariant/assets/70870741/f6a18fa3-a432-4d53-9824-20a9e309298c). If DeepVariant is not suitable for calling mitochondrial variants on PacBio mitochondrial data, are there any other software recommendations for calling variants at mitochondrial loci without assembly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:444,modifiability,Pac,PacBio,444,"Hello, . Thanks for your reply. I don't want to perform assembly. I just want to obtain a VCF file for the mitochondria based on the aligned BAM file. I am unsure if the results for chrM in the VCF generated by DeepVariant are reliable. PacBio mitochondrial data:. ![1690423792423](https://github.com/google/deepvariant/assets/70870741/f6a18fa3-a432-4d53-9824-20a9e309298c). If DeepVariant is not suitable for calling mitochondrial variants on PacBio mitochondrial data, are there any other software recommendations for calling variants at mitochondrial loci without assembly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:48,performance,perform,perform,48,"Hello, . Thanks for your reply. I don't want to perform assembly. I just want to obtain a VCF file for the mitochondria based on the aligned BAM file. I am unsure if the results for chrM in the VCF generated by DeepVariant are reliable. PacBio mitochondrial data:. ![1690423792423](https://github.com/google/deepvariant/assets/70870741/f6a18fa3-a432-4d53-9824-20a9e309298c). If DeepVariant is not suitable for calling mitochondrial variants on PacBio mitochondrial data, are there any other software recommendations for calling variants at mitochondrial loci without assembly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:227,reliability,reliab,reliable,227,"Hello, . Thanks for your reply. I don't want to perform assembly. I just want to obtain a VCF file for the mitochondria based on the aligned BAM file. I am unsure if the results for chrM in the VCF generated by DeepVariant are reliable. PacBio mitochondrial data:. ![1690423792423](https://github.com/google/deepvariant/assets/70870741/f6a18fa3-a432-4d53-9824-20a9e309298c). If DeepVariant is not suitable for calling mitochondrial variants on PacBio mitochondrial data, are there any other software recommendations for calling variants at mitochondrial loci without assembly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:48,usability,perform,perform,48,"Hello, . Thanks for your reply. I don't want to perform assembly. I just want to obtain a VCF file for the mitochondria based on the aligned BAM file. I am unsure if the results for chrM in the VCF generated by DeepVariant are reliable. PacBio mitochondrial data:. ![1690423792423](https://github.com/google/deepvariant/assets/70870741/f6a18fa3-a432-4d53-9824-20a9e309298c). If DeepVariant is not suitable for calling mitochondrial variants on PacBio mitochondrial data, are there any other software recommendations for calling variants at mitochondrial loci without assembly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:258,energy efficiency,model,models,258,"@crazysummerW mtDNA variant analysis usually requires more specialized steps, as you need to worry about NUMT and heteroplasmy among other things, especially since the number of mitochondria vary for different cell types. DeepVariant I don't believe has the models trained for that, as it is usually geared for autosomal DNA. So, if you just want a VCF file without the large amount of analysis that is required for dealing with mtDNA, you can use the mitochondria mode of Mutect2 in GATK like this, which performs a lot of it for you:. . ```. gatk Mutect2 \. -R reference.fa \. -L chrM \. --mitochondria-mode \. -I mitochondria.bam \. -O mitochondria.vcf.gz. ```. You can read more about it here:. https://gatk.broadinstitute.org/hc/en-us/articles/13832710384155-Mutect2. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:506,performance,perform,performs,506,"@crazysummerW mtDNA variant analysis usually requires more specialized steps, as you need to worry about NUMT and heteroplasmy among other things, especially since the number of mitochondria vary for different cell types. DeepVariant I don't believe has the models trained for that, as it is usually geared for autosomal DNA. So, if you just want a VCF file without the large amount of analysis that is required for dealing with mtDNA, you can use the mitochondria mode of Mutect2 in GATK like this, which performs a lot of it for you:. . ```. gatk Mutect2 \. -R reference.fa \. -L chrM \. --mitochondria-mode \. -I mitochondria.bam \. -O mitochondria.vcf.gz. ```. You can read more about it here:. https://gatk.broadinstitute.org/hc/en-us/articles/13832710384155-Mutect2. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:258,security,model,models,258,"@crazysummerW mtDNA variant analysis usually requires more specialized steps, as you need to worry about NUMT and heteroplasmy among other things, especially since the number of mitochondria vary for different cell types. DeepVariant I don't believe has the models trained for that, as it is usually geared for autosomal DNA. So, if you just want a VCF file without the large amount of analysis that is required for dealing with mtDNA, you can use the mitochondria mode of Mutect2 in GATK like this, which performs a lot of it for you:. . ```. gatk Mutect2 \. -R reference.fa \. -L chrM \. --mitochondria-mode \. -I mitochondria.bam \. -O mitochondria.vcf.gz. ```. You can read more about it here:. https://gatk.broadinstitute.org/hc/en-us/articles/13832710384155-Mutect2. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:506,usability,perform,performs,506,"@crazysummerW mtDNA variant analysis usually requires more specialized steps, as you need to worry about NUMT and heteroplasmy among other things, especially since the number of mitochondria vary for different cell types. DeepVariant I don't believe has the models trained for that, as it is usually geared for autosomal DNA. So, if you just want a VCF file without the large amount of analysis that is required for dealing with mtDNA, you can use the mitochondria mode of Mutect2 in GATK like this, which performs a lot of it for you:. . ```. gatk Mutect2 \. -R reference.fa \. -L chrM \. --mitochondria-mode \. -I mitochondria.bam \. -O mitochondria.vcf.gz. ```. You can read more about it here:. https://gatk.broadinstitute.org/hc/en-us/articles/13832710384155-Mutect2. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:781,usability,help,helps,781,"@crazysummerW mtDNA variant analysis usually requires more specialized steps, as you need to worry about NUMT and heteroplasmy among other things, especially since the number of mitochondria vary for different cell types. DeepVariant I don't believe has the models trained for that, as it is usually geared for autosomal DNA. So, if you just want a VCF file without the large amount of analysis that is required for dealing with mtDNA, you can use the mitochondria mode of Mutect2 in GATK like this, which performs a lot of it for you:. . ```. gatk Mutect2 \. -R reference.fa \. -L chrM \. --mitochondria-mode \. -I mitochondria.bam \. -O mitochondria.vcf.gz. ```. You can read more about it here:. https://gatk.broadinstitute.org/hc/en-us/articles/13832710384155-Mutect2. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:686,availability,error,errors,686,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:237,energy efficiency,frequenc,frequency,237,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:321,energy efficiency,optim,optimized,321,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:515,energy efficiency,optim,optimized,515,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:590,energy efficiency,frequenc,frequency,590,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:653,energy efficiency,frequenc,frequency,653,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:747,energy efficiency,frequenc,frequency,747,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:489,interoperability,specif,specifically,489,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:910,modifiability,paramet,parameters,910,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:1082,modifiability,Pac,PacificBiosciences,1082,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:321,performance,optimiz,optimized,321,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:515,performance,optimiz,optimized,515,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:686,performance,error,errors,686,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:111,safety,except,exception,111,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:686,safety,error,errors,686,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:41,security,ident,identifying,41,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:574,security,ident,identifying,574,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:312,usability,tool,tool,312,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:686,usability,error,errors,686,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:952,usability,workflow,workflow,952,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:1114,usability,workflow,workflow-snakemake,1114,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:104,modifiability,Pac,PacificBiosciences,104,"Thanks @williamrowell for following up. I noticed that there is already follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106 , so I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:136,usability,workflow,workflow-snakemake,136,"Thanks @williamrowell for following up. I noticed that there is already follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106 , so I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:176,usability,close,close,176,"Thanks @williamrowell for following up. I noticed that there is already follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106 , so I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/687:3350,availability,checkpoint,checkpoint,3350,"ermediate results will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3647,availability,checkpoint,checkpoint,3647,"ds_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3945,availability,checkpoint,checkpoint,3945,"--channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:194,deployability,updat,updated,194,"Hi @hosseinvk,. So just a couple of things:. $`1)`$ The `--output_gvcf_merged` flag is not supported in `run_deeptrio.py`, and GLNexus should be used instead, as indicated by issue #544. I have updated the command with the `--output_gvcf_parent1` and `--output_gvcf_parent2` flags. $`2)`$ If I run it via Docker like this, I get the dry run to work:. ```. docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. ```. The output I get:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_paren",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5253,deployability,version,version,5253,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5302,deployability,version,version,5302,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5336,deployability,version,version,5336,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5454,deployability,version,version,5454,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5472,deployability,version,version,5472,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3367,energy efficiency,model,models,3367,"ts will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3393,energy efficiency,model,model,3393,"ut/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3664,energy efficiency,model,models,3664,"kduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3691,energy efficiency,model,model,3691,""" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3962,energy efficiency,model,models,3962,"ert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3989,energy efficiency,model,model,3989,"termediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:26,integrability,coupl,couple,26,"Hi @hosseinvk,. So just a couple of things:. $`1)`$ The `--output_gvcf_merged` flag is not supported in `run_deeptrio.py`, and GLNexus should be used instead, as indicated by issue #544. I have updated the command with the `--output_gvcf_parent1` and `--output_gvcf_parent2` flags. $`2)`$ If I run it via Docker like this, I get the dry run to work:. ```. docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. ```. The output I get:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_paren",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:2517,integrability,buffer,buffer,2517,"_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. I0726 08:57:43.023837 139824599930688 run_deeptrio.py:519] Creating a directory for intermediate results in output/intermediate_results_dir. ***** Intermediate results will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5253,integrability,version,version,5253,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5302,integrability,version,version,5302,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5336,integrability,version,version,5336,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5454,integrability,version,version,5454,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5472,integrability,version,version,5472,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:26,modifiability,coupl,couple,26,"Hi @hosseinvk,. So just a couple of things:. $`1)`$ The `--output_gvcf_merged` flag is not supported in `run_deeptrio.py`, and GLNexus should be used instead, as indicated by issue #544. I have updated the command with the `--output_gvcf_parent1` and `--output_gvcf_parent2` flags. $`2)`$ If I run it via Docker like this, I get the dry run to work:. ```. docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. ```. The output I get:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_paren",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:2289,modifiability,interm,intermediate,2289,"l$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. I0726 08:57:43.023837 139824599930688 run_deeptrio.py:519] Creating a directory for intermediate results in output/intermediate_results_dir. ***** Intermediate results will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/interme",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:2352,modifiability,Interm,Intermediate,2352,"nt/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. I0726 08:57:43.023837 139824599930688 run_deeptrio.py:519] Creating a directory for intermediate results in output/intermediate_results_dir. ***** Intermediate results will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5253,modifiability,version,version,5253,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5302,modifiability,version,version,5302,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5336,modifiability,version,version,5336,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5454,modifiability,version,version,5454,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5472,modifiability,version,version,5472,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:2474,performance,time,time,2474,"rent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. I0726 08:57:43.023837 139824599930688 run_deeptrio.py:519] Creating a directory for intermediate results in output/intermediate_results_dir. ***** Intermediate results will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:2489,performance,parallel,parallel,2489,"tes/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. I0726 08:57:43.023837 139824599930688 run_deeptrio.py:519] Creating a directory for intermediate results in output/intermediate_results_dir. ***** Intermediate results will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3146,performance,time,time,3146,"parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. I0726 08:57:43.023837 139824599930688 run_deeptrio.py:519] Creating a directory for intermediate results in output/intermediate_results_dir. ***** Intermediate results will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3439,performance,time,time,3439,"***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3737,performance,time,time,3737,".bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:4035,performance,time,time,4035,"pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:4437,performance,time,time,4437,"time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:4842,performance,time,time,4842,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3350,reliability,checkpoint,checkpoint,3350,"ermediate results will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3647,reliability,checkpoint,checkpoint,3647,"ds_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3945,reliability,checkpoint,checkpoint,3945,"--channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:194,safety,updat,updated,194,"Hi @hosseinvk,. So just a couple of things:. $`1)`$ The `--output_gvcf_merged` flag is not supported in `run_deeptrio.py`, and GLNexus should be used instead, as indicated by issue #544. I have updated the command with the `--output_gvcf_parent1` and `--output_gvcf_parent2` flags. $`2)`$ If I run it via Docker like this, I get the dry run to work:. ```. docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. ```. The output I get:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_paren",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:194,security,updat,updated,194,"Hi @hosseinvk,. So just a couple of things:. $`1)`$ The `--output_gvcf_merged` flag is not supported in `run_deeptrio.py`, and GLNexus should be used instead, as indicated by issue #544. I have updated the command with the `--output_gvcf_parent1` and `--output_gvcf_parent2` flags. $`2)`$ If I run it via Docker like this, I get the dry run to work:. ```. docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. ```. The output I get:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_paren",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3367,security,model,models,3367,"ts will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3393,security,model,model,3393,"ut/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3664,security,model,models,3664,"kduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3691,security,model,model,3691,""" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3962,security,model,models,3962,"ert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3989,security,model,model,3989,"termediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:26,testability,coupl,couple,26,"Hi @hosseinvk,. So just a couple of things:. $`1)`$ The `--output_gvcf_merged` flag is not supported in `run_deeptrio.py`, and GLNexus should be used instead, as indicated by issue #544. I have updated the command with the `--output_gvcf_parent1` and `--output_gvcf_parent2` flags. $`2)`$ If I run it via Docker like this, I get the dry run to work:. ```. docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. ```. The output I get:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_paren",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:91,usability,support,supported,91,"Hi @hosseinvk,. So just a couple of things:. $`1)`$ The `--output_gvcf_merged` flag is not supported in `run_deeptrio.py`, and GLNexus should be used instead, as indicated by issue #544. I have updated the command with the `--output_gvcf_parent1` and `--output_gvcf_parent2` flags. $`2)`$ If I run it via Docker like this, I get the dry run to work:. ```. docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. ```. The output I get:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_paren",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:162,usability,indicat,indicated,162,"Hi @hosseinvk,. So just a couple of things:. $`1)`$ The `--output_gvcf_merged` flag is not supported in `run_deeptrio.py`, and GLNexus should be used instead, as indicated by issue #544. I have updated the command with the `--output_gvcf_parent1` and `--output_gvcf_parent2` flags. $`2)`$ If I run it via Docker like this, I get the dry run to work:. ```. docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. ```. The output I get:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_paren",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:206,usability,command,command,206,"Hi @hosseinvk,. So just a couple of things:. $`1)`$ The `--output_gvcf_merged` flag is not supported in `run_deeptrio.py`, and GLNexus should be used instead, as indicated by issue #544. I have updated the command with the `--output_gvcf_parent1` and `--output_gvcf_parent2` flags. $`2)`$ If I run it via Docker like this, I get the dry run to work:. ```. docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. ```. The output I get:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_paren",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:2459,usability,command,command,2459,"ta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. I0726 08:57:43.023837 139824599930688 run_deeptrio.py:519] Creating a directory for intermediate results in output/intermediate_results_dir. ***** Intermediate results will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3131,usability,command,command,3131,"rent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true. I0726 08:57:43.023837 139824599930688 run_deeptrio.py:519] Creating a directory for intermediate results in output/intermediate_results_dir. ***** Intermediate results will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3424,usability,command,command,3424,"ocker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the comman",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:3722,usability,command,command,3722,"s/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:4020,usability,command,command,4020,"cord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:4422,usability,command,command,4422,"mmand:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:4827,usability,command,command,4827,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:5507,usability,help,helps,5507,"r/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""output/S_500062.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent2.tfrecord@1.gz"" --gvcf_outfile ""output/parent2.g.vcf.gz"". paul$. ```. Are you running the latest version of DeepTrio? Here's how you can find the version your are using via the `--version` flag:. ```. paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --version. DeepTrio version 1.5.0. paul$. ```. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:317,interoperability,share,shared,317,Thank you Paul. . It seems that I was using v1.1.0 and once changed it to v.1.5.0 the --dry_run command worked. There is one thing I was going to ask and appreciate you kind advice. . I have mistakenly assigned . ```. S_500061 as child. S_500062 as parent1. S_500063 as parent2. ```. all over the initial script (not shared here) whereas it should have been. ```. S_500061 as parent1. S_500062 as parent2. S_500063 as child. ```. I reckon I will need to re-run the analysis which took me few days? Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:96,usability,command,command,96,Thank you Paul. . It seems that I was using v1.1.0 and once changed it to v.1.5.0 the --dry_run command worked. There is one thing I was going to ask and appreciate you kind advice. . I have mistakenly assigned . ```. S_500061 as child. S_500062 as parent1. S_500063 as parent2. ```. all over the initial script (not shared here) whereas it should have been. ```. S_500061 as parent1. S_500062 as parent2. S_500063 as child. ```. I reckon I will need to re-run the analysis which took me few days? Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:525,deployability,configurat,configuration,525,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:603,deployability,configurat,configuration,603,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:160,energy efficiency,model,model,160,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:367,energy efficiency,model,models,367,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:721,energy efficiency,model,model,721,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:842,energy efficiency,model,model,842,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:525,integrability,configur,configuration,525,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:603,integrability,configur,configuration,603,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:572,interoperability,format,formatted,572,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:525,modifiability,configur,configuration,525,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:603,modifiability,configur,configuration,603,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:1016,performance,content,content,1016,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:160,security,model,model,160,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:367,security,model,models,367,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:525,security,configur,configuration,525,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:603,security,configur,configuration,603,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:721,security,model,model,721,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:842,security,model,model,842,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:758,usability,support,supporting,758,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:1095,usability,help,helps,1095,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:63,usability,help,help,63,Thanks for sharing the paper. I will re-run the analysis. Your help is appreciated.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/688:37,deployability,stage,stages,37,"Hi @GaianX39,. DeepVariant has three stages it progresses through: `make_examples`, `call_variants` and `postprocess_variants`. . Only the `call_variants` stage uses a GPU, as noted in the following page:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#notes-on-gpu-image. So it might be a while until you see it, which is only after the `make_examples` stage completes and the `call_variants` stage begins. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:155,deployability,stage,stage,155,"Hi @GaianX39,. DeepVariant has three stages it progresses through: `make_examples`, `call_variants` and `postprocess_variants`. . Only the `call_variants` stage uses a GPU, as noted in the following page:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#notes-on-gpu-image. So it might be a while until you see it, which is only after the `make_examples` stage completes and the `call_variants` stage begins. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:387,deployability,stage,stage,387,"Hi @GaianX39,. DeepVariant has three stages it progresses through: `make_examples`, `call_variants` and `postprocess_variants`. . Only the `call_variants` stage uses a GPU, as noted in the following page:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#notes-on-gpu-image. So it might be a while until you see it, which is only after the `make_examples` stage completes and the `call_variants` stage begins. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:427,deployability,stage,stage,427,"Hi @GaianX39,. DeepVariant has three stages it progresses through: `make_examples`, `call_variants` and `postprocess_variants`. . Only the `call_variants` stage uses a GPU, as noted in the following page:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#notes-on-gpu-image. So it might be a while until you see it, which is only after the `make_examples` stage completes and the `call_variants` stage begins. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:168,energy efficiency,GPU,GPU,168,"Hi @GaianX39,. DeepVariant has three stages it progresses through: `make_examples`, `call_variants` and `postprocess_variants`. . Only the `call_variants` stage uses a GPU, as noted in the following page:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#notes-on-gpu-image. So it might be a while until you see it, which is only after the `make_examples` stage completes and the `call_variants` stage begins. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:295,energy efficiency,gpu,gpu-image,295,"Hi @GaianX39,. DeepVariant has three stages it progresses through: `make_examples`, `call_variants` and `postprocess_variants`. . Only the `call_variants` stage uses a GPU, as noted in the following page:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#notes-on-gpu-image. So it might be a while until you see it, which is only after the `make_examples` stage completes and the `call_variants` stage begins. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:168,performance,GPU,GPU,168,"Hi @GaianX39,. DeepVariant has three stages it progresses through: `make_examples`, `call_variants` and `postprocess_variants`. . Only the `call_variants` stage uses a GPU, as noted in the following page:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#notes-on-gpu-image. So it might be a while until you see it, which is only after the `make_examples` stage completes and the `call_variants` stage begins. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:295,performance,gpu,gpu-image,295,"Hi @GaianX39,. DeepVariant has three stages it progresses through: `make_examples`, `call_variants` and `postprocess_variants`. . Only the `call_variants` stage uses a GPU, as noted in the following page:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#notes-on-gpu-image. So it might be a while until you see it, which is only after the `make_examples` stage completes and the `call_variants` stage begins. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:393,safety,compl,completes,393,"Hi @GaianX39,. DeepVariant has three stages it progresses through: `make_examples`, `call_variants` and `postprocess_variants`. . Only the `call_variants` stage uses a GPU, as noted in the following page:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#notes-on-gpu-image. So it might be a while until you see it, which is only after the `make_examples` stage completes and the `call_variants` stage begins. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:393,security,compl,completes,393,"Hi @GaianX39,. DeepVariant has three stages it progresses through: `make_examples`, `call_variants` and `postprocess_variants`. . Only the `call_variants` stage uses a GPU, as noted in the following page:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#notes-on-gpu-image. So it might be a while until you see it, which is only after the `make_examples` stage completes and the `call_variants` stage begins. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:47,usability,progress,progresses,47,"Hi @GaianX39,. DeepVariant has three stages it progresses through: `make_examples`, `call_variants` and `postprocess_variants`. . Only the `call_variants` stage uses a GPU, as noted in the following page:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#notes-on-gpu-image. So it might be a while until you see it, which is only after the `make_examples` stage completes and the `call_variants` stage begins. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:449,usability,help,helps,449,"Hi @GaianX39,. DeepVariant has three stages it progresses through: `make_examples`, `call_variants` and `postprocess_variants`. . Only the `call_variants` stage uses a GPU, as noted in the following page:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#notes-on-gpu-image. So it might be a while until you see it, which is only after the `make_examples` stage completes and the `call_variants` stage begins. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:34,usability,help,helps,34,thanks for a quick reply and that helps a lot! I will try running again,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/689:724,availability,operat,operate,724,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:89,deployability,contain,containers,89,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:556,deployability,version,version-specific,556,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:315,energy efficiency,gpu,gpu,315,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:384,energy efficiency,gpu,gpu,384,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:509,energy efficiency,model,models,509,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:684,energy efficiency,model,model,684,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:556,integrability,version,version-specific,556,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:564,interoperability,specif,specific,564,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:596,interoperability,specif,specifically,596,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:161,modifiability,layer,layers,161,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:343,modifiability,layer,layers,343,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:556,modifiability,version,version-specific,556,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:315,performance,gpu,gpu,315,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:384,performance,gpu,gpu,384,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:520,reliability,doe,does,520,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:509,security,model,models,509,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:684,security,model,model,684,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:281,testability,context,context,281,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:467,testability,context,context,467,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:765,usability,confirm,confirmed,765,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:795,usability,help,helps,795,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:618,deployability,version,version,618,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1206,deployability,releas,release,1206,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1438,deployability,releas,release,1438,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:315,energy efficiency,model,model-case-study,315,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:618,integrability,version,version,618,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:91,modifiability,Pac,PacBio,91,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:308,modifiability,pac,pacbio-model-case-study,308,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:618,modifiability,version,version,618,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:703,modifiability,pac,pacbio-case-study,703,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:576,reliability,doe,doesn,576,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:315,security,model,model-case-study,315,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1215,testability,plan,plan,1215,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:366,usability,user,users,366,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:879,usability,tool,tools,879,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1166,usability,clear,clear,1166,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1179,usability,document,documentation,1179,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1511,usability,help,help,1511,"Hi @linlin-coder ,. Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:. Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:750,availability,operat,operate,750,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:986,availability,error,error,986,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1242,availability,operat,operations,1242,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1288,availability,operat,operations,1288,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1529,availability,error,error,1529,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:97,deployability,contain,containers,97,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:582,deployability,version,version-specific,582,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:880,deployability,instal,installed,880,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:978,deployability,fail,failed,978,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1391,deployability,version,version,1391,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:335,energy efficiency,gpu,gpu,335,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:404,energy efficiency,gpu,gpu,404,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:535,energy efficiency,model,models,535,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:710,energy efficiency,model,model,710,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1055,energy efficiency,core,core,1055,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1121,energy efficiency,optim,optimized,1121,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1201,energy efficiency,CPU,CPU,1201,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:582,integrability,version,version-specific,582,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1391,integrability,version,version,1391,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:590,interoperability,specif,specific,590,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:622,interoperability,specif,specifically,622,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1060,interoperability,platform,platform,1060,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:175,modifiability,layer,layers,175,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:363,modifiability,layer,layers,363,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:582,modifiability,version,version-specific,582,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1372,modifiability,PAC,PACBIO,1372,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1391,modifiability,version,version,1391,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1473,modifiability,pac,pacbio-case-study,1473,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:335,performance,gpu,gpu,335,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:404,performance,gpu,gpu,404,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:986,performance,error,error,986,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1121,performance,optimiz,optimized,1121,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1155,performance,Network,Network,1155,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1201,performance,CPU,CPU,1201,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1221,performance,perform,performance-critical,1221,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1529,performance,error,error,1529,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:546,reliability,doe,does,546,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:978,reliability,fail,failed,978,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:986,safety,error,error,986,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1529,safety,error,error,1529,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:535,security,model,models,535,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:710,security,model,model,710,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1155,security,Network,Network,1155,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:295,testability,context,context,295,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:487,testability,context,context,487,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:791,usability,confirm,confirmed,791,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:827,usability,help,helps,827,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:986,usability,error,error,986,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1221,usability,perform,performance-critical,1221,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1529,usability,error,error,1529,"> Hi @linlin-coder,. > . > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. > . > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). > . > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). > . > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. > . > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is . ```. 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md. ``` . cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:643,deployability,version,version,643,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1248,deployability,releas,release,1248,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1480,deployability,releas,release,1480,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:334,energy efficiency,model,model-case-study,334,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:643,integrability,version,version,643,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:98,modifiability,Pac,PacBio,98,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:327,modifiability,pac,pacbio-model-case-study,327,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:643,modifiability,version,version,643,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:728,modifiability,pac,pacbio-case-study,728,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:601,reliability,doe,doesn,601,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1669,safety,detect,detection,1669,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1768,safety,detect,detection,1768,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1795,safety,accid,accidents,1795,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:334,security,model,model-case-study,334,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1669,security,detect,detection,1669,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1768,security,detect,detection,1768,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1257,testability,plan,plan,1257,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:385,usability,user,users,385,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:910,usability,tool,tools,910,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1208,usability,clear,clear,1208,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1221,usability,document,documentation,1221,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:1559,usability,help,help,1559,"> Hi @linlin-coder , Thank you for bringing up this issue. > . > I noticed that you're working on PacBio data. > . > The reason why this is happening is:. > . > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. > . > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > . > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. > . > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. > . > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. > . > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks! Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:63,deployability,updat,update,63,"Hi @linlin-coder , . Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:147,deployability,version,version,147,"Hi @linlin-coder , . Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:286,deployability,version,version,286,"Hi @linlin-coder , . Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:147,integrability,version,version,147,"Hi @linlin-coder , . Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:286,integrability,version,version,286,"Hi @linlin-coder , . Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:79,modifiability,Pac,PacBio,79,"Hi @linlin-coder , . Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:147,modifiability,version,version,147,"Hi @linlin-coder , . Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:286,modifiability,version,version,286,"Hi @linlin-coder , . Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:306,modifiability,Pac,PacBio,306,"Hi @linlin-coder , . Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:63,safety,updat,update,63,"Hi @linlin-coder , . Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:63,security,updat,update,63,"Hi @linlin-coder , . Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:116,usability,user,users,116,"Hi @linlin-coder , . Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:449,availability,mainten,maintenance,449,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:63,deployability,updat,update,63,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:147,deployability,version,version,147,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:292,deployability,version,version,292,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:438,deployability,continu,continuous,438,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:515,deployability,continu,continue,515,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:559,deployability,updat,updates,559,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:147,integrability,version,version,147,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:292,integrability,version,version,292,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:548,integrability,sub,subsequent,548,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:579,integrability,pub,publications,579,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:633,interoperability,exchang,exchanges,633,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:79,modifiability,Pac,PacBio,79,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:147,modifiability,version,version,147,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:292,modifiability,version,version,292,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:312,modifiability,Pac,PacBio,312,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:449,reliability,mainten,maintenance,449,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:63,safety,updat,update,63,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:559,safety,updat,updates,559,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:63,security,updat,update,63,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:394,security,auth,author,394,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:559,security,updat,updates,559,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:116,usability,user,users,116,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:476,usability,tool,tools,476,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:497,usability,user,user,497,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. > . > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:80,deployability,observ,observations,80,"Thanks @linlin-coder . I'll close this issue now. If you have more questions or observations, feel free to let us know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:80,testability,observ,observations,80,"Thanks @linlin-coder . I'll close this issue now. If you have more questions or observations, feel free to let us know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:28,usability,close,close,28,"Thanks @linlin-coder . I'll close this issue now. If you have more questions or observations, feel free to let us know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/690:58,energy efficiency,model,model,58,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:114,energy efficiency,model,model,114,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:154,energy efficiency,model,models,154,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:555,energy efficiency,model,model-case-study,555,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:2076,energy efficiency,model,model,2076,"or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNPs left supported by informative reads. 4. Besides requiring that you have a model for your technology, how many variants do you see in IGV that are based on high-quality reads? The reason for RefCall is because the genotype for those calls is 0/0, as per the postprocessing that happens as the last step in DeepVariant. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:208,modifiability,PAC,PACBIO,208,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:484,modifiability,Pac,PacBio,484,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:512,modifiability,Pac,PacBio,512,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:548,modifiability,pac,pacbio-model-case-study,548,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:813,modifiability,Pac,PacBio,813,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:1336,safety,input,input,1336," or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNPs left supported by informative reads. 4. Besides requiring that you have a model for your technology, how many variants do you see in IGV that are based on high-quality reads? The reason for RefCall is because the genotype for those calls is 0/0, as per the postprocessing that happens as the last step in DeepVariant. Hope it helps,. Pau",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:1822,safety,detect,detected,1822,"or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNPs left supported by informative reads. 4. Besides requiring that you have a model for your technology, how many variants do you see in IGV that are based on high-quality reads? The reason for RefCall is because the genotype for those calls is 0/0, as per the postprocessing that happens as the last step in DeepVariant. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:58,security,model,model,58,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:114,security,model,model,114,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:154,security,model,models,154,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:555,security,model,model-case-study,555,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:1822,security,detect,detected,1822,"or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNPs left supported by informative reads. 4. Besides requiring that you have a model for your technology, how many variants do you see in IGV that are based on high-quality reads? The reason for RefCall is because the genotype for those calls is 0/0, as per the postprocessing that happens as the last step in DeepVariant. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:2076,security,model,model,2076,"or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNPs left supported by informative reads. 4. Besides requiring that you have a model for your technology, how many variants do you see in IGV that are based on high-quality reads? The reason for RefCall is because the genotype for those calls is 0/0, as per the postprocessing that happens as the last step in DeepVariant. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:603,testability,Simpl,Simplex,603,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:649,testability,Simpl,Simplex,649,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:695,testability,simpl,simplex-case-study,695,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:603,usability,Simpl,Simplex,603,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:649,usability,Simpl,Simplex,649,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:695,usability,simpl,simplex-case-study,695,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:1053,usability,command,command,1053," model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNPs left supported by informative reads. 4. Besides requiri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:1114,usability,document,document,1114,"l for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a. [whole genome](docs/deepvariant-case-study.md) or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNPs left supported by informative reads. 4. Besides requiring that you have a model for your technology, how many varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:1336,usability,input,input,1336," or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNPs left supported by informative reads. 4. Besides requiring that you have a model for your technology, how many variants do you see in IGV that are based on high-quality reads? The reason for RefCall is because the genotype for those calls is 0/0, as per the postprocessing that happens as the last step in DeepVariant. Hope it helps,. Pau",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:2007,usability,support,supported,2007,"or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNPs left supported by informative reads. 4. Besides requiring that you have a model for your technology, how many variants do you see in IGV that are based on high-quality reads? The reason for RefCall is because the genotype for those calls is 0/0, as per the postprocessing that happens as the last step in DeepVariant. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:2328,usability,help,helps,2328,"or. [whole exome](docs/deepvariant-exome-case-study.md). * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina. RNA-seq. * PacBio HiFi data, see the. [PacBio case study](docs/deepvariant-pacbio-model-case-study.md). * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the. [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md). and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md). * Hybrid PacBio HiFi + Illumina WGS, see the. [hybrid case study](docs/deepvariant-hybrid-case-study.md). * Oxford Nanopore R9.4.1 data by using. [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```. INPUT_DIR=""${PWD}/YOUR_INPUT_PATH"". OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \. --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \. --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \. --output=/output/pileup --num_records=20. # And then your images are here:. ls ""${OUTPUT_DIR}""/pileup*.png. ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file. 2. The reference file used to generate the BAM is different than the one used with DeepVariant. 3. There might not be many SNPs left supported by informative reads. 4. Besides requiring that you have a model for your technology, how many variants do you see in IGV that are based on high-quality reads? The reason for RefCall is because the genotype for those calls is 0/0, as per the postprocessing that happens as the last step in DeepVariant. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:241,deployability,resourc,resources,241,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:593,deployability,releas,released,593,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:241,energy efficiency,resourc,resources,241,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:349,energy efficiency,Frequenc,Frequency,349,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:602,energy efficiency,model,models,602,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:671,energy efficiency,current,currently,671,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:837,energy efficiency,model,models,837,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:86,interoperability,standard,standards,86,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:129,interoperability,standard,standard,129,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:241,performance,resourc,resources,241,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:241,safety,resourc,resources,241,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:409,safety,detect,detect,409,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:500,safety,detect,detect,500,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:409,security,detect,detect,409,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:500,security,detect,detect,500,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:602,security,model,models,602,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:837,security,model,models,837,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:241,testability,resourc,resources,241,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:681,usability,support,support,681,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:889,usability,help,helpful,889,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants? If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:143,energy efficiency,model,model,143,"hi @pichuan,. That makes a lot of sense. . I already lowered the threshold but it seem like this is not a way to go. My plan is to retrain the model on our dataset and hopefully it performs better.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:181,performance,perform,performs,181,"hi @pichuan,. That makes a lot of sense. . I already lowered the threshold but it seem like this is not a way to go. My plan is to retrain the model on our dataset and hopefully it performs better.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:143,security,model,model,143,"hi @pichuan,. That makes a lot of sense. . I already lowered the threshold but it seem like this is not a way to go. My plan is to retrain the model on our dataset and hopefully it performs better.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:120,testability,plan,plan,120,"hi @pichuan,. That makes a lot of sense. . I already lowered the threshold but it seem like this is not a way to go. My plan is to retrain the model on our dataset and hopefully it performs better.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:181,usability,perform,performs,181,"hi @pichuan,. That makes a lot of sense. . I already lowered the threshold but it seem like this is not a way to go. My plan is to retrain the model on our dataset and hopefully it performs better.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:530,availability,consist,consistent,530,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:58,energy efficiency,frequenc,frequencies,58,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:212,energy efficiency,frequenc,frequency,212,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:469,energy efficiency,model,model,469,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:263,integrability,sub,subpopulations,263,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:417,integrability,sub,subset,417,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:42,interoperability,specif,specific,42,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:408,interoperability,specif,specific,408,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:32,safety,detect,detecting,32,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:599,safety,valid,validated,599,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:32,security,detect,detecting,32,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:469,security,model,model,469,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:599,security,validat,validated,599,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:497,testability,simul,simulated,497,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:530,usability,consist,consistent,530,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/691:354,interoperability,specif,specific-variant-in-my-data,354,"Hi @amy-houseman . For short reads, DeepVariant does a local realignment to improve our Indel accuracy. Therefore, the reads the DeepVariant is seeing might have different positions from the original BAM. Please see this section of the FAQ for more explanation: https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:48,reliability,doe,does,48,"Hi @amy-houseman . For short reads, DeepVariant does a local realignment to improve our Indel accuracy. Therefore, the reads the DeepVariant is seeing might have different positions from the original BAM. Please see this section of the FAQ for more explanation: https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:326,reliability,doe,does-deepvariant-not-call-a-specific-variant-in-my-data,326,"Hi @amy-houseman . For short reads, DeepVariant does a local realignment to improve our Indel accuracy. Therefore, the reads the DeepVariant is seeing might have different positions from the original BAM. Please see this section of the FAQ for more explanation: https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:313,availability,Down,Downsample,313,"Hi Amy,. As Pi-Chuan mentioned is good. I only see a total of 14 reads supporting at position 41,570,158 of chromosome 15 -- hopefully its the same BAM file as the one used in DeepVariant. . Now regarding IGV here are a few things:. $`1)`$ Under View > Preferences > Alignments set the following three:. Uncheck ""Downsample reads"" like this:. ![image](https://github.com/google/deepvariant/assets/6555937/4f10a4d9-a26f-432b-adef-79095c0536b2). Check ""Show mismatch bases"":. ![image](https://github.com/google/deepvariant/assets/6555937/d0543c66-0737-4a42-bf17-35e45c48ed50). Check ""Label indels > threshold"", and have a value of 0 (and uncheck Hide indels):. ![image](https://github.com/google/deepvariant/assets/6555937/c9354639-0915-470d-b073-35817549c50c). $`2)`$ Under View > Preferences > Third Gen use these settings:. Here again perform the following:. * Uncheck ""Downsample reads"". * Check ""Label indels > label threshold"", and have a value of 0 . * Uncheck ""Hide indels < indel size threshold"" . ![image](https://github.com/google/deepvariant/assets/6555937/46777d27-0098-4899-ba9b-8c6fc0e3baa6). Let me know if it helps. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:871,availability,Down,Downsample,871,"Hi Amy,. As Pi-Chuan mentioned is good. I only see a total of 14 reads supporting at position 41,570,158 of chromosome 15 -- hopefully its the same BAM file as the one used in DeepVariant. . Now regarding IGV here are a few things:. $`1)`$ Under View > Preferences > Alignments set the following three:. Uncheck ""Downsample reads"" like this:. ![image](https://github.com/google/deepvariant/assets/6555937/4f10a4d9-a26f-432b-adef-79095c0536b2). Check ""Show mismatch bases"":. ![image](https://github.com/google/deepvariant/assets/6555937/d0543c66-0737-4a42-bf17-35e45c48ed50). Check ""Label indels > threshold"", and have a value of 0 (and uncheck Hide indels):. ![image](https://github.com/google/deepvariant/assets/6555937/c9354639-0915-470d-b073-35817549c50c). $`2)`$ Under View > Preferences > Third Gen use these settings:. Here again perform the following:. * Uncheck ""Downsample reads"". * Check ""Label indels > label threshold"", and have a value of 0 . * Uncheck ""Hide indels < indel size threshold"" . ![image](https://github.com/google/deepvariant/assets/6555937/46777d27-0098-4899-ba9b-8c6fc0e3baa6). Let me know if it helps. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:456,interoperability,mismatch,mismatch,456,"Hi Amy,. As Pi-Chuan mentioned is good. I only see a total of 14 reads supporting at position 41,570,158 of chromosome 15 -- hopefully its the same BAM file as the one used in DeepVariant. . Now regarding IGV here are a few things:. $`1)`$ Under View > Preferences > Alignments set the following three:. Uncheck ""Downsample reads"" like this:. ![image](https://github.com/google/deepvariant/assets/6555937/4f10a4d9-a26f-432b-adef-79095c0536b2). Check ""Show mismatch bases"":. ![image](https://github.com/google/deepvariant/assets/6555937/d0543c66-0737-4a42-bf17-35e45c48ed50). Check ""Label indels > threshold"", and have a value of 0 (and uncheck Hide indels):. ![image](https://github.com/google/deepvariant/assets/6555937/c9354639-0915-470d-b073-35817549c50c). $`2)`$ Under View > Preferences > Third Gen use these settings:. Here again perform the following:. * Uncheck ""Downsample reads"". * Check ""Label indels > label threshold"", and have a value of 0 . * Uncheck ""Hide indels < indel size threshold"" . ![image](https://github.com/google/deepvariant/assets/6555937/46777d27-0098-4899-ba9b-8c6fc0e3baa6). Let me know if it helps. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:836,performance,perform,perform,836,"Hi Amy,. As Pi-Chuan mentioned is good. I only see a total of 14 reads supporting at position 41,570,158 of chromosome 15 -- hopefully its the same BAM file as the one used in DeepVariant. . Now regarding IGV here are a few things:. $`1)`$ Under View > Preferences > Alignments set the following three:. Uncheck ""Downsample reads"" like this:. ![image](https://github.com/google/deepvariant/assets/6555937/4f10a4d9-a26f-432b-adef-79095c0536b2). Check ""Show mismatch bases"":. ![image](https://github.com/google/deepvariant/assets/6555937/d0543c66-0737-4a42-bf17-35e45c48ed50). Check ""Label indels > threshold"", and have a value of 0 (and uncheck Hide indels):. ![image](https://github.com/google/deepvariant/assets/6555937/c9354639-0915-470d-b073-35817549c50c). $`2)`$ Under View > Preferences > Third Gen use these settings:. Here again perform the following:. * Uncheck ""Downsample reads"". * Check ""Label indels > label threshold"", and have a value of 0 . * Uncheck ""Hide indels < indel size threshold"" . ![image](https://github.com/google/deepvariant/assets/6555937/46777d27-0098-4899-ba9b-8c6fc0e3baa6). Let me know if it helps. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:71,usability,support,supporting,71,"Hi Amy,. As Pi-Chuan mentioned is good. I only see a total of 14 reads supporting at position 41,570,158 of chromosome 15 -- hopefully its the same BAM file as the one used in DeepVariant. . Now regarding IGV here are a few things:. $`1)`$ Under View > Preferences > Alignments set the following three:. Uncheck ""Downsample reads"" like this:. ![image](https://github.com/google/deepvariant/assets/6555937/4f10a4d9-a26f-432b-adef-79095c0536b2). Check ""Show mismatch bases"":. ![image](https://github.com/google/deepvariant/assets/6555937/d0543c66-0737-4a42-bf17-35e45c48ed50). Check ""Label indels > threshold"", and have a value of 0 (and uncheck Hide indels):. ![image](https://github.com/google/deepvariant/assets/6555937/c9354639-0915-470d-b073-35817549c50c). $`2)`$ Under View > Preferences > Third Gen use these settings:. Here again perform the following:. * Uncheck ""Downsample reads"". * Check ""Label indels > label threshold"", and have a value of 0 . * Uncheck ""Hide indels < indel size threshold"" . ![image](https://github.com/google/deepvariant/assets/6555937/46777d27-0098-4899-ba9b-8c6fc0e3baa6). Let me know if it helps. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:253,usability,Prefer,Preferences,253,"Hi Amy,. As Pi-Chuan mentioned is good. I only see a total of 14 reads supporting at position 41,570,158 of chromosome 15 -- hopefully its the same BAM file as the one used in DeepVariant. . Now regarding IGV here are a few things:. $`1)`$ Under View > Preferences > Alignments set the following three:. Uncheck ""Downsample reads"" like this:. ![image](https://github.com/google/deepvariant/assets/6555937/4f10a4d9-a26f-432b-adef-79095c0536b2). Check ""Show mismatch bases"":. ![image](https://github.com/google/deepvariant/assets/6555937/d0543c66-0737-4a42-bf17-35e45c48ed50). Check ""Label indels > threshold"", and have a value of 0 (and uncheck Hide indels):. ![image](https://github.com/google/deepvariant/assets/6555937/c9354639-0915-470d-b073-35817549c50c). $`2)`$ Under View > Preferences > Third Gen use these settings:. Here again perform the following:. * Uncheck ""Downsample reads"". * Check ""Label indels > label threshold"", and have a value of 0 . * Uncheck ""Hide indels < indel size threshold"" . ![image](https://github.com/google/deepvariant/assets/6555937/46777d27-0098-4899-ba9b-8c6fc0e3baa6). Let me know if it helps. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:780,usability,Prefer,Preferences,780,"Hi Amy,. As Pi-Chuan mentioned is good. I only see a total of 14 reads supporting at position 41,570,158 of chromosome 15 -- hopefully its the same BAM file as the one used in DeepVariant. . Now regarding IGV here are a few things:. $`1)`$ Under View > Preferences > Alignments set the following three:. Uncheck ""Downsample reads"" like this:. ![image](https://github.com/google/deepvariant/assets/6555937/4f10a4d9-a26f-432b-adef-79095c0536b2). Check ""Show mismatch bases"":. ![image](https://github.com/google/deepvariant/assets/6555937/d0543c66-0737-4a42-bf17-35e45c48ed50). Check ""Label indels > threshold"", and have a value of 0 (and uncheck Hide indels):. ![image](https://github.com/google/deepvariant/assets/6555937/c9354639-0915-470d-b073-35817549c50c). $`2)`$ Under View > Preferences > Third Gen use these settings:. Here again perform the following:. * Uncheck ""Downsample reads"". * Check ""Label indels > label threshold"", and have a value of 0 . * Uncheck ""Hide indels < indel size threshold"" . ![image](https://github.com/google/deepvariant/assets/6555937/46777d27-0098-4899-ba9b-8c6fc0e3baa6). Let me know if it helps. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:836,usability,perform,perform,836,"Hi Amy,. As Pi-Chuan mentioned is good. I only see a total of 14 reads supporting at position 41,570,158 of chromosome 15 -- hopefully its the same BAM file as the one used in DeepVariant. . Now regarding IGV here are a few things:. $`1)`$ Under View > Preferences > Alignments set the following three:. Uncheck ""Downsample reads"" like this:. ![image](https://github.com/google/deepvariant/assets/6555937/4f10a4d9-a26f-432b-adef-79095c0536b2). Check ""Show mismatch bases"":. ![image](https://github.com/google/deepvariant/assets/6555937/d0543c66-0737-4a42-bf17-35e45c48ed50). Check ""Label indels > threshold"", and have a value of 0 (and uncheck Hide indels):. ![image](https://github.com/google/deepvariant/assets/6555937/c9354639-0915-470d-b073-35817549c50c). $`2)`$ Under View > Preferences > Third Gen use these settings:. Here again perform the following:. * Uncheck ""Downsample reads"". * Check ""Label indels > label threshold"", and have a value of 0 . * Uncheck ""Hide indels < indel size threshold"" . ![image](https://github.com/google/deepvariant/assets/6555937/46777d27-0098-4899-ba9b-8c6fc0e3baa6). Let me know if it helps. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1124,usability,help,helps,1124,"Hi Amy,. As Pi-Chuan mentioned is good. I only see a total of 14 reads supporting at position 41,570,158 of chromosome 15 -- hopefully its the same BAM file as the one used in DeepVariant. . Now regarding IGV here are a few things:. $`1)`$ Under View > Preferences > Alignments set the following three:. Uncheck ""Downsample reads"" like this:. ![image](https://github.com/google/deepvariant/assets/6555937/4f10a4d9-a26f-432b-adef-79095c0536b2). Check ""Show mismatch bases"":. ![image](https://github.com/google/deepvariant/assets/6555937/d0543c66-0737-4a42-bf17-35e45c48ed50). Check ""Label indels > threshold"", and have a value of 0 (and uncheck Hide indels):. ![image](https://github.com/google/deepvariant/assets/6555937/c9354639-0915-470d-b073-35817549c50c). $`2)`$ Under View > Preferences > Third Gen use these settings:. Here again perform the following:. * Uncheck ""Downsample reads"". * Check ""Label indels > label threshold"", and have a value of 0 . * Uncheck ""Hide indels < indel size threshold"" . ![image](https://github.com/google/deepvariant/assets/6555937/46777d27-0098-4899-ba9b-8c6fc0e3baa6). Let me know if it helps. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:308,safety,valid,validated,308,"Hi Both, thank you so much for your replies! I really appreciate it. I adjusted the settings with your additions Paul but got the same result as I put above. Definitely the same bam and vcf sample, I just double checked. Hopefully just a realignment thing like Pi-Chuan mentioned. We may end up getting them validated just to make sure we're not throwing any potentials away! Hard choice!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:308,security,validat,validated,308,"Hi Both, thank you so much for your replies! I really appreciate it. I adjusted the settings with your additions Paul but got the same result as I put above. Definitely the same bam and vcf sample, I just double checked. Hopefully just a realignment thing like Pi-Chuan mentioned. We may end up getting them validated just to make sure we're not throwing any potentials away! Hard choice!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:357,availability,mask,mask,357,"Hmm, having no doubts is essential. Would you be comfortable with posting a few complete rows from the VCF file, which would include this variant and a few above and below. DeepVariant is base-specific, so it would tell us something that might be captured by a few rows. Also could you post the whole script and command you used to run DeepVariant? You can mask out anything that is sensitive. . Just trying to be thorough we're not missing anything. Thanks,. `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:193,interoperability,specif,specific,193,"Hmm, having no doubts is essential. Would you be comfortable with posting a few complete rows from the VCF file, which would include this variant and a few above and below. DeepVariant is base-specific, so it would tell us something that might be captured by a few rows. Also could you post the whole script and command you used to run DeepVariant? You can mask out anything that is sensitive. . Just trying to be thorough we're not missing anything. Thanks,. `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:80,safety,compl,complete,80,"Hmm, having no doubts is essential. Would you be comfortable with posting a few complete rows from the VCF file, which would include this variant and a few above and below. DeepVariant is base-specific, so it would tell us something that might be captured by a few rows. Also could you post the whole script and command you used to run DeepVariant? You can mask out anything that is sensitive. . Just trying to be thorough we're not missing anything. Thanks,. `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:80,security,compl,complete,80,"Hmm, having no doubts is essential. Would you be comfortable with posting a few complete rows from the VCF file, which would include this variant and a few above and below. DeepVariant is base-specific, so it would tell us something that might be captured by a few rows. Also could you post the whole script and command you used to run DeepVariant? You can mask out anything that is sensitive. . Just trying to be thorough we're not missing anything. Thanks,. `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:312,usability,command,command,312,"Hmm, having no doubts is essential. Would you be comfortable with posting a few complete rows from the VCF file, which would include this variant and a few above and below. DeepVariant is base-specific, so it would tell us something that might be captured by a few rows. Also could you post the whole script and command you used to run DeepVariant? You can mask out anything that is sensitive. . Just trying to be thorough we're not missing anything. Thanks,. `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1910,deployability,contain,containers,1910,"| GA | 0 | RefCall | . | GT:GQ:DP:AD:VAF:PL | 0/0:35:12:10,2:0.166667:0,35,47. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. chr15 | 41570603 | . | G | T | 10.6 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:11:31:8,23:0.741935:10,0,39. chr15 | 41573327 | . | G | T | 11.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:11:21:12,9:0.428571:10,0,43. chr15 | 41669918 | . | C | T | 0 | RefCall | . | GT:GQ:DP:AD:VAF:PL | 0/0:42:13:11,2:0.153846:0,45,43. chr15 | 41699117 | . | A | T | 49.2 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:43:96:44,52:0.541667:49,0,44. and from another sample with the same variant described also as passed:. chr15 | 41537032 | . | A | G | 38.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:38:76:42,34:0.447368:38,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. chr15 | 41561123 | . | T | G | 0.1 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:15:19:11,3:0.157895:0,15,37. chr15 | 41565105 | . | T | C | 40.6 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:40:74:56,18:0.243243:40,0,55. chr15 | 41567477 | . | G | A | 0 | RefCall | . | GT:GQ:DP:AD:VAF:PL | 0/0:21:15:13,2:0.133333:0,20,39. chr15 | 41569024 | . | T | C | 8.9 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:9:12:9,3:0.25:8,0,41. chr15 | 41570158 | . | T | C | 6.7 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:26:19,7:0.269231:5,0,47. chr15 | 41570238 | . | A | G | 0 | RefCall | . | GT:GQ:DP:AD:VAF:PL | 0/0:35:14:9,3:0.214286:0,37,38. chr15 | 41570240 | . | G | T | 0 | RefCall | . | GT:GQ:DP:AD:VAF:PL | 0/0:32:13:10,2:0.153846:0,35,34. This was WES, and part of the script used was: . ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1842,performance,parallel,parallel,1842,"| GA | 0 | RefCall | . | GT:GQ:DP:AD:VAF:PL | 0/0:35:12:10,2:0.166667:0,35,47. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. chr15 | 41570603 | . | G | T | 10.6 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:11:31:8,23:0.741935:10,0,39. chr15 | 41573327 | . | G | T | 11.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:11:21:12,9:0.428571:10,0,43. chr15 | 41669918 | . | C | T | 0 | RefCall | . | GT:GQ:DP:AD:VAF:PL | 0/0:42:13:11,2:0.153846:0,45,43. chr15 | 41699117 | . | A | T | 49.2 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:43:96:44,52:0.541667:49,0,44. and from another sample with the same variant described also as passed:. chr15 | 41537032 | . | A | G | 38.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:38:76:42,34:0.447368:38,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. chr15 | 41561123 | . | T | G | 0.1 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:15:19:11,3:0.157895:0,15,37. chr15 | 41565105 | . | T | C | 40.6 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:40:74:56,18:0.243243:40,0,55. chr15 | 41567477 | . | G | A | 0 | RefCall | . | GT:GQ:DP:AD:VAF:PL | 0/0:21:15:13,2:0.133333:0,20,39. chr15 | 41569024 | . | T | C | 8.9 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:9:12:9,3:0.25:8,0,41. chr15 | 41570158 | . | T | C | 6.7 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:26:19,7:0.269231:5,0,47. chr15 | 41570238 | . | A | G | 0 | RefCall | . | GT:GQ:DP:AD:VAF:PL | 0/0:35:14:9,3:0.214286:0,37,38. chr15 | 41570240 | . | G | T | 0 | RefCall | . | GT:GQ:DP:AD:VAF:PL | 0/0:32:13:10,2:0.153846:0,35,34. This was WES, and part of the script used was: . ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:254,interoperability,Specif,Specifically,254,"Hi @amy-houseman ,. I realize that I should also point you to this part of the FAQ: https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work which talks about how you can output the realigned BAM to examine. Specifically, if you run make_examples with `--norealign_reads` (or `--realign_reads=false`), you can turn off the realigner. We don't recommend this by default, but can be useful if you just want to double check. And, as mentioned in the FAQ section above, if you run in the region near by and add `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`, you'll be able to get a small BAM file with the realigned reads so that you can take a closer look to confirm whether it's because of the realigner or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:174,reliability,doe,does-it-work,174,"Hi @amy-houseman ,. I realize that I should also point you to this part of the FAQ: https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work which talks about how you can output the realigned BAM to examine. Specifically, if you run make_examples with `--norealign_reads` (or `--realign_reads=false`), you can turn off the realigner. We don't recommend this by default, but can be useful if you just want to double check. And, as mentioned in the FAQ section above, if you run in the region near by and add `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`, you'll be able to get a small BAM file with the realigned reads so that you can take a closer look to confirm whether it's because of the realigner or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:744,usability,close,closer,744,"Hi @amy-houseman ,. I realize that I should also point you to this part of the FAQ: https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work which talks about how you can output the realigned BAM to examine. Specifically, if you run make_examples with `--norealign_reads` (or `--realign_reads=false`), you can turn off the realigner. We don't recommend this by default, but can be useful if you just want to double check. And, as mentioned in the FAQ section above, if you run in the region near by and add `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`, you'll be able to get a small BAM file with the realigned reads so that you can take a closer look to confirm whether it's because of the realigner or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:759,usability,confirm,confirm,759,"Hi @amy-houseman ,. I realize that I should also point you to this part of the FAQ: https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work which talks about how you can output the realigned BAM to examine. Specifically, if you run make_examples with `--norealign_reads` (or `--realign_reads=false`), you can turn off the realigner. We don't recommend this by default, but can be useful if you just want to double check. And, as mentioned in the FAQ section above, if you run in the region near by and add `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`, you'll be able to get a small BAM file with the realigned reads so that you can take a closer look to confirm whether it's because of the realigner or not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:130,deployability,observ,observed,130,"Hi Amy,. Not sure if you had a chance to check, but did the realigned reads in the BAM files generated by DeepVariant confirm the observed VCF variants? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:130,testability,observ,observed,130,"Hi Amy,. Not sure if you had a chance to check, but did the realigned reads in the BAM files generated by DeepVariant confirm the observed VCF variants? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:118,usability,confirm,confirm,118,"Hi Amy,. Not sure if you had a chance to check, but did the realigned reads in the BAM files generated by DeepVariant confirm the observed VCF variants? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:20,availability,error,error,20,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:145,deployability,manag,managed,145,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:176,deployability,contain,container,176,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:250,deployability,fail,failed,250,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:397,deployability,instal,installed,397,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:511,deployability,fail,failed,511,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:658,deployability,instal,installed,658,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:145,energy efficiency,manag,managed,145,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:458,interoperability,standard,standard,458,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:20,performance,error,error,20,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:250,reliability,fail,failed,250,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:511,reliability,fail,failed,511,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:20,safety,error,error,20,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:145,safety,manag,managed,145,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:121,testability,simpl,simple,121,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:20,usability,error,error,20,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:121,usability,simpl,simple,121,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:383,usability,support,supported,383,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:644,usability,support,supported,644,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. ```. I'll let you know when they reply! Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:454,deployability,continu,continued,454,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/. ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:636,deployability,version,version,636,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/. ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:720,deployability,releas,release,720,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/. ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:864,deployability,releas,releases,864,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/. ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:934,deployability,releas,releases,934,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/. ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:636,integrability,version,version,636,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/. ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:62,interoperability,bind,binding,62,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/. ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:62,modifiability,bind,binding,62,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/. ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:636,modifiability,version,version,636,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/. ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:811,security,sign,significantly,811,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/. ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:732,testability,plan,plan,732,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/. ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:126,usability,command,command,126,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/. ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:13,availability,fault,fault,13,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1537,availability,error,error,1537,"-mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_00000140",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1569,availability,error,error,1569,"uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:2137,availability,error,error,2137,"T_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --tas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:271,deployability,log,login,271,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:519,deployability,FAIL,FAIL,519,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:672,deployability,modul,module,672,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:686,deployability,modul,module,686,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:708,deployability,modul,module,708,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1730,deployability,contain,containers,1730,"OME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:3176,deployability,fail,failed,3176,"ill be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:3323,deployability,instal,installed,3323,"unning the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_vari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:3437,deployability,fail,failed,3437,"-mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:3584,deployability,instal,installed,3584,"ratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:36.964995 47023237326656 make_examples_core.py:239] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:5743,deployability,modul,module,5743,"', 'chrY', 'chrM']. I0807 12:32:36.975915 47023237326656 make_examples_core.py:239] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-08-07 12:32:36.976266: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0807 12:32:37.440702 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:37.546969 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1630, in make_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", lin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:7489,deployability,fail,failed,7489,"_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 559, in __init__. self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 327, in __init__. self._csv_file = open(self._root_join(self.metrics_filename), 'w'). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 346, in _root_join. tf.io.gfile.makedirs(subdir). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py"", line 514, in recursive_create_dir_v2. _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path)). tensorflow.python.framework.errors_impl.PermissionDeniedError: /output; Read-only file system. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam --examples /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz --emit_realigned_reads --gvcf /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz --realigner_diagnostics /output/realigned_reads --regions chr15:41,132,484-42,007,831 --task 0. real	0m3.220s. user	0m1.999s. sys	0m0.838s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:13,energy efficiency,fault,fault,13,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:177,energy efficiency,current,current,177,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:636,energy efficiency,cpu,cpu,636,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:693,energy efficiency,load,load,693,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:715,energy efficiency,load,load,715,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:493,integrability,event,events,493,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:2397,integrability,buffer,buffer,2397,"checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:7179,integrability,sub,subdir,7179,"tqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1630, in make_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 559, in __init__. self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 327, in __init__. self._csv_file = open(self._root_join(self.metrics_filename), 'w'). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 346, in _root_join. tf.io.gfile.makedirs(subdir). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py"", line 514, in recursive_create_dir_v2. _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path)). tensorflow.python.framework.errors_impl.PermissionDeniedError: /output; Read-only file system. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam --examples /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz --emit_realigned_reads --gvcf /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz --realigner_diagnostics /output/realigned_reads --r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:3384,interoperability,standard,standard,3384,"2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:3645,interoperability,standard,standard,3645,"AGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:36.964995 47023237326656 make_examples_core.py:239] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:672,modifiability,modul,module,672,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:686,modifiability,modul,module,686,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:708,modifiability,modul,module,708,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:2157,modifiability,Interm,Intermediate,2157,"028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:4938,modifiability,deco,decode,4938,"326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:36.964995 47023237326656 make_examples_core.py:239] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0807 12:32:36.975915 47023237326656 make_examples_core.py:239] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-08-07 12:32:36.976266: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0807 12:32:37.440702 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:37.546969 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:5743,modifiability,modul,module,5743,"', 'chrY', 'chrM']. I0807 12:32:36.975915 47023237326656 make_examples_core.py:239] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-08-07 12:32:36.976266: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0807 12:32:37.440702 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:37.546969 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1630, in make_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", lin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:7224,modifiability,pac,packages,7224,"ant/make_examples_core.py"", line 1630, in make_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 559, in __init__. self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 327, in __init__. self._csv_file = open(self._root_join(self.metrics_filename), 'w'). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 346, in _root_join. tf.io.gfile.makedirs(subdir). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py"", line 514, in recursive_create_dir_v2. _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path)). tensorflow.python.framework.errors_impl.PermissionDeniedError: /output; Read-only file system. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam --examples /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz --emit_realigned_reads --gvcf /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz --realigner_diagnostics /output/realigned_reads --regions chr15:41,132,484-42,007,831 --task 0. r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:13,performance,fault,fault,13,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:201,performance,time,time,201,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:636,performance,cpu,cpu,636,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:693,performance,load,load,693,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:698,performance,parallel,parallel,698,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:715,performance,load,load,715,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1537,performance,error,error,1537,"-mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_00000140",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1569,performance,error,error,1569,"uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1662,performance,parallel,parallel,1662,"s500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:2137,performance,error,error,2137,"T_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --tas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:2354,performance,time,time,2354,"GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:2369,performance,parallel,parallel,2369,"GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:7470,performance,parallel,parallel,7470,"_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 559, in __init__. self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 327, in __init__. self._csv_file = open(self._root_join(self.metrics_filename), 'w'). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 346, in _root_join. tf.io.gfile.makedirs(subdir). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py"", line 514, in recursive_create_dir_v2. _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path)). tensorflow.python.framework.errors_impl.PermissionDeniedError: /output; Read-only file system. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam --examples /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz --emit_realigned_reads --gvcf /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz --realigner_diagnostics /output/realigned_reads --regions chr15:41,132,484-42,007,831 --task 0. real	0m3.220s. user	0m1.999s. sys	0m0.838s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:13,reliability,fault,fault,13,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:519,reliability,FAIL,FAIL,519,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:3176,reliability,fail,failed,3176,"ill be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:3437,reliability,fail,failed,3437,"-mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:6791,reliability,Diagno,DiagnosticLogger,6791,"y4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1630, in make_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 559, in __init__. self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 327, in __init__. self._csv_file = open(self._root_join(self.metrics_filename), 'w'). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 346, in _root_join. tf.io.gfile.makedirs(subdir). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py"", line 514, in recursive_create_dir_v2. _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path)). tensorflow.python.framework.errors_impl.PermissionDeniedError: /output; Read-only file system. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_G",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:6820,reliability,diagno,diagnostics,6820,"p.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1630, in make_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 559, in __init__. self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 327, in __init__. self._csv_file = open(self._root_join(self.metrics_filename), 'w'). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 346, in _root_join. tf.io.gfile.makedirs(subdir). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py"", line 514, in recursive_create_dir_v2. _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path)). tensorflow.python.framework.errors_impl.PermissionDeniedError: /output; Read-only file system. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:7489,reliability,fail,failed,7489,"_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 559, in __init__. self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 327, in __init__. self._csv_file = open(self._root_join(self.metrics_filename), 'w'). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 346, in _root_join. tf.io.gfile.makedirs(subdir). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py"", line 514, in recursive_create_dir_v2. _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path)). tensorflow.python.framework.errors_impl.PermissionDeniedError: /output; Read-only file system. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam --examples /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz --emit_realigned_reads --gvcf /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz --realigner_diagnostics /output/realigned_reads --regions chr15:41,132,484-42,007,831 --task 0. real	0m3.220s. user	0m1.999s. sys	0m0.838s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:13,safety,fault,fault,13,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:271,safety,log,login,271,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:672,safety,modul,module,672,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:686,safety,modul,module,686,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:708,safety,modul,module,708,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1537,safety,error,error,1537,"-mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_00000140",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1569,safety,error,error,1569,"uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:2137,safety,error,error,2137,"T_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --tas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:4013,safety,input,input,4013,"rmediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:36.964995 47023237326656 make_examples_core.py:239] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0807 12:32:36.975915 47023237326656 make_examples_core.py:239] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-08-07 12:32:36.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:4217,safety,input,inputs,4217,"r locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:36.964995 47023237326656 make_examples_core.py:239] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0807 12:32:36.975915 47023237326656 make_examples_core.py:239] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-08-07 12:32:36.976266: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0807 12:32:37.440702 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:4913,safety,input,input,4913," 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:36.964995 47023237326656 make_examples_core.py:239] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0807 12:32:36.975915 47023237326656 make_examples_core.py:239] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-08-07 12:32:36.976266: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0807 12:32:37.440702 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:37.546969 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfile",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:5743,safety,modul,module,5743,"', 'chrY', 'chrM']. I0807 12:32:36.975915 47023237326656 make_examples_core.py:239] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-08-07 12:32:36.976266: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0807 12:32:37.440702 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:37.546969 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1630, in make_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", lin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:7415,safety,Permiss,PermissionDeniedError,7415,"_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 559, in __init__. self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 327, in __init__. self._csv_file = open(self._root_join(self.metrics_filename), 'w'). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 346, in _root_join. tf.io.gfile.makedirs(subdir). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py"", line 514, in recursive_create_dir_v2. _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path)). tensorflow.python.framework.errors_impl.PermissionDeniedError: /output; Read-only file system. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam --examples /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz --emit_realigned_reads --gvcf /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz --realigner_diagnostics /output/realigned_reads --regions chr15:41,132,484-42,007,831 --task 0. real	0m3.220s. user	0m1.999s. sys	0m0.838s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:271,security,log,login,271,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:271,testability,log,login,271,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:5595,testability,Trace,Traceback,5595,"'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0807 12:32:36.975915 47023237326656 make_examples_core.py:239] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-08-07 12:32:36.976266: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0807 12:32:37.440702 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:37.546969 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1630, in make_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:6791,testability,Diagno,DiagnosticLogger,6791,"y4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1630, in make_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 559, in __init__. self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 327, in __init__. self._csv_file = open(self._root_join(self.metrics_filename), 'w'). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 346, in _root_join. tf.io.gfile.makedirs(subdir). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py"", line 514, in recursive_create_dir_v2. _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path)). tensorflow.python.framework.errors_impl.PermissionDeniedError: /output; Read-only file system. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_G",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:6820,testability,diagno,diagnostics,6820,"p.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1630, in make_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 559, in __init__. self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 327, in __init__. self._csv_file = open(self._root_join(self.metrics_filename), 'w'). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 346, in _root_join. tf.io.gfile.makedirs(subdir). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py"", line 514, in recursive_create_dir_v2. _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path)). tensorflow.python.framework.errors_impl.PermissionDeniedError: /output; Read-only file system. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:546,usability,user,user,546,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```. #!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p htc. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1537,usability,error,error,1537,"-mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_00000140",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1569,usability,error,error,1569,"uk # Where to send mail. #SBATCH --array=1-2. #SBATCH --mem-per-cpu=68GB. #SBATCH --qos=maxjobs500. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file. HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam. REGIONS=""chr15:41,132,484-42,007,831"". OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:2137,usability,error,error,2137,"T_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz. OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --tas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:2339,usability,command,command,2339,"/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz. INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$REGIONS \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. with the error: . ```. ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your sy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:3309,usability,support,supported,3309," ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:3570,usability,support,supported,3570,""" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:36.964995 47023237326656 make_examples_core.py:239] Common contigs are ['chr1', 'chr2', 'chr3',",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:4013,usability,input,input,4013,"rmediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:36.964995 47023237326656 make_examples_core.py:239] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0807 12:32:36.975915 47023237326656 make_examples_core.py:239] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-08-07 12:32:36.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:4217,usability,input,inputs,4217,"r locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:36.964995 47023237326656 make_examples_core.py:239] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0807 12:32:36.975915 47023237326656 make_examples_core.py:239] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-08-07 12:32:36.976266: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0807 12:32:37.440702 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:4913,usability,input,input,4913," 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs. I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:36.964995 47023237326656 make_examples_core.py:239] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0807 12:32:36.975915 47023237326656 make_examples_core.py:239] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-08-07 12:32:36.976266: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0807 12:32:37.440702 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. I0807 12:32:37.546969 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfile",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:8242,usability,user,user,8242,"_examples_runner. region_processor.initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize. self._initialize(). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize. self.realigner = realigner.Realigner(. File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 559, in __init__. self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 327, in __init__. self._csv_file = open(self._root_join(self.metrics_filename), 'w'). File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 346, in _root_join. tf.io.gfile.makedirs(subdir). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py"", line 514, in recursive_create_dir_v2. _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path)). tensorflow.python.framework.errors_impl.PermissionDeniedError: /output; Read-only file system. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam --examples /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz --emit_realigned_reads --gvcf /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz --realigner_diagnostics /output/realigned_reads --regions chr15:41,132,484-42,007,831 --task 0. real	0m3.220s. user	0m1.999s. sys	0m0.838s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:504,deployability,updat,update,504,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:756,deployability,updat,update,756,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:832,deployability,contain,container,832,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1032,deployability,contain,containers,1032,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:286,integrability,sub,sub-directory,286,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:901,performance,parallel,parallel,901,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:504,safety,updat,update,504,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:756,safety,updat,update,756,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:110,security,access,access,110,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:194,security,access,access,194,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:504,security,updat,update,504,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:756,security,updat,update,756,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:810,security,access,accessible,810,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:11,testability,simpl,simple,11,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:11,usability,simpl,simple,11,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:159,usability,command,commands,159,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1414,usability,help,helps,1414,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```. mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads. ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` . --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \. ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:404,energy efficiency,current,current,404,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:854,energy efficiency,current,current,854,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:941,energy efficiency,current,current,941,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:210,performance,time,time,210,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:577,performance,time,time,577,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:674,performance,time,time,674,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:769,performance,time,time,769,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:890,performance,perform,performed,890,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:975,performance,perform,performed,975,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:656,reliability,Doe,Does,656,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:708,reliability,doe,does,708,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:99,usability,confirm,confirm,99,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:805,usability,confirm,confirm,805,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:890,usability,perform,performed,890,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:975,usability,perform,performed,975,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1156,usability,confirm,confirm,1156,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time? Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:166,deployability,contain,containing,166,"Looking at @amy-houseman 's latest IGV in https://github.com/google/deepvariant/issues/691#issuecomment-1669163372 , it seems like the realigned BAM would see 1 read containing the alt allele C. If this is the case, I don't expect it to even trigger our candidate generation logic. (From this post, it didn't seem like @amy-houseman has specified different vsc_ thresholds). One possibility is that the realignment BAM was generated with different region compared with when the variant calling was run, like @pgrosu mentioned above.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:275,deployability,log,logic,275,"Looking at @amy-houseman 's latest IGV in https://github.com/google/deepvariant/issues/691#issuecomment-1669163372 , it seems like the realigned BAM would see 1 read containing the alt allele C. If this is the case, I don't expect it to even trigger our candidate generation logic. (From this post, it didn't seem like @amy-houseman has specified different vsc_ thresholds). One possibility is that the realignment BAM was generated with different region compared with when the variant calling was run, like @pgrosu mentioned above.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:337,interoperability,specif,specified,337,"Looking at @amy-houseman 's latest IGV in https://github.com/google/deepvariant/issues/691#issuecomment-1669163372 , it seems like the realigned BAM would see 1 read containing the alt allele C. If this is the case, I don't expect it to even trigger our candidate generation logic. (From this post, it didn't seem like @amy-houseman has specified different vsc_ thresholds). One possibility is that the realignment BAM was generated with different region compared with when the variant calling was run, like @pgrosu mentioned above.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:275,safety,log,logic,275,"Looking at @amy-houseman 's latest IGV in https://github.com/google/deepvariant/issues/691#issuecomment-1669163372 , it seems like the realigned BAM would see 1 read containing the alt allele C. If this is the case, I don't expect it to even trigger our candidate generation logic. (From this post, it didn't seem like @amy-houseman has specified different vsc_ thresholds). One possibility is that the realignment BAM was generated with different region compared with when the variant calling was run, like @pgrosu mentioned above.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:275,security,log,logic,275,"Looking at @amy-houseman 's latest IGV in https://github.com/google/deepvariant/issues/691#issuecomment-1669163372 , it seems like the realigned BAM would see 1 read containing the alt allele C. If this is the case, I don't expect it to even trigger our candidate generation logic. (From this post, it didn't seem like @amy-houseman has specified different vsc_ thresholds). One possibility is that the realignment BAM was generated with different region compared with when the variant calling was run, like @pgrosu mentioned above.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:275,testability,log,logic,275,"Looking at @amy-houseman 's latest IGV in https://github.com/google/deepvariant/issues/691#issuecomment-1669163372 , it seems like the realigned BAM would see 1 read containing the alt allele C. If this is the case, I don't expect it to even trigger our candidate generation logic. (From this post, it didn't seem like @amy-houseman has specified different vsc_ thresholds). One possibility is that the realignment BAM was generated with different region compared with when the variant calling was run, like @pgrosu mentioned above.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:884,performance,time,time,884,"Hi both,. I reran the entire bed file both with and without the `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \ ` because I think the regions I used before it didn't like so just decided to do the entire file. Both of the entries for that variant are the same:. Without realign:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. With realign:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. When looking at the realigned bam's it actually has the realignment directory ""chr15:41570025-41570158"" so using that bam with the original, I get - with the C's actually being identified this time:. ![Image 08-08-2023 at 21 42](https://github.com/google/deepvariant/assets/110385188/cc45d8ba-cdc4-4e5e-8dd8-de1a12bc4dba). Sorry for the confusion! .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:868,security,ident,identified,868,"Hi both,. I reran the entire bed file both with and without the `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \ ` because I think the regions I used before it didn't like so just decided to do the entire file. Both of the entries for that variant are the same:. Without realign:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. With realign:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. When looking at the realigned bam's it actually has the realignment directory ""chr15:41570025-41570158"" so using that bam with the original, I get - with the C's actually being identified this time:. ![Image 08-08-2023 at 21 42](https://github.com/google/deepvariant/assets/110385188/cc45d8ba-cdc4-4e5e-8dd8-de1a12bc4dba). Sorry for the confusion! .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1175,energy efficiency,load,loaded,1175,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:414,modifiability,paramet,parameter,414,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:546,modifiability,paramet,parameters,546,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:791,modifiability,variab,variable,791,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:854,modifiability,variab,variable,854,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1175,performance,load,loaded,1175,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:66,usability,confirm,confirms,66,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1127,usability,satisfa,satisfactory,1127,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:1250,usability,help,helps,1250,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:48,usability,help,helpful,48,"Thank you both so much, you have been more than helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:87,integrability,topic,topic,87,"Hi again, sorry to be a pain! I thought I would comment under this one again since the topic is the same. I have a variant that is called in my VCF file:. ![image](https://github.com/google/deepvariant/assets/110385188/d4604a11-59bc-4468-9d54-ffa1ede430d4). However I cannot find it in the bams nor prior to realignment or after realignment (I tried with region specific and then using the entire bed region). . ![Image 21-11-2023 at 12 30](https://github.com/google/deepvariant/assets/110385188/034c745e-2562-42e0-b073-c1855e27e5d9). Top is the VCF file. Middle is the realignment bam file. Bottom is the bam file before deepvariant. . I am not sure where to go from here, thanks! Amy.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:362,interoperability,specif,specific,362,"Hi again, sorry to be a pain! I thought I would comment under this one again since the topic is the same. I have a variant that is called in my VCF file:. ![image](https://github.com/google/deepvariant/assets/110385188/d4604a11-59bc-4468-9d54-ffa1ede430d4). However I cannot find it in the bams nor prior to realignment or after realignment (I tried with region specific and then using the entire bed region). . ![Image 21-11-2023 at 12 30](https://github.com/google/deepvariant/assets/110385188/034c745e-2562-42e0-b073-c1855e27e5d9). Top is the VCF file. Middle is the realignment bam file. Bottom is the bam file before deepvariant. . I am not sure where to go from here, thanks! Amy.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:73,interoperability,share,share,73,"Hi @amy-houseman . Thank you for following up. Is it possible for you to share a small BAM that can reproduce this IGV that you're looking at? If so, I can take a look and see if I can reproduce the issue and try to understand what's going on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:216,testability,understand,understand,216,"Hi @amy-houseman . Thank you for following up. Is it possible for you to share a small BAM that can reproduce this IGV that you're looking at? If so, I can take a look and see if I can reproduce the issue and try to understand what's going on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:42,security,sensitive data,sensitive data,42,"@amy-houseman Sure. Please don't send any sensitive data, and try to restrict to just a small region that we can reproduce. You can email to pichuan@google.com.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:129,interoperability,share,shared,129,"Expanding on my answer in https://github.com/google/deepvariant/issues/691#issuecomment-1662968609. I'll post my commands that I shared with @amy-houseman in email, in case it's useful for other users in the future. I ran this on a small BAM file and BED file that @amy-houseman shared. I'll call them ""YOUR.bam"" and ""YOUR.bed"" below. ```bash. BIN_VERSION=""1.6.0"". docker run \. -v ${PWD}:/data \. -v ${PWD}/reference:/reference \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" \. --reads ""/data/YOUR.bam"" \. --output_vcf=/data/output.vcf.gz \. --num_shards=1 \. --regions ""/data/YOUR.bed"" \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/data/realigned_reads"". ```. Specifically, this step is the one that we need to look at:. ```bash. time docker run \. -v ${PWD}:/data \. -v ${PWD}/reference:/reference \. google/deepvariant:1.6.0 \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" \. --reads ""/data/YOUR.bam"" \. --examples ""/data/output.tfrecord.gz"" \. --channels ""insert_size"" \. --regions ""/data/YOUR.bed"". ```. In this case, I have a few positions that I was interested in looking at. To look at the positions, the output directories are in:. ```bash. $ ls -lh realigned_reads/chr15:41562546-41562720. total 8.0K. -rw-r--r-- 1 root root 5.7K Mar 10 06:14 realigned_reads.bam. $ ls -lh realigned_reads/chr15:41570025-41570158. total 8.0K. -rw-r--r-- 1 root root 5.3K Mar 10 06:14 realigned_reads.bam. ```. Sort and index them:. ```bash. RANGE=41562546-41562720. samtools sort realigned_reads/chr15:${RANGE}/realigned_reads.bam > ${RANGE}.sorted.bam. samtools index ${RANGE}.sorted.bam. RANGE=41570025-41570158. samtools sort realigned_reads/chr15:${RANGE}/realigned_reads.bam > ${RANGE}.sorted.bam. samtools index ${RANGE}.sorted.bam. ```. Output:. ```. $ ls -lh *sorted.bam. -rw-rw-r-- 1 pichuan d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:279,interoperability,share,shared,279,"Expanding on my answer in https://github.com/google/deepvariant/issues/691#issuecomment-1662968609. I'll post my commands that I shared with @amy-houseman in email, in case it's useful for other users in the future. I ran this on a small BAM file and BED file that @amy-houseman shared. I'll call them ""YOUR.bam"" and ""YOUR.bed"" below. ```bash. BIN_VERSION=""1.6.0"". docker run \. -v ${PWD}:/data \. -v ${PWD}/reference:/reference \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" \. --reads ""/data/YOUR.bam"" \. --output_vcf=/data/output.vcf.gz \. --num_shards=1 \. --regions ""/data/YOUR.bed"" \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/data/realigned_reads"". ```. Specifically, this step is the one that we need to look at:. ```bash. time docker run \. -v ${PWD}:/data \. -v ${PWD}/reference:/reference \. google/deepvariant:1.6.0 \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" \. --reads ""/data/YOUR.bam"" \. --examples ""/data/output.tfrecord.gz"" \. --channels ""insert_size"" \. --regions ""/data/YOUR.bed"". ```. In this case, I have a few positions that I was interested in looking at. To look at the positions, the output directories are in:. ```bash. $ ls -lh realigned_reads/chr15:41562546-41562720. total 8.0K. -rw-r--r-- 1 root root 5.7K Mar 10 06:14 realigned_reads.bam. $ ls -lh realigned_reads/chr15:41570025-41570158. total 8.0K. -rw-r--r-- 1 root root 5.3K Mar 10 06:14 realigned_reads.bam. ```. Sort and index them:. ```bash. RANGE=41562546-41562720. samtools sort realigned_reads/chr15:${RANGE}/realigned_reads.bam > ${RANGE}.sorted.bam. samtools index ${RANGE}.sorted.bam. RANGE=41570025-41570158. samtools sort realigned_reads/chr15:${RANGE}/realigned_reads.bam > ${RANGE}.sorted.bam. samtools index ${RANGE}.sorted.bam. ```. Output:. ```. $ ls -lh *sorted.bam. -rw-rw-r-- 1 pichuan d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:803,interoperability,Specif,Specifically,803,"Expanding on my answer in https://github.com/google/deepvariant/issues/691#issuecomment-1662968609. I'll post my commands that I shared with @amy-houseman in email, in case it's useful for other users in the future. I ran this on a small BAM file and BED file that @amy-houseman shared. I'll call them ""YOUR.bam"" and ""YOUR.bed"" below. ```bash. BIN_VERSION=""1.6.0"". docker run \. -v ${PWD}:/data \. -v ${PWD}/reference:/reference \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" \. --reads ""/data/YOUR.bam"" \. --output_vcf=/data/output.vcf.gz \. --num_shards=1 \. --regions ""/data/YOUR.bed"" \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/data/realigned_reads"". ```. Specifically, this step is the one that we need to look at:. ```bash. time docker run \. -v ${PWD}:/data \. -v ${PWD}/reference:/reference \. google/deepvariant:1.6.0 \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" \. --reads ""/data/YOUR.bam"" \. --examples ""/data/output.tfrecord.gz"" \. --channels ""insert_size"" \. --regions ""/data/YOUR.bed"". ```. In this case, I have a few positions that I was interested in looking at. To look at the positions, the output directories are in:. ```bash. $ ls -lh realigned_reads/chr15:41562546-41562720. total 8.0K. -rw-r--r-- 1 root root 5.7K Mar 10 06:14 realigned_reads.bam. $ ls -lh realigned_reads/chr15:41570025-41570158. total 8.0K. -rw-r--r-- 1 root root 5.3K Mar 10 06:14 realigned_reads.bam. ```. Sort and index them:. ```bash. RANGE=41562546-41562720. samtools sort realigned_reads/chr15:${RANGE}/realigned_reads.bam > ${RANGE}.sorted.bam. samtools index ${RANGE}.sorted.bam. RANGE=41570025-41570158. samtools sort realigned_reads/chr15:${RANGE}/realigned_reads.bam > ${RANGE}.sorted.bam. samtools index ${RANGE}.sorted.bam. ```. Output:. ```. $ ls -lh *sorted.bam. -rw-rw-r-- 1 pichuan d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:873,performance,time,time,873,"Expanding on my answer in https://github.com/google/deepvariant/issues/691#issuecomment-1662968609. I'll post my commands that I shared with @amy-houseman in email, in case it's useful for other users in the future. I ran this on a small BAM file and BED file that @amy-houseman shared. I'll call them ""YOUR.bam"" and ""YOUR.bed"" below. ```bash. BIN_VERSION=""1.6.0"". docker run \. -v ${PWD}:/data \. -v ${PWD}/reference:/reference \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" \. --reads ""/data/YOUR.bam"" \. --output_vcf=/data/output.vcf.gz \. --num_shards=1 \. --regions ""/data/YOUR.bed"" \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/data/realigned_reads"". ```. Specifically, this step is the one that we need to look at:. ```bash. time docker run \. -v ${PWD}:/data \. -v ${PWD}/reference:/reference \. google/deepvariant:1.6.0 \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" \. --reads ""/data/YOUR.bam"" \. --examples ""/data/output.tfrecord.gz"" \. --channels ""insert_size"" \. --regions ""/data/YOUR.bed"". ```. In this case, I have a few positions that I was interested in looking at. To look at the positions, the output directories are in:. ```bash. $ ls -lh realigned_reads/chr15:41562546-41562720. total 8.0K. -rw-r--r-- 1 root root 5.7K Mar 10 06:14 realigned_reads.bam. $ ls -lh realigned_reads/chr15:41570025-41570158. total 8.0K. -rw-r--r-- 1 root root 5.3K Mar 10 06:14 realigned_reads.bam. ```. Sort and index them:. ```bash. RANGE=41562546-41562720. samtools sort realigned_reads/chr15:${RANGE}/realigned_reads.bam > ${RANGE}.sorted.bam. samtools index ${RANGE}.sorted.bam. RANGE=41570025-41570158. samtools sort realigned_reads/chr15:${RANGE}/realigned_reads.bam > ${RANGE}.sorted.bam. samtools index ${RANGE}.sorted.bam. ```. Output:. ```. $ ls -lh *sorted.bam. -rw-rw-r-- 1 pichuan d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:113,usability,command,commands,113,"Expanding on my answer in https://github.com/google/deepvariant/issues/691#issuecomment-1662968609. I'll post my commands that I shared with @amy-houseman in email, in case it's useful for other users in the future. I ran this on a small BAM file and BED file that @amy-houseman shared. I'll call them ""YOUR.bam"" and ""YOUR.bed"" below. ```bash. BIN_VERSION=""1.6.0"". docker run \. -v ${PWD}:/data \. -v ${PWD}/reference:/reference \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" \. --reads ""/data/YOUR.bam"" \. --output_vcf=/data/output.vcf.gz \. --num_shards=1 \. --regions ""/data/YOUR.bed"" \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/data/realigned_reads"". ```. Specifically, this step is the one that we need to look at:. ```bash. time docker run \. -v ${PWD}:/data \. -v ${PWD}/reference:/reference \. google/deepvariant:1.6.0 \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" \. --reads ""/data/YOUR.bam"" \. --examples ""/data/output.tfrecord.gz"" \. --channels ""insert_size"" \. --regions ""/data/YOUR.bed"". ```. In this case, I have a few positions that I was interested in looking at. To look at the positions, the output directories are in:. ```bash. $ ls -lh realigned_reads/chr15:41562546-41562720. total 8.0K. -rw-r--r-- 1 root root 5.7K Mar 10 06:14 realigned_reads.bam. $ ls -lh realigned_reads/chr15:41570025-41570158. total 8.0K. -rw-r--r-- 1 root root 5.3K Mar 10 06:14 realigned_reads.bam. ```. Sort and index them:. ```bash. RANGE=41562546-41562720. samtools sort realigned_reads/chr15:${RANGE}/realigned_reads.bam > ${RANGE}.sorted.bam. samtools index ${RANGE}.sorted.bam. RANGE=41570025-41570158. samtools sort realigned_reads/chr15:${RANGE}/realigned_reads.bam > ${RANGE}.sorted.bam. samtools index ${RANGE}.sorted.bam. ```. Output:. ```. $ ls -lh *sorted.bam. -rw-rw-r-- 1 pichuan d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:195,usability,user,users,195,"Expanding on my answer in https://github.com/google/deepvariant/issues/691#issuecomment-1662968609. I'll post my commands that I shared with @amy-houseman in email, in case it's useful for other users in the future. I ran this on a small BAM file and BED file that @amy-houseman shared. I'll call them ""YOUR.bam"" and ""YOUR.bed"" below. ```bash. BIN_VERSION=""1.6.0"". docker run \. -v ${PWD}:/data \. -v ${PWD}/reference:/reference \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" \. --reads ""/data/YOUR.bam"" \. --output_vcf=/data/output.vcf.gz \. --num_shards=1 \. --regions ""/data/YOUR.bed"" \. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/data/realigned_reads"". ```. Specifically, this step is the one that we need to look at:. ```bash. time docker run \. -v ${PWD}:/data \. -v ${PWD}/reference:/reference \. google/deepvariant:1.6.0 \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" \. --reads ""/data/YOUR.bam"" \. --examples ""/data/output.tfrecord.gz"" \. --channels ""insert_size"" \. --regions ""/data/YOUR.bed"". ```. In this case, I have a few positions that I was interested in looking at. To look at the positions, the output directories are in:. ```bash. $ ls -lh realigned_reads/chr15:41562546-41562720. total 8.0K. -rw-r--r-- 1 root root 5.7K Mar 10 06:14 realigned_reads.bam. $ ls -lh realigned_reads/chr15:41570025-41570158. total 8.0K. -rw-r--r-- 1 root root 5.3K Mar 10 06:14 realigned_reads.bam. ```. Sort and index them:. ```bash. RANGE=41562546-41562720. samtools sort realigned_reads/chr15:${RANGE}/realigned_reads.bam > ${RANGE}.sorted.bam. samtools index ${RANGE}.sorted.bam. RANGE=41570025-41570158. samtools sort realigned_reads/chr15:${RANGE}/realigned_reads.bam > ${RANGE}.sorted.bam. samtools index ${RANGE}.sorted.bam. ```. Output:. ```. $ ls -lh *sorted.bam. -rw-rw-r-- 1 pichuan d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/692:35,deployability,releas,release,35,"Hi Kiran,. We don't have a general release for this. If you are interested, you can email awcarroll@google.com and we can follow up on options.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/692
https://github.com/google/deepvariant/issues/693:167,deployability,manag,management,167,"Hi @AldhairMedico,. Good catch and makes sense, but probably it's easier to use [`gff2bed` from bedops](https://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/gff2bed.html) and select the columns from there. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/693:167,energy efficiency,manag,management,167,"Hi @AldhairMedico,. Good catch and makes sense, but probably it's easier to use [`gff2bed` from bedops](https://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/gff2bed.html) and select the columns from there. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/693:178,interoperability,convers,conversion,178,"Hi @AldhairMedico,. Good catch and makes sense, but probably it's easier to use [`gff2bed` from bedops](https://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/gff2bed.html) and select the columns from there. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/693:144,performance,content,content,144,"Hi @AldhairMedico,. Good catch and makes sense, but probably it's easier to use [`gff2bed` from bedops](https://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/gff2bed.html) and select the columns from there. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/693:167,safety,manag,management,167,"Hi @AldhairMedico,. Good catch and makes sense, but probably it's easier to use [`gff2bed` from bedops](https://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/gff2bed.html) and select the columns from there. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/693:246,usability,help,helps,246,"Hi @AldhairMedico,. Good catch and makes sense, but probably it's easier to use [`gff2bed` from bedops](https://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/gff2bed.html) and select the columns from there. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/695:891,energy efficiency,optim,optimal,891,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:487,integrability,sub,suboptimal,487,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:877,integrability,sub,suboptimal,877,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:47,modifiability,deco,decoy,47,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:468,modifiability,deco,decoy,468,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1283,modifiability,deco,decoys,1283,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1459,modifiability,deco,decoy-aligned,1459,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1612,modifiability,deco,decoy,1612,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1789,modifiability,deco,decoy,1789,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1662,reliability,doe,does,1662,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1004,safety,test,test,1004,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1013,safety,valid,validation,1013,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1013,security,validat,validation,1013,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1194,security,ident,identical,1194,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:695,testability,coverag,coverage,695,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1004,testability,test,test,1004,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1564,usability,confirm,confirm,1564,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1880,usability,help,helps,1880,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:55,modifiability,deco,decoy,55,"Hi @Npaffen . DeepVariant won't by default call on the decoy contigs. That is behaving as expected. If you really need to call with the decoy contigs, you would have to change their name so that they don't match the exclude list. I am not sure what you mean by the calls are caught on chrY. The reads that map to a decoy should not be used. How is this manifesting for you, is there a variant call on chrY which is supported ONLY by reads that come from a decoy. Is there an IGV screenshot or a small BAM snippet that might illustrate the effect? Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:136,modifiability,deco,decoy,136,"Hi @Npaffen . DeepVariant won't by default call on the decoy contigs. That is behaving as expected. If you really need to call with the decoy contigs, you would have to change their name so that they don't match the exclude list. I am not sure what you mean by the calls are caught on chrY. The reads that map to a decoy should not be used. How is this manifesting for you, is there a variant call on chrY which is supported ONLY by reads that come from a decoy. Is there an IGV screenshot or a small BAM snippet that might illustrate the effect? Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:315,modifiability,deco,decoy,315,"Hi @Npaffen . DeepVariant won't by default call on the decoy contigs. That is behaving as expected. If you really need to call with the decoy contigs, you would have to change their name so that they don't match the exclude list. I am not sure what you mean by the calls are caught on chrY. The reads that map to a decoy should not be used. How is this manifesting for you, is there a variant call on chrY which is supported ONLY by reads that come from a decoy. Is there an IGV screenshot or a small BAM snippet that might illustrate the effect? Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:456,modifiability,deco,decoy,456,"Hi @Npaffen . DeepVariant won't by default call on the decoy contigs. That is behaving as expected. If you really need to call with the decoy contigs, you would have to change their name so that they don't match the exclude list. I am not sure what you mean by the calls are caught on chrY. The reads that map to a decoy should not be used. How is this manifesting for you, is there a variant call on chrY which is supported ONLY by reads that come from a decoy. Is there an IGV screenshot or a small BAM snippet that might illustrate the effect? Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:78,usability,behavi,behaving,78,"Hi @Npaffen . DeepVariant won't by default call on the decoy contigs. That is behaving as expected. If you really need to call with the decoy contigs, you would have to change their name so that they don't match the exclude list. I am not sure what you mean by the calls are caught on chrY. The reads that map to a decoy should not be used. How is this manifesting for you, is there a variant call on chrY which is supported ONLY by reads that come from a decoy. Is there an IGV screenshot or a small BAM snippet that might illustrate the effect? Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:415,usability,support,supported,415,"Hi @Npaffen . DeepVariant won't by default call on the decoy contigs. That is behaving as expected. If you really need to call with the decoy contigs, you would have to change their name so that they don't match the exclude list. I am not sure what you mean by the calls are caught on chrY. The reads that map to a decoy should not be used. How is this manifesting for you, is there a variant call on chrY which is supported ONLY by reads that come from a decoy. Is there an IGV screenshot or a small BAM snippet that might illustrate the effect? Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/696:1266,availability,down,download,1266,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:39,deployability,resourc,resources,39,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:57,deployability,depend,depends,57,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:216,deployability,version,version,216,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:265,deployability,version,versions,265,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:307,deployability,Version,Version,307,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:504,deployability,Version,Version,504,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:919,deployability,resourc,resources,919,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1467,deployability,depend,depends,1467,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1684,deployability,contain,containers,1684,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:39,energy efficiency,resourc,resources,39,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:173,energy efficiency,GPU,GPU,173,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:212,energy efficiency,CPU,CPU,212,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:303,energy efficiency,CPU,CPU,303,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:495,energy efficiency,gpu,gpu,495,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:500,energy efficiency,GPU,GPU,500,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:569,energy efficiency,gpu,gpu,569,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:737,energy efficiency,core,cores,737,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:919,energy efficiency,resourc,resources,919,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:57,integrability,depend,depends,57,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:216,integrability,version,version,216,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:265,integrability,version,versions,265,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:307,integrability,Version,Version,307,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:504,integrability,Version,Version,504,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1467,integrability,depend,depends,1467,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1716,integrability,coupl,couple,1716,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1758,integrability,repositor,repository,1758,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1758,interoperability,repositor,repository,1758,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:57,modifiability,depend,depends,57,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:216,modifiability,version,version,216,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:265,modifiability,version,versions,265,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:307,modifiability,Version,Version,307,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:340,modifiability,layer,layers,340,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:504,modifiability,Version,Version,504,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:537,modifiability,layer,layers,537,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1467,modifiability,depend,depends,1467,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1519,modifiability,Pac,PacBio,1519,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1584,modifiability,Pac,PacificBiosciences,1584,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1716,modifiability,coupl,couple,1716,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:39,performance,resourc,resources,39,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:173,performance,GPU,GPU,173,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:212,performance,CPU,CPU,212,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:303,performance,CPU,CPU,303,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:495,performance,gpu,gpu,495,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:500,performance,GPU,GPU,500,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:569,performance,gpu,gpu,569,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:919,performance,resourc,resources,919,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:39,safety,resourc,resources,39,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:57,safety,depend,depends,57,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:919,safety,resourc,resources,919,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1467,safety,depend,depends,1467,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:39,testability,resourc,resources,39,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:57,testability,depend,depends,57,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:451,testability,context,context,451,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:652,testability,context,context,652,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:919,testability,resourc,resources,919,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1467,testability,depend,depends,1467,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1716,testability,coupl,couple,1716,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:184,usability,prefer,prefer,184,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:1992,usability,help,helps,1992,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:96,energy efficiency,CPU,CPU,96,"Thanks for your promptly reply,. I want to ask what's the difference between docker image using CPU vs one using GPU. Is it just the difference in time processing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:113,energy efficiency,GPU,GPU,113,"Thanks for your promptly reply,. I want to ask what's the difference between docker image using CPU vs one using GPU. Is it just the difference in time processing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:96,performance,CPU,CPU,96,"Thanks for your promptly reply,. I want to ask what's the difference between docker image using CPU vs one using GPU. Is it just the difference in time processing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:113,performance,GPU,GPU,113,"Thanks for your promptly reply,. I want to ask what's the difference between docker image using CPU vs one using GPU. Is it just the difference in time processing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:147,performance,time,time,147,"Thanks for your promptly reply,. I want to ask what's the difference between docker image using CPU vs one using GPU. Is it just the difference in time processing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:303,availability,operat,operate,303,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:141,deployability,contain,containing,141,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:519,deployability,stage,stage,519,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:556,deployability,stage,stages,556,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:864,deployability,depend,depending,864,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:882,deployability,stage,stage,882,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:11,energy efficiency,CPU,CPU,11,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:16,energy efficiency,core,core,16,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:110,energy efficiency,GPU,GPU,110,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:198,energy efficiency,CPU,CPU,198,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:214,energy efficiency,core,cores,214,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:253,energy efficiency,GPU,GPU,253,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:295,energy efficiency,GPU,GPU,295,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:464,energy efficiency,GPU,GPU,464,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:500,energy efficiency,GPU,GPU,500,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:664,energy efficiency,CPU,CPU-based,664,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:864,integrability,depend,depending,864,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:864,modifiability,depend,depending,864,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:11,performance,CPU,CPU,11,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:110,performance,GPU,GPU,110,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:198,performance,CPU,CPU,198,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:253,performance,GPU,GPU,253,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:295,performance,GPU,GPU,295,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:360,performance,parallel,parallel,360,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:464,performance,GPU,GPU,464,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:500,performance,GPU,GPU,500,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:664,performance,CPU,CPU-based,664,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:857,performance,memor,memory,857,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:83,safety,input,input,83,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:864,safety,depend,depending,864,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:864,testability,depend,depending,864,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:83,usability,input,input,83,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:857,usability,memor,memory,857,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:101,energy efficiency,GPU,GPU,101,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:139,energy efficiency,GPU,GPU,139,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:101,performance,GPU,GPU,101,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:139,performance,GPU,GPU,139,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:192,performance,perform,performance,192,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:143,reliability,doe,does,143,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:37,usability,clear,clear,37,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:192,usability,perform,performance,192,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:331,deployability,stage,stage,331,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:442,deployability,stage,stages,442,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:682,deployability,stage,stage,682,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:776,deployability,log,logarithmic,776,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:889,deployability,stage,stage,889,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:23,energy efficiency,Current,Currently,23,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:90,energy efficiency,GPU,GPUs,90,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:111,energy efficiency,optim,optimizations,111,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:281,energy efficiency,GPU,GPUs,281,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:542,energy efficiency,GPU,GPU-based,542,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:604,energy efficiency,CPU,CPU,604,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:816,energy efficiency,optim,optimizations,816,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:438,integrability,sub,sub-stages,438,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:649,integrability,transform,transforming,649,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:750,integrability,sub,subsequently,750,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:227,interoperability,specif,specific,227,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:609,interoperability,Distribut,Distributing,609,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:649,interoperability,transform,transforming,649,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:90,performance,GPU,GPUs,90,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:111,performance,optimiz,optimizations,111,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:160,performance,time,time,160,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:281,performance,GPU,GPUs,281,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:542,performance,GPU,GPU-based,542,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:604,performance,CPU,CPU,604,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:816,performance,optimiz,optimizations,816,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:972,performance,time,time,972,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:776,safety,log,logarithmic,776,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:776,security,log,logarithmic,776,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:776,testability,log,logarithmic,776,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:8,usability,help,helped,8,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:209,availability,sli,slightly,209,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:320,availability,failur,failure,320,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:245,deployability,scale,scale,245,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:320,deployability,fail,failure,320,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:465,deployability,depend,depends,465,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:245,energy efficiency,scale,scale,245,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:385,energy efficiency,GPU,GPU,385,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:450,energy efficiency,optim,optimal,450,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:465,integrability,depend,depends,465,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:245,modifiability,scal,scale,245,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:291,modifiability,Pac,PacBio,291,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:465,modifiability,depend,depends,465,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:245,performance,scale,scale,245,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:320,performance,failur,failure,320,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:385,performance,GPU,GPU,385,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:209,reliability,sli,slightly,209,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:320,reliability,fail,failure,320,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:465,safety,depend,depends,465,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:465,testability,depend,depends,465,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:130,usability,effectiv,effective,130,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:251,usability,close,close,251,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:307,usability,experien,experience,307,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/697:311,availability,down,down-sampling,311,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:490,availability,down,down-sample,490,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1294,deployability,releas,release,1294,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:93,energy efficiency,measur,measure,93,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:700,energy efficiency,model,model,700,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:887,energy efficiency,model,model,887,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:700,security,model,model,700,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:887,security,model,model,887,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1249,testability,trace,trace,1249,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:60,usability,indicat,indicates,60,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:782,usability,indicat,indicating,782,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1114,usability,support,supporting,1114,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1174,usability,help,help,1174,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1182,usability,confirm,confirm,1182,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1618,usability,help,helps,1618,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1334,availability,down,down-sample,1334,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data? Very sorry to bother you, wish you a pleasant work and life,. Ji.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:401,integrability,topic,topic,401,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data? Very sorry to bother you, wish you a pleasant work and life,. Ji.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:423,performance,time,times,423,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data? Very sorry to bother you, wish you a pleasant work and life,. Ji.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:326,safety,except,excepted,326,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data? Very sorry to bother you, wish you a pleasant work and life,. Ji.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:499,safety,detect,detect,499,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data? Very sorry to bother you, wish you a pleasant work and life,. Ji.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1477,safety,detect,detect,1477,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data? Very sorry to bother you, wish you a pleasant work and life,. Ji.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1532,safety,detect,detect,1532,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data? Very sorry to bother you, wish you a pleasant work and life,. Ji.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:499,security,detect,detect,499,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data? Very sorry to bother you, wish you a pleasant work and life,. Ji.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1477,security,detect,detect,1477,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data? Very sorry to bother you, wish you a pleasant work and life,. Ji.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1532,security,detect,detect,1532,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data? Very sorry to bother you, wish you a pleasant work and life,. Ji.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:132,usability,tip,tips,132,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data? Very sorry to bother you, wish you a pleasant work and life,. Ji.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:227,usability,command,command,227,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data? Very sorry to bother you, wish you a pleasant work and life,. Ji.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1235,usability,tip,tip,1235,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data? Very sorry to bother you, wish you a pleasant work and life,. Ji.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:812,availability,state,statement,812,"Hi Ji,. Glad it was helpful, and no bother at all -- this is fun for me :) Now there are a few things going on here to better understand what DeepVariant might be seeing:. $`1)`$ The first thing is to get a BAM of your realigned reads for that region. To do that, just add the following to your script (below `--num_shards=1`):. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1595,availability,error,error,1595," comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the follo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3623,availability,operat,operates,3623,"om/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. Y",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1680,deployability,contain,contains,1680,"ake sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3206,deployability,build,build,3206,"the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3292,deployability,build,building,3292,"dels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:2932,energy efficiency,model,model,2932,"t selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy dat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3240,energy efficiency,predict,prediction,3240,"n_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3251,energy efficiency,model,model,3251,"he default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Rega",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3617,energy efficiency,model,model,3617,"ithub.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimiza",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3632,energy efficiency,optim,optimally,3632,"deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can loo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3702,energy efficiency,model,model,3702,"ecific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3828,energy efficiency,predict,prediction,3828,"$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very compl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4529,energy efficiency,predict,prediction,4529," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4612,energy efficiency,optim,optimization,4612," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4713,energy efficiency,model,model,4713," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:812,integrability,state,statement,812,"Hi Ji,. Glad it was helpful, and no bother at all -- this is fun for me :) Now there are a few things going on here to better understand what DeepVariant might be seeing:. $`1)`$ The first thing is to get a BAM of your realigned reads for that region. To do that, just add the following to your script (below `--num_shards=1`):. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1412,interoperability,specif,specific,1412,"t/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1983,interoperability,specif,specific,1983,"Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:2703,interoperability,specif,specific-variant-in-my-data,2703,"e multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3024,interoperability,specif,specify,3024,"this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4199,interoperability,specif,specific,4199," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:2112,modifiability,paramet,parameters,2112,"w the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from y",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:2575,modifiability,paramet,parameters,2575,"ariant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4050,modifiability,paramet,parameters,4050," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4086,modifiability,paramet,parameters,4086," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4208,modifiability,paramet,parameters,4208," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4592,modifiability,paramet,parameter,4592," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1595,performance,error,error,1595," comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the follo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4612,performance,optimiz,optimization,4612," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:564,reliability,doe,does-it-work,564,"Hi Ji,. Glad it was helpful, and no bother at all -- this is fun for me :) Now there are a few things going on here to better understand what DeepVariant might be seeing:. $`1)`$ The first thing is to get a BAM of your realigned reads for that region. To do that, just add the following to your script (below `--num_shards=1`):. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:2675,reliability,doe,does-deepvariant-not-call-a-specific-variant-in-my-data,2675,"so if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1595,safety,error,error,1595," comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the follo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3240,safety,predict,prediction,3240,"n_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3828,safety,predict,prediction,3828,"$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very compl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4529,safety,predict,prediction,4529," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4828,safety,compl,complex,4828," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4869,safety,valid,validation,4869," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:2932,security,model,model,2932,"t selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy dat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3251,security,model,model,3251,"he default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Rega",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3617,security,model,model,3617,"ithub.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimiza",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3702,security,model,model,3702,"ecific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4713,security,model,model,4713," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4828,security,compl,complex,4828," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4869,security,validat,validation,4869," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:126,testability,understand,understand,126,"Hi Ji,. Glad it was helpful, and no bother at all -- this is fun for me :) Now there are a few things going on here to better understand what DeepVariant might be seeing:. $`1)`$ The first thing is to get a BAM of your realigned reads for that region. To do that, just add the following to your script (below `--num_shards=1`):. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4471,testability,simul,simulate,4471," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:20,usability,help,helpful,20,"Hi Ji,. Glad it was helpful, and no bother at all -- this is fun for me :) Now there are a few things going on here to better understand what DeepVariant might be seeing:. $`1)`$ The first thing is to get a BAM of your realigned reads for that region. To do that, just add the following to your script (below `--num_shards=1`):. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1358,usability,support,supporting,1358,"mit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the defa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1512,usability,support,support,1512,"/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fractio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1595,usability,error,error,1595," comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the follo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1648,usability,help,help,1648,"es/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1809,usability,support,support,1809,"tatement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:2131,usability,prefer,preference,2131," read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2). * vsc_min_count_indels (the default is 2). * vsc_min_fraction_snps (the default is 0.12). * vsc_min_fraction_indels (the default is 0.06). * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3372,usability,support,supports,3372,"Here is an example:. ```. --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'. ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:3747,usability,support,supporting,3747,"oogle/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:4889,usability,help,helps,4889," and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:122,availability,error,errors,122,"@observer2735 Can you say more about what you are trying to do? Is it germline variant calling? Or maybe counting all the errors in the reads? Please elaborate, thank you! Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:122,performance,error,errors,122,"@observer2735 Can you say more about what you are trying to do? Is it germline variant calling? Or maybe counting all the errors in the reads? Please elaborate, thank you! Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:122,safety,error,errors,122,"@observer2735 Can you say more about what you are trying to do? Is it germline variant calling? Or maybe counting all the errors in the reads? Please elaborate, thank you! Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:122,usability,error,errors,122,"@observer2735 Can you say more about what you are trying to do? Is it germline variant calling? Or maybe counting all the errors in the reads? Please elaborate, thank you! Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:138,availability,state,statement,138,"Hi @observer2735 . I will second Maria's question about the application you are trying to do. I did want to mention one things about your statement. ```. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. ```. The VCF for DeepVariant does not report every single variant present in the reads, it reports the candidate positions which pass the minimum support (as @pgrosu mentioned). So you should see that the other possible variants in the read don't make the threshold required to generate a support. DeepVariant is operating from an assumption of a diploid organism, if you want to try to do things like detect subclonal variants at lower allele frequencies, that would be good to know, as there would be different recommendations we would make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:833,availability,operat,operating,833,"Hi @observer2735 . I will second Maria's question about the application you are trying to do. I did want to mention one things about your statement. ```. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. ```. The VCF for DeepVariant does not report every single variant present in the reads, it reports the candidate positions which pass the minimum support (as @pgrosu mentioned). So you should see that the other possible variants in the read don't make the threshold required to generate a support. DeepVariant is operating from an assumption of a diploid organism, if you want to try to do things like detect subclonal variants at lower allele frequencies, that would be good to know, as there would be different recommendations we would make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:964,energy efficiency,frequenc,frequencies,964,"Hi @observer2735 . I will second Maria's question about the application you are trying to do. I did want to mention one things about your statement. ```. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. ```. The VCF for DeepVariant does not report every single variant present in the reads, it reports the candidate positions which pass the minimum support (as @pgrosu mentioned). So you should see that the other possible variants in the read don't make the threshold required to generate a support. DeepVariant is operating from an assumption of a diploid organism, if you want to try to do things like detect subclonal variants at lower allele frequencies, that would be good to know, as there would be different recommendations we would make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:138,integrability,state,statement,138,"Hi @observer2735 . I will second Maria's question about the application you are trying to do. I did want to mention one things about your statement. ```. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. ```. The VCF for DeepVariant does not report every single variant present in the reads, it reports the candidate positions which pass the minimum support (as @pgrosu mentioned). So you should see that the other possible variants in the read don't make the threshold required to generate a support. DeepVariant is operating from an assumption of a diploid organism, if you want to try to do things like detect subclonal variants at lower allele frequencies, that would be good to know, as there would be different recommendations we would make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:929,integrability,sub,subclonal,929,"Hi @observer2735 . I will second Maria's question about the application you are trying to do. I did want to mention one things about your statement. ```. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. ```. The VCF for DeepVariant does not report every single variant present in the reads, it reports the candidate positions which pass the minimum support (as @pgrosu mentioned). So you should see that the other possible variants in the read don't make the threshold required to generate a support. DeepVariant is operating from an assumption of a diploid organism, if you want to try to do things like detect subclonal variants at lower allele frequencies, that would be good to know, as there would be different recommendations we would make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:549,reliability,doe,does,549,"Hi @observer2735 . I will second Maria's question about the application you are trying to do. I did want to mention one things about your statement. ```. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. ```. The VCF for DeepVariant does not report every single variant present in the reads, it reports the candidate positions which pass the minimum support (as @pgrosu mentioned). So you should see that the other possible variants in the read don't make the threshold required to generate a support. DeepVariant is operating from an assumption of a diploid organism, if you want to try to do things like detect subclonal variants at lower allele frequencies, that would be good to know, as there would be different recommendations we would make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:922,safety,detect,detect,922,"Hi @observer2735 . I will second Maria's question about the application you are trying to do. I did want to mention one things about your statement. ```. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. ```. The VCF for DeepVariant does not report every single variant present in the reads, it reports the candidate positions which pass the minimum support (as @pgrosu mentioned). So you should see that the other possible variants in the read don't make the threshold required to generate a support. DeepVariant is operating from an assumption of a diploid organism, if you want to try to do things like detect subclonal variants at lower allele frequencies, that would be good to know, as there would be different recommendations we would make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:922,security,detect,detect,922,"Hi @observer2735 . I will second Maria's question about the application you are trying to do. I did want to mention one things about your statement. ```. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. ```. The VCF for DeepVariant does not report every single variant present in the reads, it reports the candidate positions which pass the minimum support (as @pgrosu mentioned). So you should see that the other possible variants in the read don't make the threshold required to generate a support. DeepVariant is operating from an assumption of a diploid organism, if you want to try to do things like detect subclonal variants at lower allele frequencies, that would be good to know, as there would be different recommendations we would make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:658,usability,minim,minimum,658,"Hi @observer2735 . I will second Maria's question about the application you are trying to do. I did want to mention one things about your statement. ```. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. ```. The VCF for DeepVariant does not report every single variant present in the reads, it reports the candidate positions which pass the minimum support (as @pgrosu mentioned). So you should see that the other possible variants in the read don't make the threshold required to generate a support. DeepVariant is operating from an assumption of a diploid organism, if you want to try to do things like detect subclonal variants at lower allele frequencies, that would be good to know, as there would be different recommendations we would make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:666,usability,support,support,666,"Hi @observer2735 . I will second Maria's question about the application you are trying to do. I did want to mention one things about your statement. ```. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. ```. The VCF for DeepVariant does not report every single variant present in the reads, it reports the candidate positions which pass the minimum support (as @pgrosu mentioned). So you should see that the other possible variants in the read don't make the threshold required to generate a support. DeepVariant is operating from an assumption of a diploid organism, if you want to try to do things like detect subclonal variants at lower allele frequencies, that would be good to know, as there would be different recommendations we would make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:809,usability,support,support,809,"Hi @observer2735 . I will second Maria's question about the application you are trying to do. I did want to mention one things about your statement. ```. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. ```. The VCF for DeepVariant does not report every single variant present in the reads, it reports the candidate positions which pass the minimum support (as @pgrosu mentioned). So you should see that the other possible variants in the read don't make the threshold required to generate a support. DeepVariant is operating from an assumption of a diploid organism, if you want to try to do things like detect subclonal variants at lower allele frequencies, that would be good to know, as there would be different recommendations we would make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:771,availability,sli,slip,771,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:171,deployability,observ,observed,171,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:998,deployability,continu,continuous,998,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1278,integrability,topic,topic,1278,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1192,interoperability,bind,binding,1192,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1192,modifiability,bind,binding,1192,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:743,performance,time,times,743,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1393,performance,time,times,1393,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:771,reliability,sli,slip,771,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1074,security,sign,signal,1074,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1175,security,ident,identifying,1175,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:171,testability,observ,observed,171,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:297,usability,feedback,feedback,297,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1144,usability,efficien,efficiency,1144,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1694,usability,help,help,1694,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,. it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,. and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:217,availability,error,errors,217,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1183,availability,operat,operates,1183,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:140,energy efficiency,optim,optimal,140,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1242,energy efficiency,profil,profiling,1242,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:217,performance,error,errors,217,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1091,performance,time,time,1091,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1242,performance,profil,profiling,1242,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:217,safety,error,errors,217,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:798,safety,valid,validate,798,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:798,security,validat,validate,798,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1430,security,sign,signal,1430,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:10,testability,understand,understand,10,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:217,usability,error,errors,217,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:375,usability,Tool,Tool,375,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:460,usability,help,help,460,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:623,usability,tool,tool,623,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,. Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/). [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:309,deployability,log,logically,309,"Dear Paul,. Thank you for you suggestions about STR analysis! . Please do not apologize, as you have indeed helped me a lot and saved me a lot of time. Thank you for providing very professional guidance on Deepvariant. I am still unable to reach a conclusion on whether deepvariant can be used in some way on logically paired samples, so I will take a look at the HipSTR and deep repeat you provided, which may be helpful for my research. Thank you very much! Wich you have a nice day! Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:146,performance,time,time,146,"Dear Paul,. Thank you for you suggestions about STR analysis! . Please do not apologize, as you have indeed helped me a lot and saved me a lot of time. Thank you for providing very professional guidance on Deepvariant. I am still unable to reach a conclusion on whether deepvariant can be used in some way on logically paired samples, so I will take a look at the HipSTR and deep repeat you provided, which may be helpful for my research. Thank you very much! Wich you have a nice day! Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:309,safety,log,logically,309,"Dear Paul,. Thank you for you suggestions about STR analysis! . Please do not apologize, as you have indeed helped me a lot and saved me a lot of time. Thank you for providing very professional guidance on Deepvariant. I am still unable to reach a conclusion on whether deepvariant can be used in some way on logically paired samples, so I will take a look at the HipSTR and deep repeat you provided, which may be helpful for my research. Thank you very much! Wich you have a nice day! Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:309,security,log,logically,309,"Dear Paul,. Thank you for you suggestions about STR analysis! . Please do not apologize, as you have indeed helped me a lot and saved me a lot of time. Thank you for providing very professional guidance on Deepvariant. I am still unable to reach a conclusion on whether deepvariant can be used in some way on logically paired samples, so I will take a look at the HipSTR and deep repeat you provided, which may be helpful for my research. Thank you very much! Wich you have a nice day! Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:309,testability,log,logically,309,"Dear Paul,. Thank you for you suggestions about STR analysis! . Please do not apologize, as you have indeed helped me a lot and saved me a lot of time. Thank you for providing very professional guidance on Deepvariant. I am still unable to reach a conclusion on whether deepvariant can be used in some way on logically paired samples, so I will take a look at the HipSTR and deep repeat you provided, which may be helpful for my research. Thank you very much! Wich you have a nice day! Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:108,usability,help,helped,108,"Dear Paul,. Thank you for you suggestions about STR analysis! . Please do not apologize, as you have indeed helped me a lot and saved me a lot of time. Thank you for providing very professional guidance on Deepvariant. I am still unable to reach a conclusion on whether deepvariant can be used in some way on logically paired samples, so I will take a look at the HipSTR and deep repeat you provided, which may be helpful for my research. Thank you very much! Wich you have a nice day! Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:194,usability,guidanc,guidance,194,"Dear Paul,. Thank you for you suggestions about STR analysis! . Please do not apologize, as you have indeed helped me a lot and saved me a lot of time. Thank you for providing very professional guidance on Deepvariant. I am still unable to reach a conclusion on whether deepvariant can be used in some way on logically paired samples, so I will take a look at the HipSTR and deep repeat you provided, which may be helpful for my research. Thank you very much! Wich you have a nice day! Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:414,usability,help,helpful,414,"Dear Paul,. Thank you for you suggestions about STR analysis! . Please do not apologize, as you have indeed helped me a lot and saved me a lot of time. Thank you for providing very professional guidance on Deepvariant. I am still unable to reach a conclusion on whether deepvariant can be used in some way on logically paired samples, so I will take a look at the HipSTR and deep repeat you provided, which may be helpful for my research. Thank you very much! Wich you have a nice day! Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:194,deployability,updat,updated,194,"Dear Ji,. Glad it was helpful -- research can be fun like that during its discovery phases in opening up surprising doors :) This definitly got me thinking, which I am also thankful for. I have updated my previous post with the references to the papers on the two programs. Hope you have a wonderful day as well! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:74,integrability,discover,discovery,74,"Dear Ji,. Glad it was helpful -- research can be fun like that during its discovery phases in opening up surprising doors :) This definitly got me thinking, which I am also thankful for. I have updated my previous post with the references to the papers on the two programs. Hope you have a wonderful day as well! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:74,interoperability,discover,discovery,74,"Dear Ji,. Glad it was helpful -- research can be fun like that during its discovery phases in opening up surprising doors :) This definitly got me thinking, which I am also thankful for. I have updated my previous post with the references to the papers on the two programs. Hope you have a wonderful day as well! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:194,safety,updat,updated,194,"Dear Ji,. Glad it was helpful -- research can be fun like that during its discovery phases in opening up surprising doors :) This definitly got me thinking, which I am also thankful for. I have updated my previous post with the references to the papers on the two programs. Hope you have a wonderful day as well! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:194,security,updat,updated,194,"Dear Ji,. Glad it was helpful -- research can be fun like that during its discovery phases in opening up surprising doors :) This definitly got me thinking, which I am also thankful for. I have updated my previous post with the references to the papers on the two programs. Hope you have a wonderful day as well! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:22,usability,help,helpful,22,"Dear Ji,. Glad it was helpful -- research can be fun like that during its discovery phases in opening up surprising doors :) This definitly got me thinking, which I am also thankful for. I have updated my previous post with the references to the papers on the two programs. Hope you have a wonderful day as well! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:74,usability,discov,discovery,74,"Dear Ji,. Glad it was helpful -- research can be fun like that during its discovery phases in opening up surprising doors :) This definitly got me thinking, which I am also thankful for. I have updated my previous post with the references to the papers on the two programs. Hope you have a wonderful day as well! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:7,usability,close,close,7,I will close this issue @observer2735. If there are any further questions please reopen. Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/698:220,deployability,automat,automatically,220,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1148,deployability,continu,continue,1148,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:104,energy efficiency,model,model,104,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:383,energy efficiency,model,model,383,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1138,energy efficiency,model,model,1138,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1458,energy efficiency,model,model,1458,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:562,integrability,sub,subsequently,562,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1417,integrability,batch,batch,1417,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1195,interoperability,specif,specified,1195,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:785,performance,content,content,785,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1417,performance,batch,batch,1417,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:629,safety,valid,validation,629,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:925,safety,valid,validated,925,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1592,safety,compl,complete,1592,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:104,security,model,model,104,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:383,security,model,model,383,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:629,security,validat,validation,629,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:925,security,validat,validated,925,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1138,security,model,model,1138,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1458,security,model,model,1458,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1592,security,compl,complete,1592,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:220,testability,automat,automatically,220,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:249,usability,support,supporting,249,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:302,usability,confirm,confirm,302,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:589,usability,support,supporting,589,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1749,usability,learn,learning,1749,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1758,usability,experien,experience,1758,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1778,usability,help,helps,1778,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:43,energy efficiency,model,models,43,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:258,energy efficiency,model,models,258,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:273,energy efficiency,current,currently,273,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:488,interoperability,specif,specific,488,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:81,performance,tune,tune,81,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:328,performance,tune,tune,328,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:392,performance,tune,tune,392,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:125,safety,test,test,125,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:397,safety,test,test,397,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:43,security,model,models,43,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:258,security,model,models,258,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:125,testability,test,test,125,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:397,testability,test,test,397,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:156,safety,except,except,156,"HI,. I want to ask about shuffling. @akolesnikov has a good explanation on shuffling on this issue (#312). My training has multiple samples (HG001 to HG007 except HG003), so I don't know if shuffling means shuffling within samples or within all samples. Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1368,availability,down,downsampling,1368,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:82,energy efficiency,model,model,82,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:240,energy efficiency,power,power,240,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:343,energy efficiency,model,model,343,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:663,energy efficiency,model,model,663,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1295,energy efficiency,model,model,1295,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:281,integrability,batch,batching,281,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:314,integrability,batch,batch,314,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:366,integrability,batch,batches,366,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:536,integrability,batch,batch,536,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:636,integrability,batch,batch,636,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:804,integrability,batch,batch,804,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:671,modifiability,paramet,parameters,671,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:837,modifiability,paramet,parameters,837,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:894,modifiability,variab,variable,894,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1311,modifiability,scenario,scenario,1311,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1474,modifiability,paramet,parameter,1474,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:250,performance,memor,memory,250,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:281,performance,batch,batching,281,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:314,performance,batch,batch,314,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:366,performance,batch,batches,366,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:536,performance,batch,batch,536,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:636,performance,batch,batch,636,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:804,performance,batch,batch,804,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:971,safety,compl,complex,971,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1114,safety,valid,validation,1114,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1227,safety,except,exception,1227,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1244,safety,test,test,1244,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:82,security,model,model,82,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:343,security,model,model,343,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:663,security,model,model,663,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:971,security,compl,complex,971,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1114,security,validat,validation,1114,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1295,security,model,model,1295,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:922,testability,simpl,simple,922,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1244,testability,test,test,1244,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:250,usability,memor,memory,250,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:384,usability,visual,visual,384,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:922,usability,simpl,simple,922,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:1611,usability,help,helps,1611,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:16,usability,help,helps,16,"Thank you, that helps a lot.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/699:592,performance,perform,perform,592,". Hi Sophie,. You have a few of options:. 1) The first option is like Andrew mentioned [in a previous post](https://github.com/google/deepvariant/issues/377#issuecomment-720908040), by running DeepVariant on each of them and use GLnexus to merge them and identify the de novo mutations from the joint call file. 2) DeepTrio outputs the child VCF as noted by the flag `--output_vcf_child`. Then you would need to compare those variants across multiple samples (with some truth sets) against the parents, to ensure they are truly DNM and are not false positives. That is quite a bit of work to perform properly. 3) You can use things external tools, of which there are many :). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:255,security,ident,identify,255,". Hi Sophie,. You have a few of options:. 1) The first option is like Andrew mentioned [in a previous post](https://github.com/google/deepvariant/issues/377#issuecomment-720908040), by running DeepVariant on each of them and use GLnexus to merge them and identify the de novo mutations from the joint call file. 2) DeepTrio outputs the child VCF as noted by the flag `--output_vcf_child`. Then you would need to compare those variants across multiple samples (with some truth sets) against the parents, to ensure they are truly DNM and are not false positives. That is quite a bit of work to perform properly. 3) You can use things external tools, of which there are many :). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:592,usability,perform,perform,592,". Hi Sophie,. You have a few of options:. 1) The first option is like Andrew mentioned [in a previous post](https://github.com/google/deepvariant/issues/377#issuecomment-720908040), by running DeepVariant on each of them and use GLnexus to merge them and identify the de novo mutations from the joint call file. 2) DeepTrio outputs the child VCF as noted by the flag `--output_vcf_child`. Then you would need to compare those variants across multiple samples (with some truth sets) against the parents, to ensure they are truly DNM and are not false positives. That is quite a bit of work to perform properly. 3) You can use things external tools, of which there are many :). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:641,usability,tool,tools,641,". Hi Sophie,. You have a few of options:. 1) The first option is like Andrew mentioned [in a previous post](https://github.com/google/deepvariant/issues/377#issuecomment-720908040), by running DeepVariant on each of them and use GLnexus to merge them and identify the de novo mutations from the joint call file. 2) DeepTrio outputs the child VCF as noted by the flag `--output_vcf_child`. Then you would need to compare those variants across multiple samples (with some truth sets) against the parents, to ensure they are truly DNM and are not false positives. That is quite a bit of work to perform properly. 3) You can use things external tools, of which there are many :). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:684,usability,help,helps,684,". Hi Sophie,. You have a few of options:. 1) The first option is like Andrew mentioned [in a previous post](https://github.com/google/deepvariant/issues/377#issuecomment-720908040), by running DeepVariant on each of them and use GLnexus to merge them and identify the de novo mutations from the joint call file. 2) DeepTrio outputs the child VCF as noted by the flag `--output_vcf_child`. Then you would need to compare those variants across multiple samples (with some truth sets) against the parents, to ensure they are truly DNM and are not false positives. That is quite a bit of work to perform properly. 3) You can use things external tools, of which there are many :). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:7,usability,close,close,7,I will close this issue now. If you have further questions feel free to reopen. Thank you @pgrosu.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:203,modifiability,pac,pacbio-case-study,203,"HI @pgrosu ,. For option1, after merging into a joint call file, which tool can I use to call de novo variants? According to this [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md), RTG mendelian is only able to calculate non-mendelian rate, not identify de novo mutations. For option3, did you mean to use other tool after DeepTrio producing vcf for each sample? I have done a bit of research and there are not that many and most of the tools are old and not well-maintained. Can you list some of the recommendations here? Thanks,. Sophie",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:509,modifiability,maintain,maintained,509,"HI @pgrosu ,. For option1, after merging into a joint call file, which tool can I use to call de novo variants? According to this [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md), RTG mendelian is only able to calculate non-mendelian rate, not identify de novo mutations. For option3, did you mean to use other tool after DeepTrio producing vcf for each sample? I have done a bit of research and there are not that many and most of the tools are old and not well-maintained. Can you list some of the recommendations here? Thanks,. Sophie",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:509,safety,maintain,maintained,509,"HI @pgrosu ,. For option1, after merging into a joint call file, which tool can I use to call de novo variants? According to this [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md), RTG mendelian is only able to calculate non-mendelian rate, not identify de novo mutations. For option3, did you mean to use other tool after DeepTrio producing vcf for each sample? I have done a bit of research and there are not that many and most of the tools are old and not well-maintained. Can you list some of the recommendations here? Thanks,. Sophie",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:290,security,ident,identify,290,"HI @pgrosu ,. For option1, after merging into a joint call file, which tool can I use to call de novo variants? According to this [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md), RTG mendelian is only able to calculate non-mendelian rate, not identify de novo mutations. For option3, did you mean to use other tool after DeepTrio producing vcf for each sample? I have done a bit of research and there are not that many and most of the tools are old and not well-maintained. Can you list some of the recommendations here? Thanks,. Sophie",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:71,usability,tool,tool,71,"HI @pgrosu ,. For option1, after merging into a joint call file, which tool can I use to call de novo variants? According to this [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md), RTG mendelian is only able to calculate non-mendelian rate, not identify de novo mutations. For option3, did you mean to use other tool after DeepTrio producing vcf for each sample? I have done a bit of research and there are not that many and most of the tools are old and not well-maintained. Can you list some of the recommendations here? Thanks,. Sophie",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:357,usability,tool,tool,357,"HI @pgrosu ,. For option1, after merging into a joint call file, which tool can I use to call de novo variants? According to this [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md), RTG mendelian is only able to calculate non-mendelian rate, not identify de novo mutations. For option3, did you mean to use other tool after DeepTrio producing vcf for each sample? I have done a bit of research and there are not that many and most of the tools are old and not well-maintained. Can you list some of the recommendations here? Thanks,. Sophie",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:482,usability,tool,tools,482,"HI @pgrosu ,. For option1, after merging into a joint call file, which tool can I use to call de novo variants? According to this [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md), RTG mendelian is only able to calculate non-mendelian rate, not identify de novo mutations. For option3, did you mean to use other tool after DeepTrio producing vcf for each sample? I have done a bit of research and there are not that many and most of the tools are old and not well-maintained. Can you list some of the recommendations here? Thanks,. Sophie",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:845,availability,consist,consistency,845,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:3625,availability,operat,operate,3625,"ovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5078,availability,slo,slow,5078,"Q:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:402,deployability,updat,updated,402,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:557,deployability,updat,updated,557,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:2158,deployability,version,version,2158,"expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)). * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html). * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:3717,deployability,Configurat,Configuration,3717," not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5478,deployability,instal,installer,5478,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5488,deployability,resourc,resources,5488,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5575,deployability,pipelin,pipeline,5575,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5670,deployability,log,login,5670,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:670,energy efficiency,Frequenc,Frequency,670,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:2325,energy efficiency,model,models,2325,"other son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)). * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html). * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5488,energy efficiency,resourc,resources,5488,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:1301,integrability,FILTER,FILTER,1301,"lian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:2158,integrability,version,version,2158,"expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)). * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html). * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:3717,integrability,Configur,Configuration,3717," not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5575,integrability,pipelin,pipeline,5575,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:6090,integrability,pub,pubmed,6090,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:6207,integrability,discover,discovery,6207,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:6238,integrability,pub,pubmed,6238,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:891,interoperability,FORMAT,FORMAT,891,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:953,interoperability,FORMAT,FORMAT,953,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:1021,interoperability,FORMAT,FORMAT,1021,"ow, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on ped",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:1313,interoperability,FORMAT,FORMAT,1313,"ance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/doc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:3640,interoperability,specif,specific,3640,"//github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:6207,interoperability,discover,discovery,6207,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:309,modifiability,inherit,inheritance,309,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:460,modifiability,inherit,inheritance,460,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:759,modifiability,inherit,inheritance,759,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:2158,modifiability,version,version,2158,"expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)). * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html). * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:3649,modifiability,paramet,parameters,3649,"om/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:3717,modifiability,Configur,Configuration,3717," not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:3954,modifiability,inherit,inheritance,3954,"at is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:1932,performance,perform,perform,1932,"n=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)). * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html). * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:1989,performance,perform,perform,1989,"iption=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)). * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html). * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:3745,performance,perform,perform,3745,"ns, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. Fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5488,performance,resourc,resources,5488,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5958,performance,content,content,5958,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:1143,reliability,doe,does,1143,"ovel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5078,reliability,slo,slow,5078,"Q:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:402,safety,updat,updated,402,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:557,safety,updat,updated,557,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:4875,safety,valid,validation,4875,"e novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5488,safety,resourc,resources,5488,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5670,safety,log,login,5670,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:402,security,updat,updated,402,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:557,security,updat,updated,557,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:2325,security,model,models,2325,"other son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)). * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html). * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:3717,security,Configur,Configuration,3717," not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:4875,security,validat,validation,4875,"e novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5670,security,log,login,5670,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:4637,testability,coverag,coverage,4637,"pecific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatic",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5488,testability,resourc,resources,5488,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5670,testability,log,login,5670,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5782,testability,Unit,Units,5782,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:343,usability,tool,tools,343,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:845,usability,consist,consistency,845,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:857,usability,statu,status,857,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```. ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">. ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">. ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">. ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:1912,usability,tool,tools,1912,"ype=String,Description=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)). * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html). * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. Y",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:1932,usability,perform,perform,1932,"n=""Genotype"">. ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)). * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html). * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:1989,usability,perform,perform,1989,"iption=""De novo allele"">. ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)). * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html). * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:2318,usability,guid,guides,2318,"ather mother son1 son2 daughter1 daughter2-initial daughter2. Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)). * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html). * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:2598,usability,workflow,workflow-overview,2598,"0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y. Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y. ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio). * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)). * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html). * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:3600,usability,indicat,indicating,3600,"w-overview.html). * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:3745,usability,perform,perform,3745,"ns, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. Fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:4517,usability,support,supporting,4517,"ant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```. chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:.. ```. Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManua",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5348,usability,tool,tools,5348,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5366,usability,help,helps,5366,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5406,usability,Tool,Tools,5406,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5460,usability,tool,tools,5460,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5498,usability,tool,tools,5498,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:5924,usability,Learn,Learning,5924,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:6207,usability,discov,discovery,6207,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,. Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf). [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false). [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880). [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_. [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:51,security,modif,modified,51,"Thank you @pgrosu, I totally omitted the output of modified vcf from rtg mendelian. This is exactly what I am looking for",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/699:57,testability,understand,understandable,57,"Hi Sophie,. You are very welcome, and that is absolutely understandable. Feel free to reach out again if you have more questions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/700:217,security,privil,privileges,217,"Are you running the docker image with `sudo docker run` or just `docker run`? There is a difference sometimes of where it looks for images. For example, you listed them with `sudo` but are your also running with sudo-privileges? Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:238,deployability,instal,install,238,"If you type `sudo docker run 45f6c7767ff0`, does that also not launch it? Also you can add your username to the Docker group so you don't need sudo, as shown here -- but do that after we solve your issue:. [https://docs.docker.com/engine/install/linux-postinstall/](https://docs.docker.com/engine/install/linux-postinstall/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:297,deployability,instal,install,297,"If you type `sudo docker run 45f6c7767ff0`, does that also not launch it? Also you can add your username to the Docker group so you don't need sudo, as shown here -- but do that after we solve your issue:. [https://docs.docker.com/engine/install/linux-postinstall/](https://docs.docker.com/engine/install/linux-postinstall/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:44,reliability,doe,does,44,"If you type `sudo docker run 45f6c7767ff0`, does that also not launch it? Also you can add your username to the Docker group so you don't need sudo, as shown here -- but do that after we solve your issue:. [https://docs.docker.com/engine/install/linux-postinstall/](https://docs.docker.com/engine/install/linux-postinstall/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:96,usability,user,username,96,"If you type `sudo docker run 45f6c7767ff0`, does that also not launch it? Also you can add your username to the Docker group so you don't need sudo, as shown here -- but do that after we solve your issue:. [https://docs.docker.com/engine/install/linux-postinstall/](https://docs.docker.com/engine/install/linux-postinstall/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2567,availability,Down,Downloaded,2567,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2848,availability,operat,operations,2848,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2914,availability,operat,operations,2914,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2661,energy efficiency,core,core,2661,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2727,energy efficiency,optim,optimized,2727,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2807,energy efficiency,CPU,CPU,2807,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:3270,integrability,buffer,buffer,3270,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2666,interoperability,platform,platform,2666,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:3071,modifiability,interm,intermediate,3071,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:3119,modifiability,Interm,Intermediate,3119,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2727,performance,optimiz,optimized,2727,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2761,performance,Network,Network,2761,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2807,performance,CPU,CPU,2807,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2827,performance,perform,performance-critical,2827,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:3226,performance,time,time,3226,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:3242,performance,parallel,parallel,3242,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:276,safety,input,input,276,"Dear pgrosu, thank you very much for your suggestions. I want to wait for it to finish running and then try it immediately. Although I am not sure whether it is aff53ed783a7 or 45f6c7767ff0, this is my running command. `sudo docker run \. > -v ""/home/xxh/all_data/Deepvariant/input"":""/input"" \. > -v ""/home/xxh/all_data/Deepvariant/output"":""/output"" \. > google/deepvariant:""1.5.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > -ref=/input/bs_filled.fasta \. > --reads=/input/aln_sort.bam \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --num_shards=16 . [sudo] password for xxh: . Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant. 7608715873ec: Already exists . ff9c04d6f4fd: Already exists . ccc0633ad137: Already exists . 31c4e73f93e5: Already exists . 5f25a39487ec: Already exists . 634688f8f57d: Already exists . ce40c5e2b0ba: Already exists . 9eaa0cfabb44: Already exists . f71d9ff16a67: Already exists . 27ddda9faddb: Already exists . 6ba5fd944a25: Already exists . bec3e4e06e4d: Already exists . dc1469807dcc: Already exists . ceb45df4e1ef: Already exists . 4f6165ff322d: Already exists . f9cefd1876c2: Already exists . 931b80517c67: Already exists . 7b13ecd8df6e: Already exists . 245c9afd7ea9: Already exists . 97c2b022ac0f: Already exists . 4c15f3639f35: Already exists . fa21a1eddf03: Already exists . 6419c68a3d65: Already exists . 8751b2539913: Already exists . 20646815bf33: Already exists . 25dc07245f2e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Alre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:285,safety,input,input,285,"Dear pgrosu, thank you very much for your suggestions. I want to wait for it to finish running and then try it immediately. Although I am not sure whether it is aff53ed783a7 or 45f6c7767ff0, this is my running command. `sudo docker run \. > -v ""/home/xxh/all_data/Deepvariant/input"":""/input"" \. > -v ""/home/xxh/all_data/Deepvariant/output"":""/output"" \. > google/deepvariant:""1.5.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > -ref=/input/bs_filled.fasta \. > --reads=/input/aln_sort.bam \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --num_shards=16 . [sudo] password for xxh: . Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant. 7608715873ec: Already exists . ff9c04d6f4fd: Already exists . ccc0633ad137: Already exists . 31c4e73f93e5: Already exists . 5f25a39487ec: Already exists . 634688f8f57d: Already exists . ce40c5e2b0ba: Already exists . 9eaa0cfabb44: Already exists . f71d9ff16a67: Already exists . 27ddda9faddb: Already exists . 6ba5fd944a25: Already exists . bec3e4e06e4d: Already exists . dc1469807dcc: Already exists . ceb45df4e1ef: Already exists . 4f6165ff322d: Already exists . f9cefd1876c2: Already exists . 931b80517c67: Already exists . 7b13ecd8df6e: Already exists . 245c9afd7ea9: Already exists . 97c2b022ac0f: Already exists . 4c15f3639f35: Already exists . fa21a1eddf03: Already exists . 6419c68a3d65: Already exists . 8751b2539913: Already exists . 20646815bf33: Already exists . 25dc07245f2e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Alre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:457,safety,input,input,457,"Dear pgrosu, thank you very much for your suggestions. I want to wait for it to finish running and then try it immediately. Although I am not sure whether it is aff53ed783a7 or 45f6c7767ff0, this is my running command. `sudo docker run \. > -v ""/home/xxh/all_data/Deepvariant/input"":""/input"" \. > -v ""/home/xxh/all_data/Deepvariant/output"":""/output"" \. > google/deepvariant:""1.5.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > -ref=/input/bs_filled.fasta \. > --reads=/input/aln_sort.bam \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --num_shards=16 . [sudo] password for xxh: . Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant. 7608715873ec: Already exists . ff9c04d6f4fd: Already exists . ccc0633ad137: Already exists . 31c4e73f93e5: Already exists . 5f25a39487ec: Already exists . 634688f8f57d: Already exists . ce40c5e2b0ba: Already exists . 9eaa0cfabb44: Already exists . f71d9ff16a67: Already exists . 27ddda9faddb: Already exists . 6ba5fd944a25: Already exists . bec3e4e06e4d: Already exists . dc1469807dcc: Already exists . ceb45df4e1ef: Already exists . 4f6165ff322d: Already exists . f9cefd1876c2: Already exists . 931b80517c67: Already exists . 7b13ecd8df6e: Already exists . 245c9afd7ea9: Already exists . 97c2b022ac0f: Already exists . 4c15f3639f35: Already exists . fa21a1eddf03: Already exists . 6419c68a3d65: Already exists . 8751b2539913: Already exists . 20646815bf33: Already exists . 25dc07245f2e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Alre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:493,safety,input,input,493,"Dear pgrosu, thank you very much for your suggestions. I want to wait for it to finish running and then try it immediately. Although I am not sure whether it is aff53ed783a7 or 45f6c7767ff0, this is my running command. `sudo docker run \. > -v ""/home/xxh/all_data/Deepvariant/input"":""/input"" \. > -v ""/home/xxh/all_data/Deepvariant/output"":""/output"" \. > google/deepvariant:""1.5.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > -ref=/input/bs_filled.fasta \. > --reads=/input/aln_sort.bam \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --num_shards=16 . [sudo] password for xxh: . Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant. 7608715873ec: Already exists . ff9c04d6f4fd: Already exists . ccc0633ad137: Already exists . 31c4e73f93e5: Already exists . 5f25a39487ec: Already exists . 634688f8f57d: Already exists . ce40c5e2b0ba: Already exists . 9eaa0cfabb44: Already exists . f71d9ff16a67: Already exists . 27ddda9faddb: Already exists . 6ba5fd944a25: Already exists . bec3e4e06e4d: Already exists . dc1469807dcc: Already exists . ceb45df4e1ef: Already exists . 4f6165ff322d: Already exists . f9cefd1876c2: Already exists . 931b80517c67: Already exists . 7b13ecd8df6e: Already exists . 245c9afd7ea9: Already exists . 97c2b022ac0f: Already exists . 4c15f3639f35: Already exists . fa21a1eddf03: Already exists . 6419c68a3d65: Already exists . 8751b2539913: Already exists . 20646815bf33: Already exists . 25dc07245f2e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Alre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:3335,safety,input,input,3335,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:3368,safety,input,input,3368,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:625,security,password,password,625,"Dear pgrosu, thank you very much for your suggestions. I want to wait for it to finish running and then try it immediately. Although I am not sure whether it is aff53ed783a7 or 45f6c7767ff0, this is my running command. `sudo docker run \. > -v ""/home/xxh/all_data/Deepvariant/input"":""/input"" \. > -v ""/home/xxh/all_data/Deepvariant/output"":""/output"" \. > google/deepvariant:""1.5.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > -ref=/input/bs_filled.fasta \. > --reads=/input/aln_sort.bam \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --num_shards=16 . [sudo] password for xxh: . Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant. 7608715873ec: Already exists . ff9c04d6f4fd: Already exists . ccc0633ad137: Already exists . 31c4e73f93e5: Already exists . 5f25a39487ec: Already exists . 634688f8f57d: Already exists . ce40c5e2b0ba: Already exists . 9eaa0cfabb44: Already exists . f71d9ff16a67: Already exists . 27ddda9faddb: Already exists . 6ba5fd944a25: Already exists . bec3e4e06e4d: Already exists . dc1469807dcc: Already exists . ceb45df4e1ef: Already exists . 4f6165ff322d: Already exists . f9cefd1876c2: Already exists . 931b80517c67: Already exists . 7b13ecd8df6e: Already exists . 245c9afd7ea9: Already exists . 97c2b022ac0f: Already exists . 4c15f3639f35: Already exists . fa21a1eddf03: Already exists . 6419c68a3d65: Already exists . 8751b2539913: Already exists . 20646815bf33: Already exists . 25dc07245f2e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Alre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2761,security,Network,Network,2761,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:210,usability,command,command,210,"Dear pgrosu, thank you very much for your suggestions. I want to wait for it to finish running and then try it immediately. Although I am not sure whether it is aff53ed783a7 or 45f6c7767ff0, this is my running command. `sudo docker run \. > -v ""/home/xxh/all_data/Deepvariant/input"":""/input"" \. > -v ""/home/xxh/all_data/Deepvariant/output"":""/output"" \. > google/deepvariant:""1.5.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > -ref=/input/bs_filled.fasta \. > --reads=/input/aln_sort.bam \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --num_shards=16 . [sudo] password for xxh: . Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant. 7608715873ec: Already exists . ff9c04d6f4fd: Already exists . ccc0633ad137: Already exists . 31c4e73f93e5: Already exists . 5f25a39487ec: Already exists . 634688f8f57d: Already exists . ce40c5e2b0ba: Already exists . 9eaa0cfabb44: Already exists . f71d9ff16a67: Already exists . 27ddda9faddb: Already exists . 6ba5fd944a25: Already exists . bec3e4e06e4d: Already exists . dc1469807dcc: Already exists . ceb45df4e1ef: Already exists . 4f6165ff322d: Already exists . f9cefd1876c2: Already exists . 931b80517c67: Already exists . 7b13ecd8df6e: Already exists . 245c9afd7ea9: Already exists . 97c2b022ac0f: Already exists . 4c15f3639f35: Already exists . fa21a1eddf03: Already exists . 6419c68a3d65: Already exists . 8751b2539913: Already exists . 20646815bf33: Already exists . 25dc07245f2e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Alre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:276,usability,input,input,276,"Dear pgrosu, thank you very much for your suggestions. I want to wait for it to finish running and then try it immediately. Although I am not sure whether it is aff53ed783a7 or 45f6c7767ff0, this is my running command. `sudo docker run \. > -v ""/home/xxh/all_data/Deepvariant/input"":""/input"" \. > -v ""/home/xxh/all_data/Deepvariant/output"":""/output"" \. > google/deepvariant:""1.5.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > -ref=/input/bs_filled.fasta \. > --reads=/input/aln_sort.bam \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --num_shards=16 . [sudo] password for xxh: . Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant. 7608715873ec: Already exists . ff9c04d6f4fd: Already exists . ccc0633ad137: Already exists . 31c4e73f93e5: Already exists . 5f25a39487ec: Already exists . 634688f8f57d: Already exists . ce40c5e2b0ba: Already exists . 9eaa0cfabb44: Already exists . f71d9ff16a67: Already exists . 27ddda9faddb: Already exists . 6ba5fd944a25: Already exists . bec3e4e06e4d: Already exists . dc1469807dcc: Already exists . ceb45df4e1ef: Already exists . 4f6165ff322d: Already exists . f9cefd1876c2: Already exists . 931b80517c67: Already exists . 7b13ecd8df6e: Already exists . 245c9afd7ea9: Already exists . 97c2b022ac0f: Already exists . 4c15f3639f35: Already exists . fa21a1eddf03: Already exists . 6419c68a3d65: Already exists . 8751b2539913: Already exists . 20646815bf33: Already exists . 25dc07245f2e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Alre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:285,usability,input,input,285,"Dear pgrosu, thank you very much for your suggestions. I want to wait for it to finish running and then try it immediately. Although I am not sure whether it is aff53ed783a7 or 45f6c7767ff0, this is my running command. `sudo docker run \. > -v ""/home/xxh/all_data/Deepvariant/input"":""/input"" \. > -v ""/home/xxh/all_data/Deepvariant/output"":""/output"" \. > google/deepvariant:""1.5.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > -ref=/input/bs_filled.fasta \. > --reads=/input/aln_sort.bam \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --num_shards=16 . [sudo] password for xxh: . Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant. 7608715873ec: Already exists . ff9c04d6f4fd: Already exists . ccc0633ad137: Already exists . 31c4e73f93e5: Already exists . 5f25a39487ec: Already exists . 634688f8f57d: Already exists . ce40c5e2b0ba: Already exists . 9eaa0cfabb44: Already exists . f71d9ff16a67: Already exists . 27ddda9faddb: Already exists . 6ba5fd944a25: Already exists . bec3e4e06e4d: Already exists . dc1469807dcc: Already exists . ceb45df4e1ef: Already exists . 4f6165ff322d: Already exists . f9cefd1876c2: Already exists . 931b80517c67: Already exists . 7b13ecd8df6e: Already exists . 245c9afd7ea9: Already exists . 97c2b022ac0f: Already exists . 4c15f3639f35: Already exists . fa21a1eddf03: Already exists . 6419c68a3d65: Already exists . 8751b2539913: Already exists . 20646815bf33: Already exists . 25dc07245f2e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Alre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:457,usability,input,input,457,"Dear pgrosu, thank you very much for your suggestions. I want to wait for it to finish running and then try it immediately. Although I am not sure whether it is aff53ed783a7 or 45f6c7767ff0, this is my running command. `sudo docker run \. > -v ""/home/xxh/all_data/Deepvariant/input"":""/input"" \. > -v ""/home/xxh/all_data/Deepvariant/output"":""/output"" \. > google/deepvariant:""1.5.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > -ref=/input/bs_filled.fasta \. > --reads=/input/aln_sort.bam \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --num_shards=16 . [sudo] password for xxh: . Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant. 7608715873ec: Already exists . ff9c04d6f4fd: Already exists . ccc0633ad137: Already exists . 31c4e73f93e5: Already exists . 5f25a39487ec: Already exists . 634688f8f57d: Already exists . ce40c5e2b0ba: Already exists . 9eaa0cfabb44: Already exists . f71d9ff16a67: Already exists . 27ddda9faddb: Already exists . 6ba5fd944a25: Already exists . bec3e4e06e4d: Already exists . dc1469807dcc: Already exists . ceb45df4e1ef: Already exists . 4f6165ff322d: Already exists . f9cefd1876c2: Already exists . 931b80517c67: Already exists . 7b13ecd8df6e: Already exists . 245c9afd7ea9: Already exists . 97c2b022ac0f: Already exists . 4c15f3639f35: Already exists . fa21a1eddf03: Already exists . 6419c68a3d65: Already exists . 8751b2539913: Already exists . 20646815bf33: Already exists . 25dc07245f2e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Alre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:493,usability,input,input,493,"Dear pgrosu, thank you very much for your suggestions. I want to wait for it to finish running and then try it immediately. Although I am not sure whether it is aff53ed783a7 or 45f6c7767ff0, this is my running command. `sudo docker run \. > -v ""/home/xxh/all_data/Deepvariant/input"":""/input"" \. > -v ""/home/xxh/all_data/Deepvariant/output"":""/output"" \. > google/deepvariant:""1.5.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > -ref=/input/bs_filled.fasta \. > --reads=/input/aln_sort.bam \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --num_shards=16 . [sudo] password for xxh: . Unable to find image 'google/deepvariant:1.5.0' locally. 1.5.0: Pulling from google/deepvariant. 7608715873ec: Already exists . ff9c04d6f4fd: Already exists . ccc0633ad137: Already exists . 31c4e73f93e5: Already exists . 5f25a39487ec: Already exists . 634688f8f57d: Already exists . ce40c5e2b0ba: Already exists . 9eaa0cfabb44: Already exists . f71d9ff16a67: Already exists . 27ddda9faddb: Already exists . 6ba5fd944a25: Already exists . bec3e4e06e4d: Already exists . dc1469807dcc: Already exists . ceb45df4e1ef: Already exists . 4f6165ff322d: Already exists . f9cefd1876c2: Already exists . 931b80517c67: Already exists . 7b13ecd8df6e: Already exists . 245c9afd7ea9: Already exists . 97c2b022ac0f: Already exists . 4c15f3639f35: Already exists . fa21a1eddf03: Already exists . 6419c68a3d65: Already exists . 8751b2539913: Already exists . 20646815bf33: Already exists . 25dc07245f2e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Alre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2559,usability,Statu,Status,2559,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:2827,usability,perform,performance-critical,2827,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:3211,usability,command,command,3211,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:3335,usability,input,input,3335,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:3368,usability,input,input,3368,"e: Already exists . 6e3dea686609: Already exists . dc216b407a52: Already exists . c6710cf0efec: Already exists . 6a519085af15: Already exists . fd35c1634889: Already exists . 0e4b2b2ad2db: Already exists . 87e7c72faeb5: Already exists . 690adc142e08: Already exists . abfd217d5088: Already exists . 30b033b0505f: Already exists . 853ad599972a: Already exists . f20c79af8049: Already exists . 26703b5b7abd: Already exists . f3b33765da79: Already exists . c382f9fc227e: Already exists . 3a233bad0db5: Already exists . f6ac10e59ad4: Already exists . 9e1d2d199a37: Already exists . b50b4a1202e8: Already exists . 1286a89300c9: Already exists . 11f1b6d48e7b: Already exists . 09154ad67b50: Already exists . aad195d6c4df: Already exists . f8376ea6a177: Already exists . b44e5a321822: Already exists . e81a72561181: Already exists . e96d0f626428: Already exists . bea852b4c2f5: Already exists . 13d620954600: Already exists . 123b4e4b7a6e: Already exists . Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e. Status: Downloaded newer image for google/deepvariant:1.5.0. 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:496,modifiability,paramet,parameters,496,"Dear @XXH123a,. Congrats! You seem to be running DeepVariant now, which is most important. Don't worry about trying out anything else. The ids `aff53ed783a7` and `45f6c7767ff0` are just image ids that are another way to launch DeepTrio and DeepVariant images, as opposed to using the canonical names `google/deepvariant:deeptrio-1.5.0` or `google/deepvariant:1.5.0`. . I'm not seeing the backslashes after each line, but according to the running commands, it seems to have received the necessary parameters. You might have image layers taking up space, but if things are working for you just stick with that for now, in order to complete the analysis, which is more important. Well done! ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:529,modifiability,layer,layers,529,"Dear @XXH123a,. Congrats! You seem to be running DeepVariant now, which is most important. Don't worry about trying out anything else. The ids `aff53ed783a7` and `45f6c7767ff0` are just image ids that are another way to launch DeepTrio and DeepVariant images, as opposed to using the canonical names `google/deepvariant:deeptrio-1.5.0` or `google/deepvariant:1.5.0`. . I'm not seeing the backslashes after each line, but according to the running commands, it seems to have received the necessary parameters. You might have image layers taking up space, but if things are working for you just stick with that for now, in order to complete the analysis, which is more important. Well done! ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:629,safety,compl,complete,629,"Dear @XXH123a,. Congrats! You seem to be running DeepVariant now, which is most important. Don't worry about trying out anything else. The ids `aff53ed783a7` and `45f6c7767ff0` are just image ids that are another way to launch DeepTrio and DeepVariant images, as opposed to using the canonical names `google/deepvariant:deeptrio-1.5.0` or `google/deepvariant:1.5.0`. . I'm not seeing the backslashes after each line, but according to the running commands, it seems to have received the necessary parameters. You might have image layers taking up space, but if things are working for you just stick with that for now, in order to complete the analysis, which is more important. Well done! ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:629,security,compl,complete,629,"Dear @XXH123a,. Congrats! You seem to be running DeepVariant now, which is most important. Don't worry about trying out anything else. The ids `aff53ed783a7` and `45f6c7767ff0` are just image ids that are another way to launch DeepTrio and DeepVariant images, as opposed to using the canonical names `google/deepvariant:deeptrio-1.5.0` or `google/deepvariant:1.5.0`. . I'm not seeing the backslashes after each line, but according to the running commands, it seems to have received the necessary parameters. You might have image layers taking up space, but if things are working for you just stick with that for now, in order to complete the analysis, which is more important. Well done! ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:446,usability,command,commands,446,"Dear @XXH123a,. Congrats! You seem to be running DeepVariant now, which is most important. Don't worry about trying out anything else. The ids `aff53ed783a7` and `45f6c7767ff0` are just image ids that are another way to launch DeepTrio and DeepVariant images, as opposed to using the canonical names `google/deepvariant:deeptrio-1.5.0` or `google/deepvariant:1.5.0`. . I'm not seeing the backslashes after each line, but according to the running commands, it seems to have received the necessary parameters. You might have image layers taking up space, but if things are working for you just stick with that for now, in order to complete the analysis, which is more important. Well done! ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:58,usability,guidanc,guidance,58,"Successfully run, once again sincerely thank you for your guidance",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/701:507,availability,avail,available,507,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:608,deployability,log,login,608,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:925,deployability,log,login,925,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:507,reliability,availab,available,507,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:507,safety,avail,available,507,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:608,safety,log,login,608,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:925,safety,log,login,925,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:507,security,availab,available,507,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:608,security,log,login,608,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:925,security,log,login,925,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:363,testability,coverag,coverage,363,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:454,testability,coverag,coverage,454,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:608,testability,log,login,608,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:925,testability,log,login,925,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:801,usability,learn,learning-based,801,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site? 2) Is that SNP at an exon boundary? 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.). 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,. Paul. #### References. [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:21,deployability,updat,updated,21,"Hi @pgrosu,. 1. I've updated the table in my original post with QUAL and GQ values (the QUAL/GQ of the second variant are low). 2. How can I check this? I mainly work with WGS data. 3. Should be skin stanza. 4. No, these two sites are not in this portal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:21,safety,updat,updated,21,"Hi @pgrosu,. 1. I've updated the table in my original post with QUAL and GQ values (the QUAL/GQ of the second variant are low). 2. How can I check this? I mainly work with WGS data. 3. Should be skin stanza. 4. No, these two sites are not in this portal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:21,security,updat,updated,21,"Hi @pgrosu,. 1. I've updated the table in my original post with QUAL and GQ values (the QUAL/GQ of the second variant are low). 2. How can I check this? I mainly work with WGS data. 3. Should be skin stanza. 4. No, these two sites are not in this portal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1034,availability,down,downsample,1034," information from the [GTF-formatted GENCODE files](https://www.gencodegenes.org/human/), which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1329,availability,down,downsample,1329,"which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid, and in the recombinant regions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1460,deployability,updat,updated,1460,"which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid, and in the recombinant regions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1072,energy efficiency,model,model,1072," GENCODE files](https://www.gencodegenes.org/human/), which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1206,energy efficiency,model,model,1206,"which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid, and in the recombinant regions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1472,energy efficiency,model,model,1472,"which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid, and in the recombinant regions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1626,energy efficiency,model,model,1626,"which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid, and in the recombinant regions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:66,interoperability,format,formatted,66,"Hi @raphaelbetschart,. You can get exon information from the [GTF-formatted GENCODE files](https://www.gencodegenes.org/human/), which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything prob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1906,reliability,Doe,Does,1906,"which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid, and in the recombinant regions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1460,safety,updat,updated,1460,"which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid, and in the recombinant regions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1072,security,model,model,1072," GENCODE files](https://www.gencodegenes.org/human/), which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1206,security,model,model,1206,"which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid, and in the recombinant regions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1460,security,updat,updated,1460,"which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid, and in the recombinant regions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1472,security,model,model,1472,"which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid, and in the recombinant regions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1626,security,model,model,1626,"which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid, and in the recombinant regions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1087,usability,support,supporting,1087,"ttps://www.gencodegenes.org/human/), which will label exon regions like this (including their start and end sites):. ```. chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";. ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:72,deployability,observ,observe,72,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:139,deployability,observ,observations,139,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:195,deployability,observ,observations,195,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:499,deployability,observ,observation,499,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:603,deployability,observ,observations,603,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:107,integrability,sub,substantially,107,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:130,interoperability,standard,standard,130,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:356,interoperability,specif,specific-variant-in-my-data,356,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:456,interoperability,format,format,456,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:738,modifiability,paramet,parameters,738,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:328,reliability,doe,does-deepvariant-not-call-a-specific-variant-in-my-data,328,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1049,security,iso,isoforms,1049,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:72,testability,observ,observe,72,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:139,testability,observ,observations,139,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:195,testability,observ,observations,195,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:499,testability,observ,observation,499,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:603,testability,observ,observations,603,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:948,testability,coverag,coverage,948,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:792,usability,clear,clear,792,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)? What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:631,availability,down,downsampling,631,"Hi @pgrosu and @AndrewCarroll . - My variant doesn't fall close to an exon border. . - I've used the RNA-model from here: [RNA-model](https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard). . - You are correct, I have a diplod human germline sample. . - The variant is neither in the ENCODE blacklist nor in RepeatMasker, however the Umap M24 track has a value of 0.375. - The MAPQ value is 28.5, which shouldn't be a problem. - The coverage of the region is high, because the transcript is abundant (TPM is 1100). . My guess would be that this problem arises from downsampling? The depth at this site is almost 4000X.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:105,energy efficiency,model,model,105,"Hi @pgrosu and @AndrewCarroll . - My variant doesn't fall close to an exon border. . - I've used the RNA-model from here: [RNA-model](https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard). . - You are correct, I have a diplod human germline sample. . - The variant is neither in the ENCODE blacklist nor in RepeatMasker, however the Umap M24 track has a value of 0.375. - The MAPQ value is 28.5, which shouldn't be a problem. - The coverage of the region is high, because the transcript is abundant (TPM is 1100). . My guess would be that this problem arises from downsampling? The depth at this site is almost 4000X.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:127,energy efficiency,model,model,127,"Hi @pgrosu and @AndrewCarroll . - My variant doesn't fall close to an exon border. . - I've used the RNA-model from here: [RNA-model](https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard). . - You are correct, I have a diplod human germline sample. . - The variant is neither in the ENCODE blacklist nor in RepeatMasker, however the Umap M24 track has a value of 0.375. - The MAPQ value is 28.5, which shouldn't be a problem. - The coverage of the region is high, because the transcript is abundant (TPM is 1100). . My guess would be that this problem arises from downsampling? The depth at this site is almost 4000X.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:177,energy efficiency,model,models,177,"Hi @pgrosu and @AndrewCarroll . - My variant doesn't fall close to an exon border. . - I've used the RNA-model from here: [RNA-model](https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard). . - You are correct, I have a diplod human germline sample. . - The variant is neither in the ENCODE blacklist nor in RepeatMasker, however the Umap M24 track has a value of 0.375. - The MAPQ value is 28.5, which shouldn't be a problem. - The coverage of the region is high, because the transcript is abundant (TPM is 1100). . My guess would be that this problem arises from downsampling? The depth at this site is almost 4000X.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:45,reliability,doe,doesn,45,"Hi @pgrosu and @AndrewCarroll . - My variant doesn't fall close to an exon border. . - I've used the RNA-model from here: [RNA-model](https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard). . - You are correct, I have a diplod human germline sample. . - The variant is neither in the ENCODE blacklist nor in RepeatMasker, however the Umap M24 track has a value of 0.375. - The MAPQ value is 28.5, which shouldn't be a problem. - The coverage of the region is high, because the transcript is abundant (TPM is 1100). . My guess would be that this problem arises from downsampling? The depth at this site is almost 4000X.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:105,security,model,model,105,"Hi @pgrosu and @AndrewCarroll . - My variant doesn't fall close to an exon border. . - I've used the RNA-model from here: [RNA-model](https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard). . - You are correct, I have a diplod human germline sample. . - The variant is neither in the ENCODE blacklist nor in RepeatMasker, however the Umap M24 track has a value of 0.375. - The MAPQ value is 28.5, which shouldn't be a problem. - The coverage of the region is high, because the transcript is abundant (TPM is 1100). . My guess would be that this problem arises from downsampling? The depth at this site is almost 4000X.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:127,security,model,model,127,"Hi @pgrosu and @AndrewCarroll . - My variant doesn't fall close to an exon border. . - I've used the RNA-model from here: [RNA-model](https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard). . - You are correct, I have a diplod human germline sample. . - The variant is neither in the ENCODE blacklist nor in RepeatMasker, however the Umap M24 track has a value of 0.375. - The MAPQ value is 28.5, which shouldn't be a problem. - The coverage of the region is high, because the transcript is abundant (TPM is 1100). . My guess would be that this problem arises from downsampling? The depth at this site is almost 4000X.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:177,security,model,models,177,"Hi @pgrosu and @AndrewCarroll . - My variant doesn't fall close to an exon border. . - I've used the RNA-model from here: [RNA-model](https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard). . - You are correct, I have a diplod human germline sample. . - The variant is neither in the ENCODE blacklist nor in RepeatMasker, however the Umap M24 track has a value of 0.375. - The MAPQ value is 28.5, which shouldn't be a problem. - The coverage of the region is high, because the transcript is abundant (TPM is 1100). . My guess would be that this problem arises from downsampling? The depth at this site is almost 4000X.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:499,testability,coverag,coverage,499,"Hi @pgrosu and @AndrewCarroll . - My variant doesn't fall close to an exon border. . - I've used the RNA-model from here: [RNA-model](https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard). . - You are correct, I have a diplod human germline sample. . - The variant is neither in the ENCODE blacklist nor in RepeatMasker, however the Umap M24 track has a value of 0.375. - The MAPQ value is 28.5, which shouldn't be a problem. - The coverage of the region is high, because the transcript is abundant (TPM is 1100). . My guess would be that this problem arises from downsampling? The depth at this site is almost 4000X.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:58,usability,close,close,58,"Hi @pgrosu and @AndrewCarroll . - My variant doesn't fall close to an exon border. . - I've used the RNA-model from here: [RNA-model](https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard). . - You are correct, I have a diplod human germline sample. . - The variant is neither in the ENCODE blacklist nor in RepeatMasker, however the Umap M24 track has a value of 0.375. - The MAPQ value is 28.5, which shouldn't be a problem. - The coverage of the region is high, because the transcript is abundant (TPM is 1100). . My guess would be that this problem arises from downsampling? The depth at this site is almost 4000X.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:165,availability,recov,recovering,165,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:165,deployability,recov,recovering,165,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:647,energy efficiency,model,model,647,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:210,modifiability,scenario,scenarios,210,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1356,modifiability,variab,variable,1356,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:165,reliability,recov,recovering,165,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1092,reliability,doe,does-it-work,1092,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:165,safety,recov,recovering,165,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:165,security,recov,recovering,165,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:399,security,iso,isoforms,399,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:647,security,model,model,647,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1724,security,iso,isoforms,1724,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1572,usability,behavi,behavior,1572,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1604,usability,close,closer,1604,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1742,usability,help,helps,1742,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```. --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads. ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms? Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:60,deployability,pipelin,pipeline,60,"Hi @pgrosu . In the meantime I was running DRAGEN 4.2.4 RNA pipeline to check the variants, and I have some interesting findings. . | GT (DV) | GT(D) | AD (DV) | AD (D) |. | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 0/1 | 67,67 | 715,697. `samtools coverage` at this position reports the following:. | numreads | covbases | coverage | meandepth | meanbaseq | meanmapq. | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |. 3009 |1 | 100 | 2997 | 36.7 | 30.3. I will run DeepVariant again to inspect the realigned BAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:60,integrability,pipelin,pipeline,60,"Hi @pgrosu . In the meantime I was running DRAGEN 4.2.4 RNA pipeline to check the variants, and I have some interesting findings. . | GT (DV) | GT(D) | AD (DV) | AD (D) |. | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 0/1 | 67,67 | 715,697. `samtools coverage` at this position reports the following:. | numreads | covbases | coverage | meandepth | meanbaseq | meanmapq. | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |. 3009 |1 | 100 | 2997 | 36.7 | 30.3. I will run DeepVariant again to inspect the realigned BAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:280,testability,coverag,coverage,280,"Hi @pgrosu . In the meantime I was running DRAGEN 4.2.4 RNA pipeline to check the variants, and I have some interesting findings. . | GT (DV) | GT(D) | AD (DV) | AD (D) |. | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 0/1 | 67,67 | 715,697. `samtools coverage` at this position reports the following:. | numreads | covbases | coverage | meandepth | meanbaseq | meanmapq. | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |. 3009 |1 | 100 | 2997 | 36.7 | 30.3. I will run DeepVariant again to inspect the realigned BAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:355,testability,coverag,coverage,355,"Hi @pgrosu . In the meantime I was running DRAGEN 4.2.4 RNA pipeline to check the variants, and I have some interesting findings. . | GT (DV) | GT(D) | AD (DV) | AD (D) |. | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 0/1 | 67,67 | 715,697. `samtools coverage` at this position reports the following:. | numreads | covbases | coverage | meandepth | meanbaseq | meanmapq. | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |. 3009 |1 | 100 | 2997 | 36.7 | 30.3. I will run DeepVariant again to inspect the realigned BAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:517,energy efficiency,optim,optimal,517,"Hi @raphaelbetschart,. Absolutely it is interesting, but if we can reason out what specific internal steps produced those results then I get very excited :) The thing is that this all happens based how the reads get processed with respect to the reference. Once we understand that deeply, then the rest is just a variation on a theme. You probably could get similar results if you used the flag `--norealign_reads` (or `--realign_reads=false`), to turn off the realigner, but does that mean the pre-aligned reads are optimal for DeepVariant's matrix interpretation by the model? Looking forward to what you see with the realigned reads,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:572,energy efficiency,model,model,572,"Hi @raphaelbetschart,. Absolutely it is interesting, but if we can reason out what specific internal steps produced those results then I get very excited :) The thing is that this all happens based how the reads get processed with respect to the reference. Once we understand that deeply, then the rest is just a variation on a theme. You probably could get similar results if you used the flag `--norealign_reads` (or `--realign_reads=false`), to turn off the realigner, but does that mean the pre-aligned reads are optimal for DeepVariant's matrix interpretation by the model? Looking forward to what you see with the realigned reads,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:83,interoperability,specif,specific,83,"Hi @raphaelbetschart,. Absolutely it is interesting, but if we can reason out what specific internal steps produced those results then I get very excited :) The thing is that this all happens based how the reads get processed with respect to the reference. Once we understand that deeply, then the rest is just a variation on a theme. You probably could get similar results if you used the flag `--norealign_reads` (or `--realign_reads=false`), to turn off the realigner, but does that mean the pre-aligned reads are optimal for DeepVariant's matrix interpretation by the model? Looking forward to what you see with the realigned reads,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:476,reliability,doe,does,476,"Hi @raphaelbetschart,. Absolutely it is interesting, but if we can reason out what specific internal steps produced those results then I get very excited :) The thing is that this all happens based how the reads get processed with respect to the reference. Once we understand that deeply, then the rest is just a variation on a theme. You probably could get similar results if you used the flag `--norealign_reads` (or `--realign_reads=false`), to turn off the realigner, but does that mean the pre-aligned reads are optimal for DeepVariant's matrix interpretation by the model? Looking forward to what you see with the realigned reads,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:572,security,model,model,572,"Hi @raphaelbetschart,. Absolutely it is interesting, but if we can reason out what specific internal steps produced those results then I get very excited :) The thing is that this all happens based how the reads get processed with respect to the reference. Once we understand that deeply, then the rest is just a variation on a theme. You probably could get similar results if you used the flag `--norealign_reads` (or `--realign_reads=false`), to turn off the realigner, but does that mean the pre-aligned reads are optimal for DeepVariant's matrix interpretation by the model? Looking forward to what you see with the realigned reads,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:265,testability,understand,understand,265,"Hi @raphaelbetschart,. Absolutely it is interesting, but if we can reason out what specific internal steps produced those results then I get very excited :) The thing is that this all happens based how the reads get processed with respect to the reference. Once we understand that deeply, then the rest is just a variation on a theme. You probably could get similar results if you used the flag `--norealign_reads` (or `--realign_reads=false`), to turn off the realigner, but does that mean the pre-aligned reads are optimal for DeepVariant's matrix interpretation by the model? Looking forward to what you see with the realigned reads,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:99,safety,input,input,99,"Hi @pgrosu,. I've realigned the reads and noticed that I have only 198 reads at this position. The input BAM reports 3154 reads at this position. I guess my region of interest is indeed located in a low mappability region.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:99,usability,input,input,99,"Hi @pgrosu,. I've realigned the reads and noticed that I have only 198 reads at this position. The input BAM reports 3154 reads at this position. I guess my region of interest is indeed located in a low mappability region.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1024,integrability,discover,discovered,1024,"Hi @raphaelbetschart,. That gets us closer to what might be happening, but it still does not provide good insight into the properties of the reads that don't align to the region. Why did they align initially here and where else could they align? Regarding the reads that do align, how does the window of 200 bases (50 on each side of the variant) x 100 sampled reads support the reference versus the alternate? Is there something special regarding the reference-supporting ones vs the ones supporting the alternate that provide a 1/1 genotype? If you run the same 198 reads through Dragen does the genotype switch to 1/1? Are they all the same expression or do they have variation among them (isoforms)? If they have the same expression what is the quality of the reads, and if they don't what is the variation in the 200 x 100 window? Is there a bias among the alternate and if so what characteristics in the reads become prevalent as compared to the reference-supporting ones? Again many questions until the root cause is discovered. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1024,interoperability,discover,discovered,1024,"Hi @raphaelbetschart,. That gets us closer to what might be happening, but it still does not provide good insight into the properties of the reads that don't align to the region. Why did they align initially here and where else could they align? Regarding the reads that do align, how does the window of 200 bases (50 on each side of the variant) x 100 sampled reads support the reference versus the alternate? Is there something special regarding the reference-supporting ones vs the ones supporting the alternate that provide a 1/1 genotype? If you run the same 198 reads through Dragen does the genotype switch to 1/1? Are they all the same expression or do they have variation among them (isoforms)? If they have the same expression what is the quality of the reads, and if they don't what is the variation in the 200 x 100 window? Is there a bias among the alternate and if so what characteristics in the reads become prevalent as compared to the reference-supporting ones? Again many questions until the root cause is discovered. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:84,reliability,doe,does,84,"Hi @raphaelbetschart,. That gets us closer to what might be happening, but it still does not provide good insight into the properties of the reads that don't align to the region. Why did they align initially here and where else could they align? Regarding the reads that do align, how does the window of 200 bases (50 on each side of the variant) x 100 sampled reads support the reference versus the alternate? Is there something special regarding the reference-supporting ones vs the ones supporting the alternate that provide a 1/1 genotype? If you run the same 198 reads through Dragen does the genotype switch to 1/1? Are they all the same expression or do they have variation among them (isoforms)? If they have the same expression what is the quality of the reads, and if they don't what is the variation in the 200 x 100 window? Is there a bias among the alternate and if so what characteristics in the reads become prevalent as compared to the reference-supporting ones? Again many questions until the root cause is discovered. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:285,reliability,doe,does,285,"Hi @raphaelbetschart,. That gets us closer to what might be happening, but it still does not provide good insight into the properties of the reads that don't align to the region. Why did they align initially here and where else could they align? Regarding the reads that do align, how does the window of 200 bases (50 on each side of the variant) x 100 sampled reads support the reference versus the alternate? Is there something special regarding the reference-supporting ones vs the ones supporting the alternate that provide a 1/1 genotype? If you run the same 198 reads through Dragen does the genotype switch to 1/1? Are they all the same expression or do they have variation among them (isoforms)? If they have the same expression what is the quality of the reads, and if they don't what is the variation in the 200 x 100 window? Is there a bias among the alternate and if so what characteristics in the reads become prevalent as compared to the reference-supporting ones? Again many questions until the root cause is discovered. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:589,reliability,doe,does,589,"Hi @raphaelbetschart,. That gets us closer to what might be happening, but it still does not provide good insight into the properties of the reads that don't align to the region. Why did they align initially here and where else could they align? Regarding the reads that do align, how does the window of 200 bases (50 on each side of the variant) x 100 sampled reads support the reference versus the alternate? Is there something special regarding the reference-supporting ones vs the ones supporting the alternate that provide a 1/1 genotype? If you run the same 198 reads through Dragen does the genotype switch to 1/1? Are they all the same expression or do they have variation among them (isoforms)? If they have the same expression what is the quality of the reads, and if they don't what is the variation in the 200 x 100 window? Is there a bias among the alternate and if so what characteristics in the reads become prevalent as compared to the reference-supporting ones? Again many questions until the root cause is discovered. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:693,security,iso,isoforms,693,"Hi @raphaelbetschart,. That gets us closer to what might be happening, but it still does not provide good insight into the properties of the reads that don't align to the region. Why did they align initially here and where else could they align? Regarding the reads that do align, how does the window of 200 bases (50 on each side of the variant) x 100 sampled reads support the reference versus the alternate? Is there something special regarding the reference-supporting ones vs the ones supporting the alternate that provide a 1/1 genotype? If you run the same 198 reads through Dragen does the genotype switch to 1/1? Are they all the same expression or do they have variation among them (isoforms)? If they have the same expression what is the quality of the reads, and if they don't what is the variation in the 200 x 100 window? Is there a bias among the alternate and if so what characteristics in the reads become prevalent as compared to the reference-supporting ones? Again many questions until the root cause is discovered. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:36,usability,close,closer,36,"Hi @raphaelbetschart,. That gets us closer to what might be happening, but it still does not provide good insight into the properties of the reads that don't align to the region. Why did they align initially here and where else could they align? Regarding the reads that do align, how does the window of 200 bases (50 on each side of the variant) x 100 sampled reads support the reference versus the alternate? Is there something special regarding the reference-supporting ones vs the ones supporting the alternate that provide a 1/1 genotype? If you run the same 198 reads through Dragen does the genotype switch to 1/1? Are they all the same expression or do they have variation among them (isoforms)? If they have the same expression what is the quality of the reads, and if they don't what is the variation in the 200 x 100 window? Is there a bias among the alternate and if so what characteristics in the reads become prevalent as compared to the reference-supporting ones? Again many questions until the root cause is discovered. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:367,usability,support,support,367,"Hi @raphaelbetschart,. That gets us closer to what might be happening, but it still does not provide good insight into the properties of the reads that don't align to the region. Why did they align initially here and where else could they align? Regarding the reads that do align, how does the window of 200 bases (50 on each side of the variant) x 100 sampled reads support the reference versus the alternate? Is there something special regarding the reference-supporting ones vs the ones supporting the alternate that provide a 1/1 genotype? If you run the same 198 reads through Dragen does the genotype switch to 1/1? Are they all the same expression or do they have variation among them (isoforms)? If they have the same expression what is the quality of the reads, and if they don't what is the variation in the 200 x 100 window? Is there a bias among the alternate and if so what characteristics in the reads become prevalent as compared to the reference-supporting ones? Again many questions until the root cause is discovered. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:462,usability,support,supporting,462,"Hi @raphaelbetschart,. That gets us closer to what might be happening, but it still does not provide good insight into the properties of the reads that don't align to the region. Why did they align initially here and where else could they align? Regarding the reads that do align, how does the window of 200 bases (50 on each side of the variant) x 100 sampled reads support the reference versus the alternate? Is there something special regarding the reference-supporting ones vs the ones supporting the alternate that provide a 1/1 genotype? If you run the same 198 reads through Dragen does the genotype switch to 1/1? Are they all the same expression or do they have variation among them (isoforms)? If they have the same expression what is the quality of the reads, and if they don't what is the variation in the 200 x 100 window? Is there a bias among the alternate and if so what characteristics in the reads become prevalent as compared to the reference-supporting ones? Again many questions until the root cause is discovered. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:490,usability,support,supporting,490,"Hi @raphaelbetschart,. That gets us closer to what might be happening, but it still does not provide good insight into the properties of the reads that don't align to the region. Why did they align initially here and where else could they align? Regarding the reads that do align, how does the window of 200 bases (50 on each side of the variant) x 100 sampled reads support the reference versus the alternate? Is there something special regarding the reference-supporting ones vs the ones supporting the alternate that provide a 1/1 genotype? If you run the same 198 reads through Dragen does the genotype switch to 1/1? Are they all the same expression or do they have variation among them (isoforms)? If they have the same expression what is the quality of the reads, and if they don't what is the variation in the 200 x 100 window? Is there a bias among the alternate and if so what characteristics in the reads become prevalent as compared to the reference-supporting ones? Again many questions until the root cause is discovered. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:962,usability,support,supporting,962,"Hi @raphaelbetschart,. That gets us closer to what might be happening, but it still does not provide good insight into the properties of the reads that don't align to the region. Why did they align initially here and where else could they align? Regarding the reads that do align, how does the window of 200 bases (50 on each side of the variant) x 100 sampled reads support the reference versus the alternate? Is there something special regarding the reference-supporting ones vs the ones supporting the alternate that provide a 1/1 genotype? If you run the same 198 reads through Dragen does the genotype switch to 1/1? Are they all the same expression or do they have variation among them (isoforms)? If they have the same expression what is the quality of the reads, and if they don't what is the variation in the 200 x 100 window? Is there a bias among the alternate and if so what characteristics in the reads become prevalent as compared to the reference-supporting ones? Again many questions until the root cause is discovered. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1024,usability,discov,discovered,1024,"Hi @raphaelbetschart,. That gets us closer to what might be happening, but it still does not provide good insight into the properties of the reads that don't align to the region. Why did they align initially here and where else could they align? Regarding the reads that do align, how does the window of 200 bases (50 on each side of the variant) x 100 sampled reads support the reference versus the alternate? Is there something special regarding the reference-supporting ones vs the ones supporting the alternate that provide a 1/1 genotype? If you run the same 198 reads through Dragen does the genotype switch to 1/1? Are they all the same expression or do they have variation among them (isoforms)? If they have the same expression what is the quality of the reads, and if they don't what is the variation in the 200 x 100 window? Is there a bias among the alternate and if so what characteristics in the reads become prevalent as compared to the reference-supporting ones? Again many questions until the root cause is discovered. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:1044,usability,help,helps,1044,"Hi @raphaelbetschart,. That gets us closer to what might be happening, but it still does not provide good insight into the properties of the reads that don't align to the region. Why did they align initially here and where else could they align? Regarding the reads that do align, how does the window of 200 bases (50 on each side of the variant) x 100 sampled reads support the reference versus the alternate? Is there something special regarding the reference-supporting ones vs the ones supporting the alternate that provide a 1/1 genotype? If you run the same 198 reads through Dragen does the genotype switch to 1/1? Are they all the same expression or do they have variation among them (isoforms)? If they have the same expression what is the quality of the reads, and if they don't what is the variation in the 200 x 100 window? Is there a bias among the alternate and if so what characteristics in the reads become prevalent as compared to the reference-supporting ones? Again many questions until the root cause is discovered. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:154,deployability,fail,fail,154,"Hi @pgrosu,. After checking my region of interest, I came to the conclusion that is doesn't make sense to perform variant calling at all. Most of my SNVs fail several filters. Maybe at a later point I can get my hands on WGS data from the same samples to perform some comparisons.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:167,integrability,filter,filters,167,"Hi @pgrosu,. After checking my region of interest, I came to the conclusion that is doesn't make sense to perform variant calling at all. Most of my SNVs fail several filters. Maybe at a later point I can get my hands on WGS data from the same samples to perform some comparisons.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:106,performance,perform,perform,106,"Hi @pgrosu,. After checking my region of interest, I came to the conclusion that is doesn't make sense to perform variant calling at all. Most of my SNVs fail several filters. Maybe at a later point I can get my hands on WGS data from the same samples to perform some comparisons.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:255,performance,perform,perform,255,"Hi @pgrosu,. After checking my region of interest, I came to the conclusion that is doesn't make sense to perform variant calling at all. Most of my SNVs fail several filters. Maybe at a later point I can get my hands on WGS data from the same samples to perform some comparisons.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:84,reliability,doe,doesn,84,"Hi @pgrosu,. After checking my region of interest, I came to the conclusion that is doesn't make sense to perform variant calling at all. Most of my SNVs fail several filters. Maybe at a later point I can get my hands on WGS data from the same samples to perform some comparisons.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:154,reliability,fail,fail,154,"Hi @pgrosu,. After checking my region of interest, I came to the conclusion that is doesn't make sense to perform variant calling at all. Most of my SNVs fail several filters. Maybe at a later point I can get my hands on WGS data from the same samples to perform some comparisons.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:106,usability,perform,perform,106,"Hi @pgrosu,. After checking my region of interest, I came to the conclusion that is doesn't make sense to perform variant calling at all. Most of my SNVs fail several filters. Maybe at a later point I can get my hands on WGS data from the same samples to perform some comparisons.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:255,usability,perform,perform,255,"Hi @pgrosu,. After checking my region of interest, I came to the conclusion that is doesn't make sense to perform variant calling at all. Most of my SNVs fail several filters. Maybe at a later point I can get my hands on WGS data from the same samples to perform some comparisons.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:32,energy efficiency,cool,cool,32,"Hi @raphaelbetschart,. That's a cool insight! Sometimes a negative result can be just as important as a positive one, pointing you towards a new way of looking at your experiment. Hope you uncover the underlying mechanism,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:52,performance,perform,perform,52,"@raphaelbetschart if you do have any opportunity to perform a comparison between RNA-seq and WGS data, we would be very interested in the results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:52,usability,perform,perform,52,"@raphaelbetschart if you do have any opportunity to perform a comparison between RNA-seq and WGS data, we would be very interested in the results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:105,testability,Trace,Traces,105,"Hi Daniel (@danielecook),. Would the following comparison experiment help:. https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SRP253177&o=acc_s%3Aa. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:69,usability,help,help,69,"Hi Daniel (@danielecook),. Would the following comparison experiment help:. https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SRP253177&o=acc_s%3Aa. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:73,performance,time,time,73,@pgrosu - yes that dataset is helpful. It's just a matter of finding the time to do the analysis (:. I will close this for now. @raphaelbetschart if you have any further questions please feel free to reopen or create a new issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:30,usability,help,helpful,30,@pgrosu - yes that dataset is helpful. It's just a matter of finding the time to do the analysis (:. I will close this for now. @raphaelbetschart if you have any further questions please feel free to reopen or create a new issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:108,usability,close,close,108,@pgrosu - yes that dataset is helpful. It's just a matter of finding the time to do the analysis (:. I will close this for now. @raphaelbetschart if you have any further questions please feel free to reopen or create a new issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:25,availability,avail,available,25,"@danielecook If you have available resources through which I can help you out to perform this analysis, feel free to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:35,deployability,resourc,resources,35,"@danielecook If you have available resources through which I can help you out to perform this analysis, feel free to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:35,energy efficiency,resourc,resources,35,"@danielecook If you have available resources through which I can help you out to perform this analysis, feel free to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:35,performance,resourc,resources,35,"@danielecook If you have available resources through which I can help you out to perform this analysis, feel free to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:81,performance,perform,perform,81,"@danielecook If you have available resources through which I can help you out to perform this analysis, feel free to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:25,reliability,availab,available,25,"@danielecook If you have available resources through which I can help you out to perform this analysis, feel free to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:25,safety,avail,available,25,"@danielecook If you have available resources through which I can help you out to perform this analysis, feel free to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:35,safety,resourc,resources,35,"@danielecook If you have available resources through which I can help you out to perform this analysis, feel free to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:25,security,availab,available,25,"@danielecook If you have available resources through which I can help you out to perform this analysis, feel free to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:35,testability,resourc,resources,35,"@danielecook If you have available resources through which I can help you out to perform this analysis, feel free to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:65,usability,help,help,65,"@danielecook If you have available resources through which I can help you out to perform this analysis, feel free to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:81,usability,perform,perform,81,"@danielecook If you have available resources through which I can help you out to perform this analysis, feel free to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/702:1015,availability,down,downsample,1015,"m assuming you are working with germline diploid samples. $`1)`$ Ti/Tv ratio is calculated as follows. For every variant that is a biallelic SNP, it will check as follows:. ```Python. is_transition = if {'A' <-> 'G'} or {'C' <-> 'T'}. is_transversion = not( is_transition ). else is_transition = is_transversion = False. ```. The Ti/Tv ratio is calculated as `float(variant_counts(is_transition)) / variant_counts(is_transversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1960,deployability,depend,depends,1960,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:661,energy efficiency,model,model,661,"Hi @jianqi-chen,. I'm assuming you are working with germline diploid samples. $`1)`$ Ti/Tv ratio is calculated as follows. For every variant that is a biallelic SNP, it will check as follows:. ```Python. is_transition = if {'A' <-> 'G'} or {'C' <-> 'T'}. is_transversion = not( is_transition ). else is_transition = is_transversion = False. ```. The Ti/Tv ratio is calculated as `float(variant_counts(is_transition)) / variant_counts(is_transversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site wa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:899,energy efficiency,model,models,899,"Hi @jianqi-chen,. I'm assuming you are working with germline diploid samples. $`1)`$ Ti/Tv ratio is calculated as follows. For every variant that is a biallelic SNP, it will check as follows:. ```Python. is_transition = if {'A' <-> 'G'} or {'C' <-> 'T'}. is_transversion = not( is_transition ). else is_transition = is_transversion = False. ```. The Ti/Tv ratio is calculated as `float(variant_counts(is_transition)) / variant_counts(is_transversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site wa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1140,energy efficiency,model,model,1140,"at is a biallelic SNP, it will check as follows:. ```Python. is_transition = if {'A' <-> 'G'} or {'C' <-> 'T'}. is_transversion = not( is_transition ). else is_transition = is_transversion = False. ```. The Ti/Tv ratio is calculated as `float(variant_counts(is_transition)) / variant_counts(is_transversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1687,energy efficiency,predict,predicted,1687,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1856,energy efficiency,model,model,1856,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1873,energy efficiency,predict,predicted,1873,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1960,integrability,depend,depends,1960,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:505,interoperability,format,formatted,505,"Hi @jianqi-chen,. I'm assuming you are working with germline diploid samples. $`1)`$ Ti/Tv ratio is calculated as follows. For every variant that is a biallelic SNP, it will check as follows:. ```Python. is_transition = if {'A' <-> 'G'} or {'C' <-> 'T'}. is_transversion = not( is_transition ). else is_transition = is_transversion = False. ```. The Ti/Tv ratio is calculated as `float(variant_counts(is_transition)) / variant_counts(is_transversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site wa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:2163,interoperability,specif,specific-variant-in-my-data,2163,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1960,modifiability,depend,depends,1960,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:914,performance,perform,perform,914,"Hi @jianqi-chen,. I'm assuming you are working with germline diploid samples. $`1)`$ Ti/Tv ratio is calculated as follows. For every variant that is a biallelic SNP, it will check as follows:. ```Python. is_transition = if {'A' <-> 'G'} or {'C' <-> 'T'}. is_transversion = not( is_transition ). else is_transition = is_transversion = False. ```. The Ti/Tv ratio is calculated as `float(variant_counts(is_transition)) / variant_counts(is_transversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site wa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:2135,reliability,doe,does-deepvariant-not-call-a-specific-variant-in-my-data,2135,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:2417,reliability,Doe,Does,2417,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1687,safety,predict,predicted,1687,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1873,safety,predict,predicted,1873,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1960,safety,depend,depends,1960,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:661,security,model,model,661,"Hi @jianqi-chen,. I'm assuming you are working with germline diploid samples. $`1)`$ Ti/Tv ratio is calculated as follows. For every variant that is a biallelic SNP, it will check as follows:. ```Python. is_transition = if {'A' <-> 'G'} or {'C' <-> 'T'}. is_transversion = not( is_transition ). else is_transition = is_transversion = False. ```. The Ti/Tv ratio is calculated as `float(variant_counts(is_transition)) / variant_counts(is_transversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site wa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:899,security,model,models,899,"Hi @jianqi-chen,. I'm assuming you are working with germline diploid samples. $`1)`$ Ti/Tv ratio is calculated as follows. For every variant that is a biallelic SNP, it will check as follows:. ```Python. is_transition = if {'A' <-> 'G'} or {'C' <-> 'T'}. is_transversion = not( is_transition ). else is_transition = is_transversion = False. ```. The Ti/Tv ratio is calculated as `float(variant_counts(is_transition)) / variant_counts(is_transversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site wa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1140,security,model,model,1140,"at is a biallelic SNP, it will check as follows:. ```Python. is_transition = if {'A' <-> 'G'} or {'C' <-> 'T'}. is_transversion = not( is_transition ). else is_transition = is_transversion = False. ```. The Ti/Tv ratio is calculated as `float(variant_counts(is_transition)) / variant_counts(is_transversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1856,security,model,model,1856,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1600,testability,Simpl,Simplifying,1600,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1960,testability,depend,depends,1960,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:914,usability,perform,perform,914,"Hi @jianqi-chen,. I'm assuming you are working with germline diploid samples. $`1)`$ Ti/Tv ratio is calculated as follows. For every variant that is a biallelic SNP, it will check as follows:. ```Python. is_transition = if {'A' <-> 'G'} or {'C' <-> 'T'}. is_transversion = not( is_transition ). else is_transition = is_transversion = False. ```. The Ti/Tv ratio is calculated as `float(variant_counts(is_transition)) / variant_counts(is_transversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site wa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:1600,usability,Simpl,Simplifying,1600,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:2427,usability,help,help,2427,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help? Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:108,integrability,coupl,couple,108,"Hi @jianqi-chen,. In order to give you a more concrete answer it would be very helpful if you could provide couple of examples from your validation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:108,modifiability,coupl,couple,108,"Hi @jianqi-chen,. In order to give you a more concrete answer it would be very helpful if you could provide couple of examples from your validation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:137,safety,valid,validation,137,"Hi @jianqi-chen,. In order to give you a more concrete answer it would be very helpful if you could provide couple of examples from your validation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:137,security,validat,validation,137,"Hi @jianqi-chen,. In order to give you a more concrete answer it would be very helpful if you could provide couple of examples from your validation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:108,testability,coupl,couple,108,"Hi @jianqi-chen,. In order to give you a more concrete answer it would be very helpful if you could provide couple of examples from your validation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:79,usability,help,helpful,79,"Hi @jianqi-chen,. In order to give you a more concrete answer it would be very helpful if you could provide couple of examples from your validation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/703:187,deployability,depend,dependent,187,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:260,deployability,depend,depends,260,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:680,energy efficiency,optim,optimal,680,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:877,energy efficiency,model,model,877,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:187,integrability,depend,dependent,187,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:260,integrability,depend,depends,260,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:406,interoperability,compatib,compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications,406,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:187,modifiability,depend,dependent,187,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:260,modifiability,depend,depends,260,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:429,performance,perform,performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications,429,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:187,safety,depend,dependent,187,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:260,safety,depend,depends,260,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:915,safety,valid,validation,915,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:877,security,model,model,877,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:915,security,validat,validation,915,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:147,testability,understand,understand-genetic-disease,147,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:187,testability,depend,dependent,187,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:260,testability,depend,depends,260,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:693,testability,coverag,coverage,693,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:839,testability,coverag,coverage,839,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:119,usability,effectiv,effective-dna-sequencing-to-understand-genetic-disease,119,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:287,usability,prefer,prefer,287,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:429,usability,perform,performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications,429,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:1116,usability,help,helps,1116,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:49,energy efficiency,model,model,49,"Hi @Fred-07 ,. Yes, you can use our existing WES model. @AndrewCarroll can add more details later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:49,security,model,model,49,"Hi @Fred-07 ,. Yes, you can use our existing WES model. @AndrewCarroll can add more details later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:119,deployability,observ,observed,119,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:172,deployability,observ,observed,172,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:98,energy efficiency,model,models,98,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:246,energy efficiency,model,models,246,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:459,energy efficiency,model,model,459,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:563,energy efficiency,model,model,563,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:527,performance,perform,performance,527,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:601,performance,perform,performance,601,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:336,reliability,doe,does,336,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:98,security,model,models,98,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:246,security,model,models,246,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:459,security,model,model,459,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:563,security,model,model,563,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:119,testability,observ,observed,119,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:172,testability,observ,observed,172,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:527,usability,perform,performance,527,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:589,usability,feedback,feedback,589,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:601,usability,perform,performance,601,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:52,energy efficiency,model,model,52,Thanks for the feedback and the good news about the model. I plan to get some WES data from Element for testing.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:104,safety,test,testing,104,Thanks for the feedback and the good news about the model. I plan to get some WES data from Element for testing.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:52,security,model,model,52,Thanks for the feedback and the good news about the model. I plan to get some WES data from Element for testing.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:61,testability,plan,plan,61,Thanks for the feedback and the good news about the model. I plan to get some WES data from Element for testing.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:104,testability,test,testing,104,Thanks for the feedback and the good news about the model. I plan to get some WES data from Element for testing.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:15,usability,feedback,feedback,15,Thanks for the feedback and the good news about the model. I plan to get some WES data from Element for testing.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/704:1625,availability,replic,replicate,1625,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:375,energy efficiency,model,models,375,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:750,energy efficiency,model,model,750,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1012,energy efficiency,model,model,1012,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:918,interoperability,standard,standard,918,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1556,modifiability,inherit,inheritance,1556,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:302,safety,compl,complex,302,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1646,safety,valid,validate,1646,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1690,safety,valid,validation,1690,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:166,security,sign,signal,166,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:302,security,compl,complex,302,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:375,security,model,models,375,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:750,security,model,model,750,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1012,security,model,model,1012,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1477,security,control,controlled,1477,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1646,security,validat,validate,1646,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1690,security,validat,validation,1690,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1477,testability,control,controlled,1477,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:119,deployability,pipelin,pipeline,119,"Hi Paul. Thanks for all the informations. As learnt from above, I should use the ""DeepVariant -> WhatsHap -> DeepTrio"" pipeline for better accuracy of the Deeptrio, as it do not support the functionality of reading haplotagging. But the DeepVariant can do it since 1.4.0, which means in case of trio analysis, ""DeepVariant + GLnexus"" will already be most accurate way currently, am I get it right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:368,energy efficiency,current,currently,368,"Hi Paul. Thanks for all the informations. As learnt from above, I should use the ""DeepVariant -> WhatsHap -> DeepTrio"" pipeline for better accuracy of the Deeptrio, as it do not support the functionality of reading haplotagging. But the DeepVariant can do it since 1.4.0, which means in case of trio analysis, ""DeepVariant + GLnexus"" will already be most accurate way currently, am I get it right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:119,integrability,pipelin,pipeline,119,"Hi Paul. Thanks for all the informations. As learnt from above, I should use the ""DeepVariant -> WhatsHap -> DeepTrio"" pipeline for better accuracy of the Deeptrio, as it do not support the functionality of reading haplotagging. But the DeepVariant can do it since 1.4.0, which means in case of trio analysis, ""DeepVariant + GLnexus"" will already be most accurate way currently, am I get it right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:45,usability,learn,learnt,45,"Hi Paul. Thanks for all the informations. As learnt from above, I should use the ""DeepVariant -> WhatsHap -> DeepTrio"" pipeline for better accuracy of the Deeptrio, as it do not support the functionality of reading haplotagging. But the DeepVariant can do it since 1.4.0, which means in case of trio analysis, ""DeepVariant + GLnexus"" will already be most accurate way currently, am I get it right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:178,usability,support,support,178,"Hi Paul. Thanks for all the informations. As learnt from above, I should use the ""DeepVariant -> WhatsHap -> DeepTrio"" pipeline for better accuracy of the Deeptrio, as it do not support the functionality of reading haplotagging. But the DeepVariant can do it since 1.4.0, which means in case of trio analysis, ""DeepVariant + GLnexus"" will already be most accurate way currently, am I get it right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:80,deployability,version,version,80,"Hi Gaian,. You're absolutely right! Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional `--use_hp_information` flag following `whatshap` processing), while `DeepVariant -> GLnexus` should work as is. Good catch! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:80,integrability,version,version,80,"Hi Gaian,. You're absolutely right! Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional `--use_hp_information` flag following `whatshap` processing), while `DeepVariant -> GLnexus` should work as is. Good catch! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:80,modifiability,version,version,80,"Hi Gaian,. You're absolutely right! Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional `--use_hp_information` flag following `whatshap` processing), while `DeepVariant -> GLnexus` should work as is. Good catch! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:71,deployability,releas,release,71,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1074,deployability,depend,depends,1074,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1114,energy efficiency,cloud,cloud,1114,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:665,integrability,sub,subset,665,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1052,integrability,translat,translates,1052,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1074,integrability,depend,depends,1074,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1052,interoperability,translat,translates,1052,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1074,modifiability,depend,depends,1074,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:193,performance,tune,tuned,193,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:234,performance,perform,performance,234,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:558,performance,perform,perform-analysis-with-happy-against-,558,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:833,performance,content,content,833,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:914,performance,time,time,914,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:961,reliability,doe,does,961,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:117,safety,detect,detection,117,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:225,safety,valid,validate,225,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1074,safety,depend,depends,1074,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:117,security,detect,detection,117,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:225,security,validat,validate,225,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:85,testability,plan,planning,85,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:1074,testability,depend,depends,1074,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:234,usability,perform,performance,234,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:558,usability,perform,perform-analysis-with-happy-against-,558,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/705:458,availability,down,downsampled,458,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:520,availability,operat,operates,520,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:514,energy efficiency,model,model,514,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:662,energy efficiency,model,model,662,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:352,performance,perform,performed,352,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:501,safety,input,input,501,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:872,safety,valid,validates,872,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:514,security,model,model,514,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:662,security,model,model,662,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:872,security,validat,validates,872,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:352,usability,perform,performed,352,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:501,usability,input,input,501,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:837,usability,help,help,837,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:20,performance,perform,performance,20,"@aizhimin I suspect performance will be poor, but if you have a method for validating we would be interested in seeing the results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:75,safety,valid,validating,75,"@aizhimin I suspect performance will be poor, but if you have a method for validating we would be interested in seeing the results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:75,security,validat,validating,75,"@aizhimin I suspect performance will be poor, but if you have a method for validating we would be interested in seeing the results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:20,usability,perform,performance,20,"@aizhimin I suspect performance will be poor, but if you have a method for validating we would be interested in seeing the results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:52,deployability,observ,observed,52,"@aizhimin . For 10x genomics data, We've previously observed lower accuracy both across many methods and DeepVariant as well. I think we will do ""OK"" on 10x data, likely not what I would recommend for 10x data. . For sc-RNA seq, I have a similar reaction, but it may also be the case that the alternatives are even fewer in number. As @danielecook and @pgrosu mention, it might be worth doing if you have some way of assessing and validating the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:431,safety,valid,validating,431,"@aizhimin . For 10x genomics data, We've previously observed lower accuracy both across many methods and DeepVariant as well. I think we will do ""OK"" on 10x data, likely not what I would recommend for 10x data. . For sc-RNA seq, I have a similar reaction, but it may also be the case that the alternatives are even fewer in number. As @danielecook and @pgrosu mention, it might be worth doing if you have some way of assessing and validating the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:417,security,assess,assessing,417,"@aizhimin . For 10x genomics data, We've previously observed lower accuracy both across many methods and DeepVariant as well. I think we will do ""OK"" on 10x data, likely not what I would recommend for 10x data. . For sc-RNA seq, I have a similar reaction, but it may also be the case that the alternatives are even fewer in number. As @danielecook and @pgrosu mention, it might be worth doing if you have some way of assessing and validating the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:431,security,validat,validating,431,"@aizhimin . For 10x genomics data, We've previously observed lower accuracy both across many methods and DeepVariant as well. I think we will do ""OK"" on 10x data, likely not what I would recommend for 10x data. . For sc-RNA seq, I have a similar reaction, but it may also be the case that the alternatives are even fewer in number. As @danielecook and @pgrosu mention, it might be worth doing if you have some way of assessing and validating the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:52,testability,observ,observed,52,"@aizhimin . For 10x genomics data, We've previously observed lower accuracy both across many methods and DeepVariant as well. I think we will do ""OK"" on 10x data, likely not what I would recommend for 10x data. . For sc-RNA seq, I have a similar reaction, but it may also be the case that the alternatives are even fewer in number. As @danielecook and @pgrosu mention, it might be worth doing if you have some way of assessing and validating the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/706:1219,availability,checkpoint,checkpoints,1219," to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-16",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1948,deployability,updat,updating,1948,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:338,energy efficiency,model,models,338,"Hi Sophie,. Similarly to my reply in https://github.com/google/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:394,energy efficiency,model,model,394,"Hi Sophie,. Similarly to my reply in https://github.com/google/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:441,energy efficiency,model,model,441,"Hi Sophie,. Similarly to my reply in https://github.com/google/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:553,energy efficiency,model,model,553,"Hi Sophie,. Similarly to my reply in https://github.com/google/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1324,energy efficiency,model,model,1324,"ple-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1389,energy efficiency,model,model,1389,"e model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1791,energy efficiency,model,model,1791,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1812,energy efficiency,model,modeling,1812,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2095,energy efficiency,model,model,2095,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2408,energy efficiency,model,model,2408,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2436,energy efficiency,model,model,2436,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:686,modifiability,paramet,parameter,686,"Hi Sophie,. Similarly to my reply in https://github.com/google/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1462,modifiability,paramet,parameter,1462,"ed to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1495,modifiability,paramet,parameter,1495,"nt as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I shoul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1821,modifiability,paramet,parameters,1821,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1921,modifiability,paramet,parameters-to-tune,1921,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1935,performance,tune,tune,1935,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1219,reliability,checkpoint,checkpoints,1219," to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-16",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:930,safety,valid,validation,930,"Hi Sophie,. Similarly to my reply in https://github.com/google/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1057,safety,valid,validation,1057,"/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1177,safety,valid,validation,1177,"e bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/go",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1375,safety,Test,Test,1375,"d representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1525,safety,test,test,1525,"ive samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1948,safety,updat,updating,1948,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2050,safety,input,input,2050,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2306,safety,test,test,2306,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2451,safety,test,tested,2451,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2467,safety,test,test,2467,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:338,security,model,models,338,"Hi Sophie,. Similarly to my reply in https://github.com/google/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:394,security,model,model,394,"Hi Sophie,. Similarly to my reply in https://github.com/google/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:441,security,model,model,441,"Hi Sophie,. Similarly to my reply in https://github.com/google/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:553,security,model,model,553,"Hi Sophie,. Similarly to my reply in https://github.com/google/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:930,security,validat,validation,930,"Hi Sophie,. Similarly to my reply in https://github.com/google/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1057,security,validat,validation,1057,"/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1177,security,validat,validation,1177,"e bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/go",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1324,security,model,model,1324,"ple-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1389,security,model,model,1389,"e model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1791,security,model,model,1791,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1812,security,model,modeling,1812,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1948,security,updat,updating,1948,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2095,security,model,model,2095,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2408,security,model,model,2408,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2436,security,model,model,2436,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1375,testability,Test,Test,1375,"d representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1525,testability,test,test,1525,"ive samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2306,testability,test,test,2306,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2451,testability,test,tested,2451,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2467,testability,test,test,2467,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2050,usability,input,input,2050,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2530,usability,help,helps,2530,"train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: . 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords. 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately. 3) Run `model_train` on shuffled training set shuffled data. 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files. 5) Pick best model listed in the `best_checkpoint.txt` file. 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. . 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study. 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:134,performance,time,times,134,"Hi @sophienguyen01 , . if you want to generate examples from multiple samples, you'll need to run `make_examples` separately multiple times. And then you can use the shuffling step to put all the examples from multiple samples in the same dataset for training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:275,energy efficiency,model,model,275,"Thank you all for your suggestions, I went ahead and shuffle across my training examples. I also done the `model_train` and `model_eval` as in the tutorial but the evaluation result on HG003_chr20 does not look good at all. The F1 score drop dramatically ( 0 for one trained model and almost 0 for another model). So far, I kept the same value for paramater in`model_train` step:. `--model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. `. I know that changing these values will affect the evaluation results. I went through [DeepVariant paper](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.full.pdf) but couldn't find any information on what parameters were used during the training. which values of the training parameter do you suggest that I can try?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:306,energy efficiency,model,model,306,"Thank you all for your suggestions, I went ahead and shuffle across my training examples. I also done the `model_train` and `model_eval` as in the tutorial but the evaluation result on HG003_chr20 does not look good at all. The F1 score drop dramatically ( 0 for one trained model and almost 0 for another model). So far, I kept the same value for paramater in`model_train` step:. `--model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. `. I know that changing these values will affect the evaluation results. I went through [DeepVariant paper](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.full.pdf) but couldn't find any information on what parameters were used during the training. which values of the training parameter do you suggest that I can try?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:733,modifiability,paramet,parameters,733,"Thank you all for your suggestions, I went ahead and shuffle across my training examples. I also done the `model_train` and `model_eval` as in the tutorial but the evaluation result on HG003_chr20 does not look good at all. The F1 score drop dramatically ( 0 for one trained model and almost 0 for another model). So far, I kept the same value for paramater in`model_train` step:. `--model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. `. I know that changing these values will affect the evaluation results. I went through [DeepVariant paper](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.full.pdf) but couldn't find any information on what parameters were used during the training. which values of the training parameter do you suggest that I can try?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:804,modifiability,paramet,parameter,804,"Thank you all for your suggestions, I went ahead and shuffle across my training examples. I also done the `model_train` and `model_eval` as in the tutorial but the evaluation result on HG003_chr20 does not look good at all. The F1 score drop dramatically ( 0 for one trained model and almost 0 for another model). So far, I kept the same value for paramater in`model_train` step:. `--model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. `. I know that changing these values will affect the evaluation results. I went through [DeepVariant paper](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.full.pdf) but couldn't find any information on what parameters were used during the training. which values of the training parameter do you suggest that I can try?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:645,performance,content,content,645,"Thank you all for your suggestions, I went ahead and shuffle across my training examples. I also done the `model_train` and `model_eval` as in the tutorial but the evaluation result on HG003_chr20 does not look good at all. The F1 score drop dramatically ( 0 for one trained model and almost 0 for another model). So far, I kept the same value for paramater in`model_train` step:. `--model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. `. I know that changing these values will affect the evaluation results. I went through [DeepVariant paper](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.full.pdf) but couldn't find any information on what parameters were used during the training. which values of the training parameter do you suggest that I can try?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:197,reliability,doe,does,197,"Thank you all for your suggestions, I went ahead and shuffle across my training examples. I also done the `model_train` and `model_eval` as in the tutorial but the evaluation result on HG003_chr20 does not look good at all. The F1 score drop dramatically ( 0 for one trained model and almost 0 for another model). So far, I kept the same value for paramater in`model_train` step:. `--model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. `. I know that changing these values will affect the evaluation results. I went through [DeepVariant paper](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.full.pdf) but couldn't find any information on what parameters were used during the training. which values of the training parameter do you suggest that I can try?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:275,security,model,model,275,"Thank you all for your suggestions, I went ahead and shuffle across my training examples. I also done the `model_train` and `model_eval` as in the tutorial but the evaluation result on HG003_chr20 does not look good at all. The F1 score drop dramatically ( 0 for one trained model and almost 0 for another model). So far, I kept the same value for paramater in`model_train` step:. `--model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. `. I know that changing these values will affect the evaluation results. I went through [DeepVariant paper](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.full.pdf) but couldn't find any information on what parameters were used during the training. which values of the training parameter do you suggest that I can try?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:306,security,model,model,306,"Thank you all for your suggestions, I went ahead and shuffle across my training examples. I also done the `model_train` and `model_eval` as in the tutorial but the evaluation result on HG003_chr20 does not look good at all. The F1 score drop dramatically ( 0 for one trained model and almost 0 for another model). So far, I kept the same value for paramater in`model_train` step:. `--model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. `. I know that changing these values will affect the evaluation results. I went through [DeepVariant paper](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.full.pdf) but couldn't find any information on what parameters were used during the training. which values of the training parameter do you suggest that I can try?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1399,deployability,automat,automatically,1399," underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![im",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2547,deployability,updat,update,2547,"ce yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:24,energy efficiency,model,model,24,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:390,energy efficiency,model,model,390,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:503,energy efficiency,optim,optimizing,503,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:514,energy efficiency,model,model,514,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:765,energy efficiency,Optim,Optimization,765,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:804,energy efficiency,estimat,estimators,804,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1080,energy efficiency,model,models,1080,"wer initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to so",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1505,energy efficiency,model,models,1505,"izing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1707,energy efficiency,model,modeling,1707,"al Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to selec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1789,energy efficiency,optim,optimal,1789,"red Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1839,energy efficiency,optim,optimal,1839,"cle](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning y",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1943,energy efficiency,model,model,1943,"ry of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2046,energy efficiency,optim,optimal,2046,"ters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2203,energy efficiency,model,model,2203,"ing-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2595,energy efficiency,model,model,2595,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2804,energy efficiency,model,model,2804,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3010,energy efficiency,model,models,3010,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3142,energy efficiency,model,model,3142,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3161,energy efficiency,optim,optimal,3161,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3311,energy efficiency,optim,optimal,3311,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3319,energy efficiency,model,model,3319,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:536,integrability,discover,discovery,536,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2622,integrability,batch,batch,2622,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2866,integrability,batch,batch,2866,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:536,interoperability,discover,discovery,536,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:358,modifiability,paramet,parameters,358,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1165,modifiability,paramet,parameter-tuning-techniques-in-deep-learning-,1165,"le/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you wan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1615,modifiability,paramet,parameters,1615,"ps://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ Fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3036,modifiability,paramet,parameters,3036,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:503,performance,optimiz,optimizing,503,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:546,performance,perform,performed,546,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:765,performance,Optimiz,Optimization,765,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1389,performance,perform,performed,1389,"l, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2622,performance,batch,batch,2622,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2866,performance,batch,batch,2866,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2897,performance,perform,performs,2897,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3184,reliability,doe,does,3184,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:904,safety,compl,complete-guide,904,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2547,safety,updat,update,2547,"ce yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3555,safety,valid,validation,3555,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:24,security,model,model,24,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:390,security,model,model,390,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:514,security,model,model,514,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:904,security,compl,complete-guide,904,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1080,security,model,models,1080,"wer initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to so",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1505,security,model,models,1505,"izing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1707,security,model,modeling,1707,"al Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to selec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1943,security,model,model,1943,"ry of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2203,security,model,model,2203,"ing-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2547,security,updat,update,2547,"ce yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2595,security,model,model,2595,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2804,security,model,model,2804,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3010,security,model,models,3010,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3142,security,model,model,3142,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3319,security,model,model,3319,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3555,security,validat,validation,3555,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1399,testability,automat,automatically,1399," underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![im",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1575,testability,simpl,simple,1575,"ing. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:536,usability,discov,discovery,536,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:546,usability,perform,performed,546,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:913,usability,guid,guide,913,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:994,usability,visual,visually,994,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1030,usability,guid,guide-to-hyperparameters-search-for-deep-learning-models,1030,". Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1201,usability,learn,learning-,1201,"ues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1389,usability,perform,performed,1389,"l, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1455,usability,guid,guide-to-hyperparameters-search-for-deep-learning-models,1455,"re the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1575,usability,simpl,simple,1575,"ing. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning. * Random Search. * Grid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1738,usability,Learn,Learning,1738,"rid Search. * Bayesian Optimization. * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1779,usability,close,closer,1779,"ree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select wha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1829,usability,close,closer,1829," to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1878,usability,minim,minimal,1878,"eter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1959,usability,learn,learn,1959,"h [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learni",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2028,usability,close,closer,2028,"uide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2066,usability,minim,minimize,2066,"-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2119,usability,learn,learning,2119,"cle](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2558,usability,learn,learning,2558,"like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2897,usability,perform,performs,2897,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2956,usability,learn,learning,2956,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3110,usability,interact,interact,3110,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3582,usability,help,helps,3582,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:524,availability,checkpoint,checkpoint,524,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:186,energy efficiency,model,model,186,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:542,energy efficiency,model,model,542,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:583,energy efficiency,model,model,583,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:632,energy efficiency,model,model,632,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:656,energy efficiency,model,model,656,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:524,reliability,checkpoint,checkpoint,524,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:418,safety,input,input,418,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:536,safety,input,input,536,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:577,safety,input,input,577,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:186,security,model,model,186,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:542,security,model,model,542,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:583,security,model,model,583,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:632,security,model,model,632,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:656,security,model,model,656,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:134,usability,help,helps,134,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:418,usability,input,input,418,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:536,usability,input,input,536,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:577,usability,input,input,577,"Thanks Paul for this very insightful comment,. I went ahead and redoing my retraining. I see improvements in the accuracy score. That helps a lot. Now my next step is to use the retrain model to run on our internal sample. I want to convert the .ckpt to .pb file. Here is the comment I got from the [tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md):. `sudo docker run \. -v $PWD:/input \. -v $PWD:/output \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/freeze_graph \. --checkpoint /input/model.ckpt \. --example_info_json /input/model.ckpt.example_info.json \. --output /output/model.pb`. What is the `model.ckpt.example_info.json` here? I don't see the json file being created when I do `model_train` which is the step that generate the `.ckpt` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:206,availability,checkpoint,checkpoint,206,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1148,availability,checkpoint,checkpointed,1148," So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:570,deployability,version,versions,570,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1401,deployability,automat,automatically,1401,"ould be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1577,deployability,contain,contains,1577," the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1590,deployability,version,version,1590,"th additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1827,deployability,version,version,1827,"the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1943,deployability,version,version,1943,"onvert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2056,deployability,version,version,2056," . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2163,deployability,version,version,2163,"s JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2289,deployability,version,version,2289,"or image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-incep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:174,energy efficiency,model,model,174,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:276,energy efficiency,model,model,276,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:379,energy efficiency,model,model,379,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:586,energy efficiency,model,model,586,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:780,energy efficiency,model,model,780,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:845,energy efficiency,model,model,845,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:859,energy efficiency,model,model,859,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:954,energy efficiency,model,modelckpt-to-modelpb-savedmodel-format,954,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1085,energy efficiency,model,model,1085," rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1161,energy efficiency,model,model,1161,"a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1334,energy efficiency,model,model,1334,"deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""chan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2468,energy efficiency,model,model,2468,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2596,energy efficiency,model,models,2596,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2666,energy efficiency,model,model,2666,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2754,energy efficiency,model,models,2754,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2831,energy efficiency,model,model,2831,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2915,energy efficiency,model,models,2915,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2989,energy efficiency,model,model,2989,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3073,energy efficiency,model,models,3073,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3147,energy efficiency,model,model,3147,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3251,energy efficiency,model,models,3251,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3328,energy efficiency,model,model,3328,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:570,integrability,version,versions,570,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1590,integrability,version,version,1590,"th additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1827,integrability,version,version,1827,"the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1943,integrability,version,version,1943,"onvert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2056,integrability,version,version,2056," . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2163,integrability,version,version,2163,"s JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2289,integrability,version,version,2289,"or image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-incep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:189,interoperability,specif,specify,189,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:448,interoperability,specif,specified,448,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:560,interoperability,format,formatted,560,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:658,interoperability,format,format,658,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:815,interoperability,specif,specified,815,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:986,interoperability,format,format,986,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1045,interoperability,format,formatted,1045,"is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Js",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:570,modifiability,version,versions,570,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1590,modifiability,version,version,1590,"th additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1827,modifiability,version,version,1827,"the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1924,modifiability,Pac,PacBio,1924,"variant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepV",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1943,modifiability,version,version,1943,"onvert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2056,modifiability,version,version,2056," . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2163,modifiability,version,version,2163,"s JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2261,modifiability,Pac,PacBio,2261,"h as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2289,modifiability,version,version,2289,"or image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-incep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:2703,modifiability,Pac,PacBio,2703,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:3191,modifiability,Pac,PacBio,3191,". It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-pacbio_standard/model.ckpt.example_info.json. ##### WES. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wes_standard/model.ckpt.example_info.json. ##### WGS. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-wgs_standard/model.ckpt.example_info.json . ##### Hybrid PacBio/Illumina. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-hybrid_standard/model.ckpt.example_info.json. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:719,performance,perform,performing,719,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1353,performance,perform,performing,1353,"case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1768,performance,content,contents,1768,"`.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### Hybrid PacBio/Illumina. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}. ```. Feel free to generate one, or copy it from the following locations, but it should be based on the model technology you trained on (*meaning they have to be exact*):. ##### ONT R10.4. https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-ont_r104/model.ckpt.example_info.json . ##### PacBio. https://storage.googleapis.com/deepvariant/models/DeepVariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:206,reliability,checkpoint,checkpoint,206,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1148,reliability,checkpoint,checkpointed,1148," So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ##### WGS. ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:1135,safety,valid,validate,1135,"rately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### WES. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:174,security,model,model,174,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:276,security,model,model,276,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:379,security,model,model,379,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:586,security,model,model,586,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:780,security,model,model,780,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:845,security,model,model,845,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:859,security,model,model,859,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. ```. ##### PacBio. ```Json. {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
